---
title: "Sesión 2: Estadística Descriptiva (II) y Prueba de Hipótesis (I)"
subtitle: "Taller de Métodos y Técnicas de Investigación I"
---







En esta sesión entraremos de lleno en estadística descriptiva, retomando desde contenidos de la sesión pasada, pero con un enfoque más riguroso. Sumado a esto, nos enfocaremos más en esto desde una perspectiva más estadística y usando R aplicadamente, sin deternos tanto en R mismo. Aunque también aprovecharé de mostrarle como crear funciones de usuario para hacer estadísticas descriptivas con una sola función. 


## Objetivos de la sesión

- Comprender la estadística descriptiva
- Conocer medidas de la estadística descriptiva
- Crear visualizaciones avanzadas.
- Introducción a test de hipótesis y estadística inferencial

Pero antes de comenzar, carguemos los paquetes que utilizaremos, donde además les muestro mi forma favorita de cargar y traer paquetes en un solo paso (con `pacman`). 







```{r}
#| label: cargar-paquetes
#| collapse: true
#| eval: false
#| echo: true

#Forma 1 (Forma clásica)
install.packages("tidyverse") # colección de paquetes, dplyr entre ellos
install.packages("dplyr") # manipular datos
install.packages("sjmisc") # explorar datos

#Forma 2 (Mejor para reproductibilidad)
invisible(lapply(c("tidyverse", # colección de paquetes, dplyr entre ellos
                   "dplyr", # manipular datos
                   "sjmisc"), # explorar datos
                 function(p) 
  if (!requireNamespace(p, quietly = TRUE)) install.packages(p)))

# Llamar librería 
library(tidyverse)
library(dplyr)
library(sjmisc)

# Forma 3 (Mi favorita. Es `install.packages()` -si no está bajado- más `library`)

if (!requireNamespace("pacman", quietly = TRUE))
  install.packages("pacman")
pacman::p_load(tidyverse, # colección de paquetes, dplyr entre ellos, pero también haven
               dplyr, # manipular datos
               psych, # para estadísticas descriptivas
               sjmisc, # explorar datos
               ggplot2, # para visualizar gráficos
               scales) # para ajustar gráficos
```

```{r}
#| label: cargar-paquetes-real
#| include: false
if (!requireNamespace("pacman", quietly = TRUE))
  install.packages("pacman")
pacman::p_load(tidyverse, # colección de paquetes, dplyr entre ellos, pero también haven
               dplyr, # manipular datos
               psych, # para estadísticas descriptivas
               sjmisc, # explorar datos
               ggplot2, # para visualizar gráficos
               scales) # para ajustar gráficos

options(scipen = 999)
```









# 1. Estadística descriptiva

## Datos 

Para comprender que son las estadísticas descriptivas, y para que las usaremos como futurxs sociólogxs, partamos viendo *qué son los datos*. Los **datos** son valores (o mediciones) de variables que han sido recolectados y organizados para su análisis. La palabra datos se define como información factual (como mediciones o estadísticas) utilizada como base para el razonamiento, para la discusión o para cálculos [(Merrian-Webster)](https://www.merriam-webster.com/dictionary/data). Existen tres tipos de *estrucutas de datos*: 

1. **Datos de corte transversal**: son mediciones de un conjunto de variables para un gurpo de unidades (personas, hogares, empresas, países, etc.) en un mismo momento del tiempo. Un ejemplo de esto puede ser los puntajes PSU de un año dado. Una base de datos de datos de corte transversal, e.g., es la CASEN, la ENUT, la ENE, etc. 

2. **Datos de series de tiempo**: sucesión de registros de una o varias variables de una única unidad, que son medidos en determinados momento del tiempo en un orden cronológico claro. Es importante resaltar que registra una **única** entidad/unidad a lo largo del tiempo. Un ejemplo podría ser el IMACEC o el PIB de tal a tal periodo. 

3. **Datos de panel o longitudinales**: sucesión de registros de un conjunto de variables para un grupo de unidades medidas en varios momentos del tiempo. Los datos de panel/longitudinales combinan las dimensiones de corte transversal y series de tiempo de los datos, pues es una sucesión de registros de un *conjunto de variables* para un *grupo de unidades*, medidas en varios momentos del tiempo con un orden cronológico claro. Un ejemplo de esto podría ser la tasa de crecimiento anual del PIB real para un grupo de países de tal a cual periodo. 

A su vez, existen dos tipos de recolección de datos:

1. **Estudios observacionales**: los datos se recopilan por medio de la observación de los valores de las variables de interés sin intervención o influencia en ellos; o 

2. **Experimentos**: tras una exposición de ciertas unidades a una intervención, se observan los valores de las variables de interés presentes en los resultados. 

## Tipos de variables

Una variable es una característica o atributo que puede tomar valores diferentes para distintas unidades. Las variables tienen dos categorías principales que las distinguen: 

1. **Variables categóricas**: puede tomar un número limitado de valores definido sobre la base de alguna característica cualitativa. A su vez, dentro de las variables categóricas se encuentran
  
  - *Variable categórica nominal*: es una variable categórica/cualitativa sin ningún orden o jerarquía clara. Por ejemplo, una variable que registre nombres de un curso, comunas, países, etc. Dentro de las variables categóricas nominales, se encuentran las variables *dummy*, las que poseen solo dos valores a los que se les puede asignar un número, generalmente $0$ o $1$, donde $1$ indica presencia del atributo y $0$ ausencia.
  
  - *Varible categórica ordinal*: es una variable categórica/cualitativa que tiene un orden o jerarquí establecida entre sus categorías. Por ejemplo, el nivel educacional o las variables en "Escala Likert"
  
En R, para crear una variable cualitativa, deben escribirse entre comillas sus categorías, como ya vimos la sesión pasada. Por ejemplo, 







```{r}
#| label: eg-variables-categoricas
#| collapse: true 
# Variables categóricas

var_nominal <- c("Karl Marx", "Rosa Luxemburgo", "Wooldrigde")

class(var_nominal) # Comprobar naturaleza de la variable

var_ordinal_educ<- c("Secundaria Incompleta", "Secundaria Completa",
                     "Educación Superior Incompleta", "Educación Superior Completa")
var_ordinal_likert <- c("Muy malo", "Malo", "Maomeno",
                        "Bueno", "Muy bueno")

class(var_ordinal_educ) # Comprobar naturaleza de la variable
class(var_ordinal_likert) # Comprobar naturaleza de la variable

```







Suele ser útil, en R, asignarles números a las variables cualitativas para un análisis estadísticos posteriores. Por ejemplo, podríamos tomar `var_ordinal_likert` y, *además* de que tenga sus categorías asociadas, también tengan un número. Por ejemplo, que `"Muy malo"` sea `1` y `"Muy bueno"` sea `5`. Esto podemos hacerlo de dos maneras que mostramos de inmediato. Por cierto, la segunda la hacemos con `haven` que es un paquete dentro de `tidyverse`, y que usualmente usaremos ese paquete para cargar bases de datos, pero que también sirve para recodificar:







```{r}
#| label: eg-variables-categoricas2
#| collapse: true
# 1) Versión con un factor ordenado (tienes la categoría y su código interno)
var_ordinal_likert <- factor(
  var_ordinal_likert,
  levels  = c("Muy malo", "Malo", "Maomeno", "Bueno", "Muy bueno"),
  ordered = TRUE
)
# Para ver el código numérico (1=Muy malo … 5=Muy bueno):
as.integer(var_ordinal_likert)


# 2) Si prefieres un vector numérico “double” con etiquetas (dos niveles),
#    puedes usar haven::labelled

var_ordinal_likert <- haven::labelled(
  as.integer(factor(
    var_ordinal_likert,
    levels  = c("Muy malo", "Malo", "Maomeno", "Bueno", "Muy bueno"),
    ordered = TRUE
  )),
  labels = c(
    "Muy malo"  = 1,
    "Malo"      = 2,
    "Maomeno"   = 3,
    "Bueno"     = 4,
    "Muy bueno" = 5
  )
)

# Comprobamos:
var_ordinal_likert
```







2. **Variables cuantitativas**: son variables medidas en una escala númerica, como la edad, temperatura, estatura, etc. Y, por lo tanto, que el número mismo indica ordinalidad y jerarquía entre los valores ($x<x+1$). También hay dos tipos de variables cuantitativas

  - *Variable cuantitativa discreta*: es aquella que *solo* puede tomar números específicos como valores, sin poder tomar un valor intermedio entre dos valores específicos. Es decir, que se expresan en números enteros. Matemáticamente, simplemente son variables en que sus valores $x \in \mathbb{N}$. Un ejemplo, puede ser el número de ventas (no se puede vender -100), la edad de las personas encuestadas (no se puede tener -2), etc. 
  
  - *Variable cuantiativa continua*: puede tomar como valores un número infinito de posibilidades dentro de un rango de números, por lo que se expresa en números reales: $x\in \mathbb{R}$. Un ejemplo de esto puede ser una cuenta corriente, el peso de las personas, la temperatura, etc. 
  
  En R, las variables cuantitativas tienen por categorías números, sin "...". Además, si se quiere dejar como variable cuantitativa discreta (`integer`), se le puede añadir una `L` después del número. Esto es porque en R cualquier literal numérico sin sufijo se interpreta por defecto como tipo `numeric` (variable cuantitativa). Al añadir la `L` se le indica a R que es un `integer` (número entero). No obstante, generalmente para el análisis estadístico no afectará mucho. 
  






```{r}
#| label: eg-variables-cuantitativas
#| collapse: true
# Variables cuantitativas discretas
var_discreta_hijos <- c(0L, 1L, 2L, 3L, 4L)
var_discreta_autos <- c(1L, 0L, 2L, 1L, 3L)
class(var_discreta_hijos)   
class(var_discreta_autos)   

# Variables cuantitativas continuas
var_continua_altura <- c(1.65, 1.72, 1.58, 1.80, 1.75)
var_continua_peso   <- c(65.4, 70.2, 58.9, 80.0, 72.5)
class(var_continua_altura)  # numeric
class(var_continua_peso)    # numeric

```








## Estadísticas descriptivas 

Las estadísticas descriptivas son valores fácilmente interpretables y que entregan un "resumen" de las características más importantes de un conjunto de datos, además de que utilizan para su cálculo operaciones aritméticas simples. Las estadísticas descriptivas son el primer paso en el análisis exploratorio de datos. Antes del análisis estadístico más riguroso de los datos, es esencial examinar las estadísticas descriptivas de todas las variables para:

- Detectar errores de medición o valores atípicos (*outliers*) en los datos.

- Detectar patrones y tendencias en los datos.

- Evaluar la plausibilidad de los supuestos de trabajo.

- Establecer hipótesis de trabajo.

A su vez, las estadísticas descriptivas se pueden clasificar en tres grupos:

  1. **Medidas de localización o tendencia central**: Se calculan para describir un valor central alrededor del cual se ubican (distribuyen) los datos. En otras palabras, estas medidas determinan el valor donde se ubican mayoritariamente las observaciones

  2. **Medidas de dispersión o variabilidad**: Proporcionan una descripción de la “dispersión de los datos” o qué tan lejos están las observaciones de la tendencia central.


  3. **Medidas de posición**: Resumen la posición relativa de valores específicos en los datos.
  
Antes de seguir profundizando esto, utilizaremos bases de datos reales y conocidas para ir ejemplificando el contenido con datos observacionales. Partamos con la Encuesta de Caracterización Socioeconómica Nacional (CASEN) más actual, la CASEN 2022. Esta se puede obtener en el [siguiente link](https://observatorio.ministeriodesarrollosocial.gob.cl/encuesta-casen-2022), que cuenta con 202.231 casos (encuestados) y 918 variables. 








```{r}
#| label: cargar-casen
#| collapse: true

## Obtener ruta 

getwd() # Para obtener nuestro directorio (en este casode nuestro RProject)

# Cargar formato sav (SPSS)
casen2022<-haven::read_sav("data-sesiones/CASEN 2022.sav")

dim(casen2022)

# Cargar formato dta (STATA)

# casen2022<-haven::read_dta(".../CASEN 2022.dta")

```







Continuemos. Antes de pasar a estos tres grupos de estadísticas descriptivas, conviene tener en claro el concepto de estadísticos de orden. Los **estadísticos de orden** es la ordenación de un conjunto de datos u obsrvaciones, dond $y_k$ es el $k-$ésimo menor valor del conjunto de observaciones. Por ejemplo, si tenemos estos cuatro datos: 
$$
x_1 =9; x_2=3, x_3 = 12; x_3 =1 ; x_5=2
$$
Los estadísticos de orden, $y_i$, corresponderían a
$$
y_1=1; y_2=2; y_3=3; y_4=9; y_5=12
$$
Es decir, la ordenación de $x_i$ de menor a mayor. Dicho esto, pasemos, en primer lugar, a las medidas de localización o tendencia central. 

## 1.1. Medidas de localización o tendencia central

#### Media o promedio

La *media* de $n$ observaciones de una variable, \(x_1, \dots, x_n\), que denotaremos por \(\bar{x}\), está definida por la suma de los valores de todas las observaciones, dividida por el número de observaciones: 
$$
\bar{x} = \frac{x_1 + x_2 + \dots + x_n}{n} = \frac{1}{n} \sum_{i=1}^{n} x_i.
$$
Por ejemplo, supongamos que los datos son:
$$
x_1 = 0.7; \quad x_2 = 1.2; \quad x_3 = 0.9; \quad x_4 = 0.6; \quad x_5 = 1.4; \quad x_6 = 1.8; \quad x_7 = 2.5.
$$ 
La media en este caso viene dada por:
$$
\bar{x} = \frac{0.7 + 1.2 + 0.9 + 0.6 + 1.4 + 1.8 + 2.5}{7} = 1.3.
$$
En R, la media se obtiene simplemente con el comando `mean()`. Veamos como obtener la media de alguna variable de interés en la CASEN 2022.  






```{r}
#| label: eg-mean
#| collapse: true
# Obtener media mediante mean()
mean(casen2022$ytrabajocor) # Ingreso del trabajo corregido
                            # Ups! 
mean(casen2022$ytrabajocor,
     na.rm = TRUE) # ahora si :)
```






Como se ve, el salario promedio de los encuestados de la CASEN 2022 es de 667.690CLP. El comando `na.rm = TRUE` es para sacar las observaciones `NA` de la variable, las cuales registran los casos 
  
#### Mediana  

La **mediana** es el valor medio de un conjunto de observaciones cuando se les ordena de menor a mayor. Si el número de observaciones es par, entonces la *mediana* es la suma de los dos valores medios, dividida por 2. Si ordenamos las observaciones del ejemplo anterior de menor a mayor obtenemos los **estadísticos de orden** del conjunto de observaciones: 
$$
y_1 = 0.6, \quad y_2 = 0.7, \quad y_3 = 0.9, \quad y_4 = 1.2, \quad y_5 = 1.4, \quad y_6 = 1.8, \quad y_7 = 2.5.
$$ 
Notar que los *estadísticos de orden* cumplen con 
$$
y_1 = \min(x_1, x_2, \dots, x_7), \quad y_7 = \max(x_1, x_2, \dots, x_7), \quad y_1 \leq y_2 \leq \dots \leq y_7.
$$ 
Entonces, para calcular la mediana se tiene que ordenar primero la muestra de datos de menor a mayor, tal que se cumpla con los estadísticos de orden. Así, la *mediana* en este ejemplo, que denotamos por $\tilde{x}$, viene dada por: 
$$
\tilde{x} = y_4 = 1.2.
$$ 
Ahora bien, en este caso, tenemos un $n$ impar. Luego, es más fácil establecer la mediana. Supongamos ahora que tenemos un número par de cantidad de datos, un $n=6$, por ejemplo. 
$$
x_1 = 0.7, \quad x_2 = 1.2, \quad x_3 = 0.9, \quad x_4 = 0.6, \quad x_5 = 1.4, \quad x_6 = 1.8.
$$ 
Es decir, los mismos datos que antes, salvo que hemos excluido el valor más grande. Entonces los estadísticos de orden son: 
$$
y_1 = 0.6, \quad y_2 = 0.7, \quad y_3 = 0.9, \quad y_4 = 1.2, \quad y_5 = 1.4, \quad y_6 = 1.8.
$$ 
Y la mediana es: 
$$
\tilde{x} = \frac{y_3 + y_4}{2} = \frac{0.9 + 1.2}{2} = 1.05.
$$ 
Por lo tanto, la mediana en este caso es el promedio simple entre los dos números ``al medio'' del conjunto de datos. Si tenemos que $n=6$, entonces la mediana es el promedio entre $x_3$ y $x_4$, arrojando un valor nuevo. En suma, la mediana es un valor que puede o no puede estar en el conjunto de datos. Si el conjunto de dato es par, entonces será el promedio simple entre los 2 datos del medio y si es impar simplemente será el valor del medio.  

En R esto se obtiene con `median()`:







```{r }
#| label: eg-median
#| collapse: true
# Obtener mediana a través de median()
median(casen2022$ytrabajocor,
     na.rm = TRUE) # Ingreso del trabajo corregido
``` 






  
Como vemos, la mediana de los ingresos del trabajo (salario), es 480.000 CLP. Lo cual habla bastante de la dispersión de los ingresos, pues el promedio  es de 667.690 CLP. Siguiendo la lógica de los estadísticos de orden, lo que tenemos es que el $50\%$ de los encuestados gana 480.000 CLP *o menos*. Y, sin embargo, el promedio es de 667.690 CLP, i.e., de 187.690 CLP más. Esto se debe a que hay ingresos muy altos, que elevan el promedio. Y la media es una medida *sensible* a los valores extremos. En breve, profundizaremos sobre esto, pero tiene que ver con robustez de las medidas de localización, lo cual será clave más adelante para para realizas **estimaciones** con estas medidas. 
  
#### Moda   
  
La *moda* es el valor más frecuente de un conjunto de observaciones. Un conjunto de valores puede tener más de una moda. Por ejemplo, supongamos que los datos son: 
$$
x_1 = 0.7; \quad x_2 = 1.2; \quad x_3 = 1.2; \quad x_4 = 0.6; \quad x_5 = 1.4; \quad x_6 = 1.8; \quad x_7 = 2.5.
$$
Entonces la moda, que denotaremos por $\tilde{\tilde{x}}$^[El LaTex se coloca `\dbtilde{}`. No me deja en Quarto, pero colocar `\tilde{\tilde{x}}` es para representar que se denota con doble tilde], viene dada por: 
$$
\tilde{\tilde{x}}= 1.2. 
$$ 
Para un conjunto de valores puede haber más de una moda.

En R, no hay un "paquete base" para extraer la moda. Pero si se puede hacer una función, o bien, usar paquetes. Veamos







```{r}
#| label: eg-mode
#| collapse: true
# Moda en el caso de datos unimodales discretos
x <- c(1, 5, 1, 6, 2, 1, 6, 7, 1)

#Función para calcular la moda
mode <- function(x) {
   return(as.numeric(names(which.max(table(x)))))
}

mode(x)

options(scipen = 999) # Para desactivar notación cientifica

mode(casen2022$ytrabajocor)

# Moda con mfv() de modeest
pacman::p_load(modeest)

modeest::mfv(casen2022$ytrabajocor, na_rm = TRUE)

```






Como se puede observar, la moda de los ingresos del trabajo es de 400.000 CLP. Es decir, que el valor que más se repite entre los salarios de los encuestados es de 400 lucas. 


### 1.1.1. Robustez de las medidas de localización

Ahora bien, **¿cuán robustas son las medidas de localización?** Un tema a considerar es cuál es el impacto de observaciones aberrantes en las medidas de localización, es decir, qué impacto tiene la presencia de una fracción pequeña de observaciones con errores muy grandes en su registro o que son atípicas por otro motivo en el valor de las medidas de localización.

Hablaremos indistintamente de observaciones aberrantes, observaciones extrañas y *outliers*. Si el número de observaciones es pequeño, se puede investigar uno por uno los valores aberrantes y corregir, reemplazar o eliminar las observaciones en cuestión. Sin embargo, cuando el número de observaciones es más grande, uno quisiera medidas de localización que sean insensibles a la presencia de una fracción moderada de observaciones extrañas y que, al mismo tiempo, sean un buen resumen numérico de la tendencia central de los datos. Analicemos, pues, cuán robusta son estas medidas. 
  
#### Robustez de la media

Supongamos que los datos son 
$$
x_1 = 0.7;\ x_2 = 1.2;\ x_3 = 0.9;\ x_4 = 0.6;\ x_5 = 1.4;\ x_6 = 1.8;\ x_7 = 2.5.
$$
Como vimos la media viene dada por $\bar{x} = 1.3$. Intuitivamente la media **no** es robusta, pues un solo outlier puede tener un efecto devastador sobre su valor. En efecto, si una de las observaciones, digamos la primera, es reportada con un error de medición $e$, de modo que 
$$
x_1 = 0.7 + e
$$
entonces la media, como función del error $e$, será:
$$
\bar{x}(e) = 1.3 + \frac{e}{7}
$$ 
y tenemos que
$$
\lim_{e \to \infty} \bar{x}(e) = \infty, \quad \lim_{e \to -\infty} \bar{x}(e) = -\infty.
$$ 
En el caso más general tenemos $\bar{x}(e) = \bar{x}(0) + \frac{e}{n}$, de modo que esta expresión tiende a $\pm\infty$ cuando $e$ tiende a $\pm\infty$.

En suma, la media es una medida de localización central **muy** sensible a valores atípicos/*outliers*. En este sentido, a pesar de que indica información, hay que tener cuidado y en general suele ser complementada con otras medidas, tal como vimos con la mediana.  
  
#### Robustez de la mediana
  
Como vimos, los *estadísticos de orden* de las observaciones anteriores están dadas por: 
$$
y_1 = 0.6, \quad y_2 = 0.7, \quad y_3 = 0.9, \quad y_4 = 1.2, \quad y_5 = 1.4, \quad y_6 = 1.8, \quad y_7 = 2.5.
$$ 
Vimos también que la mediana, que denotamos por $\tilde{x}$, viene dada por: 
$$
\tilde{x} = y_4 = 1.2.
$$ 
Ahora, suponiendo nuevamente que la primera observación se midió con error $e$, de modo que $x_1 = 0.7 + e$, y denotando por $\tilde{x}(e)$ la mediana cuando $x_1$ se reemplaza por $x_1 + e$, tenemos 
$$
\tilde{x}(e) =
\begin{cases}
    1.2 & \text{si } e \leq 0.5, \\
    0.7 + e & \text{si } 0.5 < e \leq 0.7, \\
    1.4 & \text{si } e > 0.7.
\end{cases}
$$ 
En el caso general con $n = 2m+1$ observaciones, donde elegimos tamaño impar solo para simplificar las expresiones, tendremos que $\tilde{x}(e) \in [y_m, y_{m+2}]$. Por tanto, la influencia de un valor aberrante en la mediana está acotada. Es decir, la **mediana es una medida robusta**.   

#### Robustez de la moda  

Supongamos que los datos están dados por:
$$
x_1 = 0.7, \quad x_2 = 1.2, \quad x_3 = 1.2, \quad x_4 = 0.6, \quad x_5 = 1.4, \quad x_6 = 1.8, \quad x_7 = 99.9.
$$ 
Entonces la moda viene dada por $\tilde{\tilde{x}} = 1.2$.

Suponiendo nuevamente que la primera observación se midió con error $e$, de modo que $x_1 = 0.7 + e$, y denotando por $\tilde{\tilde{x}}(e)$ la moda cuando $x_1$ se reemplaza por $x_1 + e$, tenemos que:
$$
\lim_{e \to \infty} \tilde{\tilde{x}}(e) = 1.2, \quad \lim_{e \to -\infty} \tilde{\tilde{x}}(e) = 1.2.
$$ 
En cambio, si la observación con error es la segunda observación, de modo que se midió $x_2 = 1.2 + e$, para $e = 0.6$, la moda será $1.8$. El cambio en la moda puede ser mucho mayor, existen valores de $e$ para los cuales la moda es $99.9$. En contraste, no existen valores de $e$ para los cuales la mediana cambie tanto.En suma, la moda es **muy** sensible a la presencia de *outliers*, no obstante va depende en qué localización de los datos se encuentre. Entonces, no siempre es muy sensible, pero a veces sí.   
  
#### Sintesis 

Como se desprende de las expresiones derivadas en los apartados anteriores, el impacto de valores extremos o atípicos es acotado tanto para la mediana como para la moda, mientras que para la media crece sin límite cuando $|e|$ tiende a infinito.Al mismo tiempo, la mediana pareciera ser más robusta que la moda, pues la influencia de errores de medición es menor. La moda pareciera estar entremedio, el impacto de outliers es acotado pero puede ser mucho más grande que su impacto en la mediana. Entonces, **¿media, mediana o moda?**

En general, la moda se usa poco para resumir en una cifra la localización de los datos. Probablemente, porque es “demasiado localizada”. Por ejemplo, puede que la moda esté en 0, porque un 10% de las observaciones son iguales a cero, pero la mayoría de los datos sean mayores que 100. En casos como este la moda captura muy mal la localización de los datos, pues uno quisiera algún número mayor que 100, no cero. La media es la medida de localización más utilizada en la práctica, seguida de la mediana. Esto nos lleva a considerar las siguientes ventajas y desventajas

- **Mediana**: más robusta.

- **Media**: usa toda la información. Al menos intuitivamente, la robustez de la mediana tiene como contraparte que usa poco la información disponible.
  
Entonces, la media entrega más información, pero es menos robusta, en cambio la mediana sufre la ventaja/desventaja opuesta. La digresión anterior motiva considerar medidas de localización más robustas que la media y que usan más información que la mediana. Con ese objetivo consideramos a continuación **las medias podadas**.  
  
### Medias podadas 

Supongamos que los datos están dado por:
$$
x_1 = 0.7, \; x_2 = 1.2, \; x_3 = 0.9, \; x_4 = 0.6, \; x_5 = 1.4, \; x_6 = 1.8, \; x_7 = 2.5,
$$ 
Como vimos, sus estadísticos de orden correspondientes son:
$$
y_1 = 0.6, \; y_2 = 0.7, \; y_3 = 0.9, \; y_4 = 1.2, \; y_5 = 1.4, \; y_6 = 1.8, \; y_7 = 2.5.
$$

Definimos la siguiente medida de localización:
$$
\bar{x}^p = \frac{1}{5}(y_2 + y_3 + \dots + y_6).
$$ 
Es decir, primero **podamos** los datos de la muestra ordenada de menor a mayor, removiendo un valor de cada extremo y luego calculamos el promedio de las observaciones podadas. El estadístico descriptivo que resulta se conoce como *media podada*, de nivel $k=1$ porque removimos una observación de cada extremo. Lo denominamos por $\bar{x}^p$. Para las observaciones en cuestión tenemos que la media podada de nivel 1 es 1,2.

En síntesis, se eliminan los valores extremos, el más pequeño y el más grande en este caso. Y después calculamos el promedio. Con ello, se le agrega mayor robustez y estaremos usando más información que usando la mediana. Veamos el caso general, no solo para $k=1$

#### Medias podadas: Caso General 

:::{.definition}
 Sean $x_1, \dots, x_n$ las observaciones y $y_1, \dots, y_n$ los estadísticos de orden correspondientes. Entonces, para $k < n/2$ definimos la media podada de nivel $k$ mediante: 
$$
    \bar{x}^p = \frac{1}{n - 2k} \sum_{i = k+1}^{n-k} y_i.
$$
:::
  

En palabras: dadas observaciones $x_1, \dots, x_n$, la media podada de nivel $k$ será el promedio de los valores que se obtiene luego de remover $k$ observaciones de cada uno de sus extremos (las $k$ más grandes y las $k$ más pequeñas). Las medias podadas de nivel $k = 1$ —se remueve el mínimo y máximo— se utilizan para pasar de las evaluaciones individuales de los jurados en varios deportes, entre ellos nado sincronizado y gimnasia, a la evaluación agregada.

En R, esto se puede hacer de dos formas, con una función o con `mean()` y especificaciones: 







```{r}
#| label: eg-media-podada
#| collapse: true
# Datos de ejemplo
x <- c(0.7, 1.2, 0.9, 0.6, 1.4, 1.8, 2.5)

# 1) Función manual para media podada de nivel k
media_podada <- function(x, k) {
  n <- length(x)
  if (2*k >= n) stop("k debe ser menor que n/2")
  y <- sort(x)
  mean(y[(k + 1):(n - k)])
}

# Cálculo de la media podada para k = 1
media_podada(x, 1)

# 2) Usando la función base mean() con el argumento trim
#    trim espera la fracción a recortar por cada extremo: k/n
k <- 1
n <- length(x)
mean(x, trim = k / n)

# Media podada de los ingresos del trabajo
mean(casen2022$ytrabajocor, 
     trim = k/n,
     na.rm=TRUE)

```







### Medias Windorizadas (opcional) 

Esto es una medición opcional en el sentido de que no se usa mucho. Pero puede ser útil. Supongamos que los datos están dado por:
$$
x_1 = 0.7; \, x_2 = 1.2; \, x_3 = 0.9; \, x_4 = 0.6; \, x_5 = 1.4; \, x_6 = 1.8; \, x_7 = 2.5.
$$ 
Como vimos, sus estadísticos de orden correspondientes son: 
$$
y_1 = 0.6, \, y_2 = 0.7, \, y_3 = 0.9, \, y_4 = 1.2, \, y_5 = 1.4, \, y_6 = 1.8, \, y_7 = 2.5.
$$
Definimos la siguiente medida de localización:
$$
\bar{x}^p = \frac{1}{7} [y_2 + y_2 + y_3 + y_4 + y_5 + y_6 + y_6].
$$

Es decir, ordenadas las observaciones de menor a mayor, sustituimos un valor de cada extremo por el valor inmediatamente anterior o posterior y luego calculamos el promedio de todos los valores. El estadístico descriptivo que resulta se conoce como **media winsorizada**, de nivel $k = 1$ porque sustituimos una observación de cada extremo. Lo denotamos por $\bar{x}^w$. Para las observaciones en cuestión tenemos que la media winsorizada de nivel 1 es 1,21 (aproximadamente).

:::{.definition}

**Medias Winsorizadas: Caso General**

Sean $x_1, \dots, x_n$ las observaciones y $y_1, \dots, y_n$ los estadísticos de orden correspondientes. Entonces, para $k < n/2$ definimos la media winsorizada de nivel $k$ mediante:
$$
  \bar{x}^w = \frac{k y_{k+1} + \sum_{i=k+1}^{n-k} y_i + k y_{n-k}}{n}.
$$
:::

En palabras: dadas observaciones $x_1, \dots, x_n$, la media winsorizada de nivel $k$ será el promedio de los valores que se obtiene luego de sustituir $k$ observaciones de cada uno de sus extremos (las $k$ más grandes y las $k$ más pequeñas) por el valor inmediatamente anterior o posterior. En R, se puede hacer con funciones nuevamente, o con el paquete `DescTools`.







```{r}
#| label: eg-media-winsorizada
#| collapse: true
# Datos de ejemplo
x <- c(0.7, 1.2, 0.9, 0.6, 1.4, 1.8, 2.5)

# 1) Función winsorizada + argumento na.rm
media_winsorizada <- function(x, k, na.rm = FALSE) {
  if (na.rm) x <- x[!is.na(x)]
  n <- length(x)
  if (2*k >= n) stop("k debe ser menor que n/2")
  y <- sort(x)
  # sustituir k extremos
  y[1:k]           <- y[k + 1]
  y[(n - k + 1):n] <- y[n - k]
  mean(y)
}

# Cálculo de la media winsorizada para k = 1
media_winsorizada(x, 1)

media_winsorizada(casen2022$ytrabajocor, 1,
                  na.rm=TRUE)


# 2) Usando la función Winsorize() del paquete DescTools
pacman::p_load(DescTools)

#  Vector limpio de NAs
x <- na.omit(casen2022$ytrabajocor)

# 3) Definimos k y calculamos n
k <- 1
n <- length(x)

# 4) Calculamos los cuantiles que usarán de umbrales
umbrales <- quantile(x, probs = c(k/n, 1 - k/n), na.rm = TRUE)

# 5) Winsorizamos usando 'val = umbrales'
x_wins <- Winsorize(x, val = umbrales)

# 6) Y finalmente sacamos la media
mean(x_wins)

```







#### Síntesis 
  
¿Qué hacemos en la práctica?

- Si se desea resumir la localización de los datos, parta por calcular la media, mediana y moda, así como también las medias podadas para algunos valores de $k$ (por ejemplo, $k/n \simeq 0,05$ y $k/n \simeq 0,1$)

- Si obtiene valores similares, reportar la media y mediana

- Si hay diferencias importantes, mire los datos con cuidado, entienda de dónde vienen las diferencias y luego decida qué hacer.

## 1.2. Medidas de dispersión
  
### La varianza

Las medidas de dispersión lo que buscan es determinar cuán dispersos están los datos con respectos a una medida de tendencia central. Una de ellas es la varianza. La **varianza** es una medida que resume la **dispersión de las observaciones _con respecto_ a su media**.
  
:::{.definition}
Dadas $n$ observaciones, $x_1, \ldots, x_n$, la varianza de este conjunto de datos, que denotaremos por $\hat{\sigma}^2$, está dada por:
$$
\begin{aligned}
\hat{\sigma}^2 &= \frac{(x_1 - \bar{x})^2 + (x_2 - \bar{x})^2 + \dots + (x_n - \bar{x})^2}{n}\\
&= \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2 \\
&= \frac{1}{n} \sum_{i=1}^{n} x_i^2 - \bar{x}^2.
\end{aligned}
$$

:::  
  
Por ejemplo, si tuvieramos que
$$
x_1 = 0.7; \quad x_2 = 1.2; \quad x_3 = 0.9; \quad x_4 = 0.6; \quad x_5 = 1.4; \quad x_6 = 1.8; \quad x_7 = 2.5,
$$
entonces, la varianza de estos datos sería 
$$
\begin{aligned}
  \hat{\sigma}^2 = \frac{(0.7 - 1.3)^2 + (1.2 - 1.3)^2 + (0.9 - 1.3)^2}{7} \\
  + \frac{(0.6 - 1.3)^2 + (1.4 - 1.3)^2 + (1.8 - 1.3)^2 + (2.5 - 1.3)^2}{7} = 0.39.
\end{aligned}
$$
  
Ojo,cuando hay muchos datos, la fórmula más simple es el promedio de las observaciones al cuadrado, menos la media al cuadrado. O sea, la última igualdad que pusimos anteriormente, es decir,  
$$
  \hat{\sigma}^2 =\frac{1}{n} \sum_{i=1}^{n} x_i^2 - \bar{x}^2.
$$
Esta, sin embargo, es una demostración de la primera sumatoria, aunque bastante trivial.
 
:::{.proof}
Si
$$
\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2 = \frac{1}{n} \sum_{i=1}^n x_i^2 - \bar{x}^2
$$
En efecto,
$$
\begin{aligned}
\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2 &= \frac{1}{n} \sum_{i=1}^n \left( x_i^2 - 2x_i \bar{x} + \bar{x}^2 \right)\\
&= \frac{1}{n} \sum_{i=1}^n x_i^2 - 2 \bar{x} \frac{1}{n} \sum_{i=1}^n x_i + \frac{1}{n} \sum_{i=1}^n \bar{x}^2\\
&= \frac{1}{n} \sum_{i=1}^n x_i^2 - 2 \bar{x}^2 + \bar{x}^2\\
&= \frac{1}{n} \sum_{i=1}^n x_i^2 - \bar{x}^2. 
\end{aligned}
$$

::: 
  
En R es mucho más fácil que estos cálculos tediosos. Esto se hace con el comando de R Base `var()`. No obstante, hay que tener en cuenta que `var()` es por defecto la varianaza muestra. Asumiendo que tenemos datos poblaciales podemos hacer dos cosas^[Esto no lo haré para los datos muestrales de la CASEN 2022. Principalmente, porque para eso usaremos, más adelante, paquetes especializados, como `survey` y `srvyr`]. Veamos cómo: 
  






```{r}
#| label: eg-varianza
#| collapse: true
# Datos de ejemplo
x <- c(0.7, 1.2, 0.9, 0.6, 1.4, 1.8, 2.5)

# Varianza poblacional
var_poblacional <- mean((x - mean(x))^2)

# Varianza simplificada
var_pobl_simplificada <- mean(x^2) - mean(x)^2

# Varianza muestral
var_muestral <- var(x)

# Varianza muestral de casen2022$ytrabajocor
var_casen_muestral <- var(casen2022$ytrabajocor, na.rm = TRUE)

# Mostrar resultados
var_poblacional
var_pobl_simplificada
var_muestral
var_casen_muestral

```







### La desviación estándar

Una vez se tiene la varianza, la desviación estándar es muy fácil de calcular, pues es simplemente la raíz cuadrada de la varianza. La *desviación estándar* también es una medida de dispersión de los elementos del conjunto de observaciones con respecto a su media. 

Dadas $n$ observaciones, $x_1, \ldots, x_n $, la desviación estándar de este conjunto de datos, que denotaremos por $\hat{\sigma}$, está dada por la raíz cuadrada de la varianza:
$$
\hat{\sigma} = \sqrt{\hat{\sigma}^2} = \sqrt{\frac{(x_1 - \bar{x})^2 + (x_2 - \bar{x})^2 + \ldots + (x_n - \bar{x})^2}{n}} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2}.
$$
Por ejemplo, si 
$$
x_1 = 0.7; \, x_2 = 1.2; \, x_3 = 0.9; \, x_4 = 0.6; \, x_5 = 1.4; \, x_6 = 1.8; \, x_7 = 2.5,
$$
entonces, la desviación estándar de estos datos es: $\hat{\sigma} = 0.62$. A diferencia de lo que ocurre con la varianza, la desviación estándar está medida en las mismas unidades que el conjunto de observaciones bajo análisis. Como veremos más adelante, esto hace que su interpretación sea más fácil y explica por qué se usa más que la varianza como medida de dispersión. Por lo tanto, la ventaja principal es esa. La desviación estándar está medida en las mismas unidades que las observaciones que estamos trabajando, y no así la varianza. En R es muy simple de hacer, con `sd()`. Al igual que con `var()`, `sd()` calcula por defecto al desviación estándar muestral. 







```{r}
#| label: eg-desviacion-estandar
#| collapse: true
# Datos de ejemplo
x <- c(0.7, 1.2, 0.9, 0.6, 1.4, 1.8, 2.5)

# Desviación estándar poblacional
sd_poblacional <- sqrt(mean((x - mean(x))^2))

# Desviación estándar simplificada
sd_simplificada <- sqrt(mean(x^2) - mean(x)^2)

# Desviación estándar muestral
sd_muestral <- sd(x)

# Desviación estándar muestral de casen2022$ytrabajocor
sd_casen_muestral <- sd(casen2022$ytrabajocor, na.rm = TRUE)


# Mostrar resultados
sd_poblacional
sd_simplificada
sd_muestral
sd_casen_muestral


```







Considerando la desviación estándar es una medida de dispersión que describe la descripción de los datos en las mismas unidades que el conjunto de observaciones nos sirve para comparar ahora `var_casen_muestral` con `sd_casen_muestral`. En el primer caso, `var_casen_muestral = 665844368478`, significa simplemente que la varianza de los datos es de ese valor, esa es su varianza. En cambio, `sd_casen_muestral = 815992.9` implica que la variación de los datos es de 815.992 CLP, pues la variable `ytrabajocor` mide pesos chilenos. En el caso de la varianza, nos dice cuán dispersos están los datos respecto a la media. En el caso de la desviación estándar, es también una medida de dispersión de los elementos del conjunto de observaciones con respecto a su media, pero que está en las unidades de la variable en cuestión, es decir, que hay una dispersión de los datos de 815.992 CLP respecto de la media (667.690 CLP, como habíamos visto). Por cierto, los valores elevados de dispersión son típicos en los ingresos, pues dada la desigualdad en estos, suelen tener una alta dispersión los datos de ingresos. 



### 1.2.1. Robustez en las medidas de dispersión 

Ahora bien, **¿cuál es más robusta?** La *desviación estándar muestral* y la *varianza muestral* no son medidas robustas de la dispersión de los datos. Aunque el álgebra es un poco más complejo, el problema es similar al que vimos con la media muestral.

Por ejemplo, denotando por $\hat{\sigma}^2(e)$ la varianza muestral cuando $x_1$ se mide con error $e$, se tiene que, usando el resultado de que la varianza muestral es igual a la media muestral de los cuadrados menos el cuadrado de la media muestral:

:::{.proof}

$$
\begin{aligned}
\hat{\sigma}^2(e) &= \frac{1}{n} \left[ (x_1 + e)^2 + \sum_{i=2}^{n} x_i^2 \right] - \left( \frac{1}{n} \left[ (x_1 + e) + x_2 + \ldots + x_n \right] \right)^2 \\
&= \frac{1}{n} \sum_{i=1}^{n} x_i^2 + \frac{x_1 e}{n} + \frac{e^2}{n} - \left( \bar{x} + \frac{e}{n} \right)^2 \\
&= \frac{1}{n} \sum_{i=1}^{n} x_i^2 + \frac{x_1 e}{n} + \frac{e^2}{n} - \bar{x}^2 - 2 \bar{x} \frac{e}{n} - \frac{e^2}{n^2} \\
&= \hat{\sigma}^2(0) + \frac{2}{n}(x_1 - \bar{x})e + \frac{n-1}{n^2}e^2.
\end{aligned}
$$
De donde se concluye que 
$$
\lim_{e \to \pm \infty} \hat{\sigma}^2(e) = +\infty,
$$
y, por lo tanto, también que 
$$
\lim_{e \to \pm \infty} \hat{\sigma}(e) = +\infty.
$$

::: 

En simple, las dos medidas de dispersión, la varianza y la desviación estándar, tienden a infinito positivo cuando el término de error tiende - o + infinito. Por lo tanto, ambas son medidas que no son robustas y, por lo tanto, sensibles a la presencia de valores extremos, de *outliers*. 

### El rango

El *rango* es la diferencia entre los valores **más grande** y **más pequeño** de un conjunto de observaciones de una variable. Al tomar el valor máximo y mínimo de los datos, permite obtener una idea de la *dispersión* de los datos. Así, cuanto mayor es el rango, aún más dispersos están los datos. Por ejemplo, si
$$
x_1 = 0.7; \quad x_2 = 1.2; \quad x_3 = 0.9; \quad x_4 = 0.6; \quad x_5 = 1.4; \quad x_6 = 1.8; \quad x_7 = 2.5,
$$
entonces, como ya vimos:
$$
y_1 = 0.6, \quad y_2 = 0.7, \quad y_3 = 0.9, \quad y_4 = 1.2, \quad y_5 = 1.4, \quad y_6 = 1.8, \quad y_7 = 2.5.
$$
Entonces el rango está dado por:
$$
y_7 - y_1 = 2.5 - 0.6 = 1.9.
$$

Como ya se puede intuir a estas alturas, el rango tampoco es una medida de dispersión robusta, pues basta un *outlier* en los valores mínimos o máximos para que el rango se distorsiones. Así, aunque el rango de un conjunto de datos es simple de calcular, su valor no es muy útil porque depende fuertemente de valores extremos y, por tanto, no es robusto a la presencia de *outliers*. Formalmente, si tomamos el ejemplo anterior, tendríamos que 
$$
\begin{aligned}
y_1 = 0.6, \quad y_2 = 0.7, \quad y_3 = 0.9, &\quad y_4 = 1.2, \quad y_5 = 1.4, \quad y_6 = 1.8, \quad y_7 = 2.5.\\
y_7 - y_1 = &2.5 - 0.6 = 1.9.
\end{aligned}
$$
Ahora bien, suponiendo nuevamente que la primera observación se midió con error $e$, de modo que $x_1 = 0.7 + e$, y denotando por $y_7(e)$ y $y_1(e)$ al valor máximo y mínimo cuando $x_1$ se reemplaza por $x_1 + e$, respectivamente, tenemos que:
$$
\lim_{e \to -\infty}(y_7(e) - y_1(e)) = \infty, \quad \lim_{e \to +\infty}(y_7(e) - y_1(e)) = \infty.
$$
Por lo que valores aberrantes o *outliers* tienen efectos brutales en el rango. En R es muy fácil de hacer, se puede "manualmente" o con `range()` y `diff()`. Lo mostramos y luego pasamos a la primera medida de dispersión robusta: el rango intercuartil. 







```{r}
#| label: eg-rango
#| collapse: true


# Para obtener el rango (max–min) usa diff(range(x, na.rm=TRUE))
x <- c(0.7, 1.2, 0.9, 0.6, 1.4, 1.8, 2.5)

# Rango con max–min
r1 <- max(x) - min(x)

# Rango con diff(range())
r2 <- diff(range(x, na.rm = TRUE)) # range(x, na.rm=TRUE) devuelve c(min, max), no la diferencia

# En tu variable de casen2022
r_casen_1 <- max(casen2022$ytrabajocor, na.rm = TRUE) - 
             min(casen2022$ytrabajocor, na.rm = TRUE)
r_casen_2 <- diff(range(casen2022$ytrabajocor, na.rm = TRUE))

# Mostrar resultados
r1
r2
r_casen_1
r_casen_2


```







### El rango intercuartil

El **primer cuartil** de un conjunto de observaciones de una variable, que denotaremos por $q_1$, es el valor para el cual el 25\% de las observaciones son más pequeñas y el 75\% son más grandes. En otras palabras, el primer cuartil es la mediana del 50\% más pequeño de los datos.

El **segundo cuartil**, $q_2$, es el mismo valor que aquel de la mediana (50\% son más pequeños, 50\% son más grandes).

El **tercer cuartil**, $q_3$, es el valor para el cual el 25\% de las observaciones son más grandes y el 75\% son más pequeñas. En otras palabras, el tercer cuartil es la mediana del 50\% más grande de los datos.

El **rango intercuartil**, que denotaremos \textbf{RIC}, está dado por: $q_3 - q_1$.

Notar que el RIC es el **rango** del conjunto de observaciones con los cuartos más pequeño y más grande eliminados (*podados*), similar en espíritu a las observaciones que se consideran para calcular la media podada.Entonces, formalicemos su robustez.

Por ejemplo, supongamos que las observaciones (ordenadas de menor a mayor) son:
$$
43, 48, 50, 50, 52, 53, 56, 58, 59, 60, 62, 65, 66, 68, 70, 71, 74, 76, 78, 80,
$$
entonces los cuartiles de este conjunto de observaciones son: 
$$
q_1 = 52.5, \quad q_2 = 61, \quad q_3 = 70.5
$$
y el RIC viene dado por $q_3 - q_1 = 18$

Ahora bien, suponiendo que el valor 43 se midió con un error $e$, de modo que su valor es $43 + e$, y denotando por $RIC(e)$ el valor del rango intercuartil cuando el valor 43 se reemplaza por $43 + e$, tenemos que: 
$$
RIC(e) \in [16,18]
$$ 
De modo que **el RIC es una medida de dispersión de los datos robusta a la presencia de _outliers_**. Para calcular cuartiles en R se usa `quantile()`. Lo importante es especificar adecuadamente el cuantil de interés (50%, 75%, etc.) A su vez, entre los paquete de R base, específicamente en `stats`, se tiene `IQR()` nos permite calcular rangos intercuartiles. 







```{r}
#| label: eg-IQR
#| collapse: true
# Datos de ejemplo
x <- c(43, 48, 50, 50, 52, 53, 56, 58, 59, 60, 62, 65, 66, 68, 70, 71, 74, 76, 78, 80)

# Calcular cuartiles
q1   <- quantile(x, 0.25)
q2   <- median(x)
q3   <- quantile(x, 0.75)

# Rango intercuartil
ric  <- q3 - q1

# Para casen2022$ytrabajocor
x_casen    <- casen2022$ytrabajocor
q1_casen   <- quantile(x_casen, 0.25, na.rm = TRUE)
q2_casen   <- median(x_casen, na.rm = TRUE)
q3_casen   <- quantile(x_casen, 0.75, na.rm = TRUE)
ric_casen  <- q3_casen - q1_casen

# Alternativa con función base
ric_casen2 <- IQR(x_casen, na.rm = TRUE)

# Mostrar resultados
q1; q2; q3; ric
q1_casen; q2_casen; q3_casen; ric_casen; ric_casen2

```








## 1.3. Medidas de posición

### Los percentiles 

El percentil de un conjunto de observaciones es una medida de posición que indica, una vez ordenados los datos de menor a mayor, el valor por debajo del cual se encuentra un porcentaje dado de observaciones. En general, el percentil $k$-ésimo, que denotaremos por $p_k$, es un valor para el cual un $k\%$ por ciento de las observaciones son menores que ella.

La definición anterior es un tanto vaga: ¿menor o igual?, ¿qué pasa si no hay valores que cumplan?, ¿qué pasa si hay varios? Se pueden adoptar varios criterios, todos los cuales arrojan resultados parecidos cuando se aplican con conjuntos grandes de observaciones.

Notar que, en general, $p_{25} = q_1$, $p_{50}$ es la mediana y $p_{75} = q_3$. Por ejemplo, si las observaciones (ordenadas de menor a mayor) son: 
$$
43, 48, 50, 50, 52, 53, 56, 58, 59, 60, 62, 65, 66, 68, 70, 71, 74, 76, 78, 80,
$$
y exigimos que los percentiles sean observaciones y la definición es con "menor o igual", tenemos: 
$$
p_{10} = 48, \quad p_{25} = 52, \quad p_{50} = 60, \quad p_{75} = 68, \quad p_{100} = 76.
$$
En R no hay una distinción entre cuantiles y percentiles a nivel de comando. Ahora bien, para obtener los percentiles en la posición que nos interesan podemos hacer lo siguiente: 







```{r}
#| label: eg-percentiles
#| collapse: true
# Datos de ejemplo
x <- c(43, 48, 50, 50, 52, 53, 56, 58, 59, 60, 62, 65, 66, 68, 70, 71, 74, 76, 78, 80)

# Definimos percentiles deseados
p <- c(0.10, 0.25, 0.50, 0.75, 1.00)

# Calculamos percentiles como observaciones (type = 1)
percentiles <- quantile(x, probs = p, na.rm = TRUE, type = 1)
p10   <- percentiles[1]
p25   <- percentiles[2]
p50   <- percentiles[3]
p75   <- percentiles[4]
p100  <- percentiles[5]

# Para casen2022$ytrabajocor
x_casen <- casen2022$ytrabajocor
percentiles_casen <- quantile(x_casen, probs = p, na.rm = TRUE, type = 1)
p10_casen  <- percentiles_casen[1]
p25_casen  <- percentiles_casen[2]
p50_casen  <- percentiles_casen[3]
p75_casen  <- percentiles_casen[4]
p100_casen <- percentiles_casen[5]

# Mostrar resultados
percentiles
percentiles_casen

```







## 1.4. Transformaciones lineales de los datos descriptivos 

Una transformación lineal de un conjunto de datos es aquella en la que cada observación se multiplica por una constante y se le suma otra constante. Concretamente, dadas $n$ observaciones, $x_1, x_2, \dots, x_n$, una transformación lineal de ellas está dada por 
$$
ax_1 + b, ax_2 + b, \dots, ax_n + b,
$$ 
con $a$ y $b$ constantes cualesquiera. 

De tal modo, las transformaciones lineales permiten transformar los datos de las estadísticas descriptivas transformadas linealmente con las estadísticas descriptivas originales. Un ejemplo de esto, podría ser la transformación de los datos de precios a los mismos precios en UF de algún día dado. Con ello, podríamos ver, e.g., el ingreso en pesos y queremos verlo en UF. De tal modo, podríamos comprar el ingreso original en pesos con el ingreso en UF sin distorsionar la proporción mediante esta multiplicación con constantes (ponderar en la misma cantidad básicamente). En nuestro ejemplo, de CLP $\to$ UF tendríamos que la constante $a$ sería el valor de la UF y la constante $b$ simplemente sería 0.  

Lo que se sigue de esto, es determinar la relación entre las medidas que hemos visto hasta hora, en sus estadísticas descriptivas originales, y compararlas con sus respectivas transformaciones lineales. 

- Si denotamos por $\bar{x}$ a la *media* de los valores de las observaciones originales y por $\bar{x}_t$ a la media de los datos después de aplicarles la transformación lineal, tenemos que 
$$
\bar{x}_t = \frac{ax_1 + b + ax_2 + b + \dots + ax_n + b}{n} = a\bar{x} + b.
$$

- Si denotamos por $\hat{\sigma}^2$ a la *varianza* de los valores de las observaciones originales y por $\hat{\sigma}^2_t$ a la varianza de los datos después de aplicarles la transformación lineal, tenemos que
$$
\hat{\sigma}^2_t = \frac{(ax_1 + b - \bar{x}_t)^2 + (ax_2 + b - \bar{x}_t)^2 + \dots + (ax_n + b - \bar{x}_t)^2}{n} = a^2 \hat{\sigma}^2.
$$
- De modo que para el caso de la *desviación estándar*, tenemos que: 
$$
\hat{\sigma}_t = |a| \hat{\sigma}.
$$
- Si denotamos por $\tilde{\tilde{x}}$ a la *moda* de los valores originales y por $\tilde{\tilde{x}}_t$ a la moda de los datos transformados, tenemos que: 
$$
\tilde{\tilde{x}}_t = a \tilde{\tilde{x}} + b.
$$
- Si denotamos por $y_1, y_2, \dots, y_n$ a los *estadísticos de orden* de los valores originales de los datos, tenemos que los estadísticos de orden de los datos transformados linealmente están dados por $a y_1 + b, a y_2 + b, \dots, a y_n + b$ si $a \geq 0$, o por $a y_n + b, a y_{n-1} + b, \dots, a y_1 + b$ si $a < 0$.
- Si denotamos por $\bar{x}$ a la *mediana* de las observaciones originales y por $\tilde{x}_t$ a la mediana de los datos después de aplicarles la transformación lineal, tenemos que: 
$$
\tilde{x}_t = a \tilde{x} + b.
$$
- El rango de los datos transformados está dado por $\left| a \right| (y_n - y_1)$.
- El rango intercuartil de los datos después de aplicarles la transformación lineal está dado por $\left| a \right| (q_3 - q_1)$, donde $q_3$ y $q_1$ son los cuartiles 3 y 1 de los datos originales, respectivamente.


# 2. Estadística descriptiva y visualización de los datos


La **visualización de datos** es otro paso importante en el *análisis exploratorio de datos* y, por tanto, para la *estadística descriptiva*. La **visualización de datos** es el proceso de resumir gráficamente la información, por lo que suelen ser fundamentales para las estadísticas descriptiva. Su utilidad radica en que nos ayuda a interpretar los datos disponibles y a detectar patrones, tendencias y anomalías en ellos. Además, la visualización de datos es una herramienta que nos permite comunicar claramente ideas complejas de una forma atractiva. Concretamente, el **gráfico** presenta lo que los números no pueden comunicar por sí mismos, y lo transmite de una manera visible y más fácil de asimilar y de recordar. Un **(buen) gráfico** vale más que mil palabras. Y el propósito principal de graficar los datos es **comunicar** información de los datos.

En R, existe el comando `plot()` en los paquetes bases. Pero se suele graficar con el paquete `ggplot2`. También hay paquetes basados en `ggplot2` pero que incorporan otro tipos de gramáticas, como `tidyplots` que se especializa en gráficos para artículos científicos. Por no ser un apunte sobre cómo usar paquetes, usaré `ggplot2`. También usaré `scales` que complementará `ggplot2` ajustando escalas. No obstante, ambos paquetes tienen múltiples páginas webs, tutoriales, etc., sobre cómo usarlos. También cargamos la CASEN 2022 nuevamente



## Tipos de gráficos 

La elección de la **visualización de datos* depende de varios elementos, pero principalmente de 

- Qué tipo de variable se quiere investigar/presentar.

- Lo que esperamos mostrar. 

Hay gráficos que son más adecuados para presentar ciertos tipos de datos, al igual que hay gráficos que no sirven para visualizar algunas variables según su naturaleza. Al mismo tiempo, dependerá de dónde y a quién se quiera presentar los gráficos. Dado que nuestra intención es *comunicar* información de los datos de manera gráfica, es importante tener en consideración si los gráficos se expondran en un artículo científico, una presentación en un congreso, en una afiche divulgativo, para una clase, etc. No obstante, aquí dejo una pauta general de qué gráficos usar según los tipos de variables y cuántas variables queramos comunicar. 

1. Los **gráficos univariados**: muestran la distribución básica de una variable. Para ello utilizamos 
  a. Gráficos de barras,
  b. Histogramas y
  c. Gráficos de caja (boxplots)
  
2. Los **gráficos bivariados**: muestran cómo dos variables están *relacionadas* entre sí. Cuál usar depende del tipo de variables bajo análisis:
  a. Dos variables categóricas: gráficos de barras.
  b. Una categórica y una cuantitativa: gráficos de cajas.
  c. Dos variables cuantitativas: gráficos de dispersión de puntos.
  
  
Veamos cómo graficar estos tipos de gráficos en R usando `ggplot2` y datos reales de la CASEN 2022.   

## Gráficos univariados

### Gráficos de barras







```{r eg-barplots, echo=TRUE, collapse=TRUE}
#| label: eg-barplots
#| collapse: true
# Vemos datos de nuestra variable de interés

sjmisc::frq(casen2022$sexo) # ups! no funciona en RMarkdown


# Gráfico de barras de la variable sexo
ggplot(casen2022, aes(x = factor(sexo, labels = c("Hombre", "Mujer")))) +
  geom_bar() +
  labs(
    x = "Sexo",
    y = "Frecuencia",
    title = "Distribución de la variable Sexo"
  ) +
  theme_minimal()

```







Si queremos agregarle más colores^[Yo uso las paletas de colores de [aquí](https://r-charts.com/es/colores/)], cambiar el tamaño de las letras, ponerle caption, etc., solo hay que usar comandos de `ggplot2` que sirvan para ello y seguir su gramática "en capas" adecuadamente 







```{r}
#| label: eg-barplots2
#| collapse: true

# Gráfico de barras de la variable sexo más bonito
ggplot(casen2022, aes(x = factor(sexo, labels = c("Hombre", "Mujer")), 
                      fill = factor(sexo))) +
  geom_bar(width = 0.7, color = "black") +
  scale_fill_manual(values = c("#8F8F8F", "#8B0000")) +
  labs(
    x = "Sexo",
    y = "Frecuencia",
    title = "Distribución de Sexo en CASEN 2022", 
    caption = "Elaboración propia a partir de Encuesta CASEN 2022"
  ) +
  theme_minimal(base_family = "serif", base_size = 12) +
  theme(
    plot.title     = element_text(size = 18, face = "bold", hjust = 0.5),
    axis.title     = element_text(size = 14),
    axis.text      = element_text(size = 12),
    panel.grid.minor = element_blank(),
    legend.position = "none"
  )

``` 






  
Más allá de "enchular" el gráfico, lo importante es que el gráfico es más fácil de comprender rápidamente que una tabla de frecuencias, como vimos con `frq()`. Y cuando se agrega más categorías, facilita aún más. Por ejemplo, veamos Clasificación Internacional Normalizada de Educación (CINE-F). Para ello usaremos `cinef13_area` de la CASEN 2022, pero excluiremos las categorías sin datos 







```{r}
#| label: eg-barplots3
#| collapse: true

#Exploramos nuestra variable de interés

frq(casen2022$cinef13_area)


# Como vemos Doctorado en Ciencias Básicas no tiene casos
# y Sin dato no nos interesa. Tampoco NA
# Usamos dplyr

area_labels <- c(
  "Salud y Bienestar",
  "Ingeniería, Industria y Construcción",
  "Educación",
  "Servicios",
  "Administración de Empresas y Derecho",
  "Ciencias Sociales, Periodismo e Información",
  "Ciencias naturales, matemáticas y estadística",
  "Agricultura, Silvicultura, Pesca y Veterinaria",
  "Tecnología de la Información y la Comunicación (TIC)",
  "Artes y Humanidades"
)

df_area <- casen2022 |> 
  filter(!cinef13_area %in% c(11, 88, NA)) |> 
  count(cinef13_area) |> 
  mutate(area = factor(cinef13_area, levels = 1:10, labels = area_labels))

ggplot(df_area, aes(x = area, y = n, fill = area)) +
  geom_col(color = "white", width = 0.8) +
  labs(
    x = "Área CINE-F",
    y = "Frecuencia",
    title = "Distribución de Áreas CINE-F (códigos 1–10)"
  ) +
  theme_minimal(base_family = "serif", base_size = 14) +
  theme(
    axis.text.x    = element_text(angle = 45, hjust = 1),
    plot.title     = element_text(face = "bold", size = 16, hjust = 0.5),
    legend.position = "none"
  )


```   






  
Esto es mucho más amable que una tabla en la que comparamos números, porque permite una comparación visual entre categorías. Entonces, la ganancia del gráfico de barra se da en la medida que haya más categorías a analizar. 
  

### Histogramas (y kernels)

Este tipo de visualizaciones es recomendable para resumir gráficamente la información de una **variable cuantitativa**. Un histograma muestra la distribución empírica de las observaciones de un variable cuantitativa. Muchas veces, por ejemplo, se usa para observar si una variable sigue o no una distribución normal. 

En un histograma, la altura de cada barra representa cuántas observaciones en el conjunto de datos caen dentro de un cierto rango (intervalo) de la variable cuantitativa bajo análisis. Un elemento fundamental al momento de hacer un histograma es la definición del número de rangos (intervalos) de la variable cuantitativa que se considerarán para agrupar los datos. Si no se elige el número correcto (ancho adecuado) para dichos intervalos, se corre el riesgo de que el histograma no refleje las características esenciales del conjunto de datos, llevándonos a malinterpretar cómo se distribuyen las observaciones. 
  
Veamos la distribución de los ingresos del trabajo en un histograma: 






```{r}
#| label: eg-histogram
#| collapse: true

ggplot(casen2022, aes(x = ytrabajocor)) +
  geom_histogram(bins = 30, fill = "#4E79A7", color = "white", na.rm = TRUE) +
  scale_x_continuous(labels = scales::comma) + # para sacar notación científica
  labs(
    x = "Ingreso de trabajo corregido",
    y = "Frecuencia",
    title = "Histograma de ytrabajocor"
  ) +
  theme_minimal(base_family = "serif", base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
    axis.title = element_text(size = 14),
    axis.text  = element_text(size = 12)
  )
```







Como se ve, la desigualdad de los ingresos nuevamente ataca. Al haber pocas personas con ingresos muy altos, y muchas personas con ingresos bajos, ni se alcanzan a ver las personas de ingresos altos. Ciertamente, esta forma de elefante dentro de una serpiente hacia un lado del gráfico es habitual verla en gráficos de distribución en los ingresos. Podemos para ver cómo varía esto, por ejemplo, usar percentiles, deciles, quintiles, etc., para filtar casos y observar la distribución empírica de los ingresos, e.g., para el 90% de los ingresos (excluyendo al 10% más alto):







```{r}
#| label: eg-histogram2
#| collapse: true

casen2022 |> 
  filter(!is.na(ytrabajocor), dau < 10) |> 
  ggplot(aes(x = ytrabajocor)) +
    geom_histogram(bins = 30, fill = "#4E79A7", color = "white") +
    scale_x_continuous(labels = comma) +
    labs(
      x = "Ingreso de trabajo corregido",
      y = "Frecuencia",
      title = "Histograma de ytrabajocor (deciles 1–9)"
    ) +
    theme_minimal(base_family = "serif", base_size = 14) +
    theme(
      plot.title     = element_text(face = "bold", size = 16, hjust = 0.5),
      axis.title     = element_text(size = 14),
      axis.text      = element_text(size = 12)
    )
```







¿Y si queremos, por ejemplo, para el 80%? Solo basta modificar `dau < x` en el código, pues `dau` es la variable del Decil autónomo nacional dentro de la CASEN 2022. 







```{r}
#| label: eg-histogram3
#| collapse: true

casen2022 |> 
  filter(!is.na(ytrabajocor), dau < 9) |> 
  ggplot(aes(x = ytrabajocor)) +
    geom_histogram(bins = 30, fill = "#4E79A7", color = "white") +
    scale_x_continuous(labels = comma) +
    labs(
      x = "Ingreso de trabajo corregido",
      y = "Frecuencia",
      title = "Histograma de ytrabajocor (deciles 1–8)"
    ) +
    theme_minimal(base_family = "serif", base_size = 14) +
    theme(
      plot.title     = element_text(face = "bold", size = 16, hjust = 0.5),
      axis.title     = element_text(size = 14),
      axis.text      = element_text(size = 12)
    )
```








Algo que también se puede hacer, similar a los histogramas, es observar esto con *kernels*, que son como histogramas pero con densidad y continuos (no en barras). Esto se hace con `geom_density()`. Además, esto nos facilita (para visualizar la distribución empírica) ver nuestra variable de interés agrupada según otra variable que nos interese. Por ejemplo, podríamso ver el ingreso del trabajo según sexo. 







```{r}
#| label: eg-kernels
#| collapse: true


casen2022 |> 
  filter(!is.na(ytrabajocor), dau < 10) |> 
  mutate(sexo = factor(sexo, levels = c(1, 2), labels = c("Hombre", "Mujer"))) |> 
  ggplot(aes(x = ytrabajocor, fill = sexo)) +
    geom_density(alpha = 0.7,adjust =2, color = "black") +
    scale_x_continuous(labels = comma) +
    labs(
      title = "Distribución de ingreso de trabajo corregido por sexo",
      x     = "Ingreso de trabajo corregido",
      y     = "Densidad",
      fill  = "Sexo"
    ) +
    theme_minimal(base_family = "serif", base_size = 12) +
    theme(
      plot.title     = element_text(face = "bold", size = 16, hjust = 0.5),
      legend.position = "bottom",
      axis.text       = element_text(size = 12)
    )

```








Otra cosa que puede resultarnos util es ver donde está mediana en el gráfico. Aprovechamos de mostrar como se verían los gráficos ya no superpuestos, sino al lado: 







```{r}
#| label: eg-kernels2
#| collapse: true


df <- casen2022 |>
  filter(!is.na(ytrabajocor), dau < 10) |>
  mutate(sexo = factor(sexo, levels = c(1, 2), labels = c("Hombre", "Mujer")))

# Extraemos medianas
medianas <- df |>
  group_by(sexo) |>
  summarise(mediana = median(ytrabajocor))

# Graficamos 
ggplot(df, aes(x = ytrabajocor, fill = sexo, color = sexo)) +
  geom_density(alpha = 0.3, adjust = 1, size = 1) +
  geom_vline(data = medianas, aes(xintercept = mediana, color = sexo),
             linetype = "dashed", size = 1) +
  facet_wrap(~ sexo) +
  scale_fill_manual(values = c("Hombre" = "#C51517", "Mujer" = "#BAB3EB")) +
  scale_color_manual(values = c("Hombre" = "#9C0824", "Mujer" = "#312271")) +
  scale_x_continuous(labels = comma) +
  labs(
    title = "Distribución de ingreso de trabajo corregido por sexo",
    x     = "Ingreso de trabajo corregido",
    y     = "",
    fill  = "Sexo",
    color = "Sexo"
  ) +
  theme_minimal(base_family = "serif", base_size = 12) +
  theme(
    plot.title     = element_text(face = "bold", size = 16, hjust = 0.5),
    legend.position = "bottom",
    axis.text       = element_text(size = 12)
  )

```







### Gráfico de cajas (boxplots)

Cuando son gráficos univariados este tipo de visualizaciones tipo *boxplot* es recomendable para resumir gráficamente la información de una **variable cuantitativa** (continua o discreta). Un grafico de caja o *boxplot* muestra las **características de la distribución** de las observaciones de una variable cuantitativa: “mínimo”, primer cuartil ($q_1$), mediana, tercer cuartil ($q_3$), “máximo”. También etiqueta posibles valores atípicos como puntos separados en el gráfico. En un gráfico de caja, pues, se tiene que:

- El “mínimo” corresponde a $q_1 - 1,5RIC$ (no es el valor más pequeño entre las observaciones).

- El “máximo” corresponde a $q_3 + 1,5RIC$ (no es el valor más grande entre las observaciones).

En esta herramienta gráfica, las observaciones etiquetadas como posibles valores atípicos (*outliers*) son aquellas que se ubican fuera del intervalo dado por:
$$
[q_1 - 1,5RIC, q_3 + 1,5RIC]
$$

Veamos como hacer esto con `ggplot2`. Usaremos otra variable cuantitativa, y ya no ingresos del trabajo porque, dada su dispersión se verá así: 







```{r}
#| label: eg-boxplot
#| collapse: true
# Para ingresos del trabajo


ggplot(casen2022, aes(y = ytrabajocor)) +
  geom_boxplot(fill = "#4E79A7", color = "black", outlier.color = "red", na.rm = TRUE) +
  scale_y_continuous(labels = comma) +
  labs(
    y     = "Ingreso de trabajo corregido",
    title = "Boxplot de ingreso de trabajo corregido"
  ) +
  theme_minimal(base_family = "serif", base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
    axis.text  = element_text(size = 12)
  )

```







Probemos con otra variable que sea interesante, por ejemplo con la varriable `tot_per_h` que registra el "Total de personas en el hogar" de los hogares encuestados por la CASEN







```{r}
#| label: eg-boxplot2
#| collapse: true


# Para Total de personas en el hogar

casen2022 |>
  filter(!is.na(tot_per_h)) |>
  ggplot(aes(y = tot_per_h)) +
    geom_boxplot(fill = "#CAE1FF", color = "black", outlier.color = "red") +
    scale_y_continuous(breaks = 1:13, limits = c(1, 13)) +
    labs(
      y     = "Total de personas en el hogar",
      title = "Boxplot de total de personas en el hogar"
    ) +
    theme_minimal(base_family = "serif", base_size = 14) +
    theme(
      plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
      axis.text  = element_text(size = 12)
    )
```







La intepretación del boxplot, entonces, debe incluir lo siguiente: 

- La mediana está en 3 miembros, lo que indica que la mitad de los hogares tiene 3 o menos personas y la otra mitad 3 o más.

- El primer cuartil ($Q1$) se sitúa en 2 personas y el tercer cuartil ($Q3$) en 4 personas. Esto nos dice que el 50% central de los hogares (entre $Q1$ y $Q3$) agrupa tamaños de 2 a 4 miembros.

- El rango intercuartílico ($RIC = Q3 – Q1$) es 2 personas, reflejando una variabilidad moderada en el tamaño típico de los hogares.

- Los “bigotes” se extienden desde 1 hasta 7 miembros aproximadamente, marcando los límites dentro de $1,5·RIC$.

- Más allá de 7 personas aparecen puntos aislados (*outliers*), correspondientes a hogares muy numerosos (8 a 13 personas), que representan casos excepcionales en la distribución.

En conjunto, el gráfico revela que la mayoría de los hogares chilenos encuestados están compuestos por entre 2 y 4 personas, con un hogar típico de 3 miembros, y que los tamaños extremos (hogares muy grandes) son poco frecuentes.


## Gráficos bivariados

### Gráficos de barras de dos variables

Esta visualización es recomendable para resumir gráficamente la información sobre la relación entre **dos variables categóricas**. Específicamente, este gráfico destaca cómo una variable categórica puede diferir entre las categorías de otra variable categórica. Por ejemplo, podríamos mostrar cómo la matrícula en la educación superior difiere por género.

Para ello, tomaré la variable `e6a_asiste` que responde a la pregunta ¿Cuál es el nivel educacional al que asiste?, dado que supone que asiste, al momento de ser encuestado/a, a un establecimiento educacional o no. En este caso, solo tomaremos la categoría `13` que es "13. Profesional (Carreras 4 o más años)". Aunque no es una variable sobre la matricula como tal, es un buen *proxy*^[Para ello, hay muchas bases de datos que si registran esta información. Además, el MINEDUC una buena plataforma de datos abiertos, pero para centrarnos, en este capítulo, en la CASEN] 







```{r}
#| label: eg-barplot-bivariado
#| collapse: true


# Creamos un df ad hoc
df_asiste_prof <- casen2022 |>
  filter(e6a_asiste == 13) |>
  count(sexo) |>
  mutate(sexo = factor(sexo, levels = c(1, 2), labels = c("Hombre", "Mujer")))

# Ploteamos
ggplot(df_asiste_prof, aes(x = sexo, y = n, fill = sexo)) +
  geom_col(width = 0.6, color = "white") +
  scale_y_continuous(labels = comma) +
  labs(
    x     = "Sexo",
    y     = "Número de estudiantes en carreras profesionales (4+ años)",
    title = "Asistencia a la universidad (Profesional) por sexo"
  ) +
  theme_minimal(base_family = "serif", base_size = 14) +
  theme(
    plot.title     = element_text(face = "bold", size = 16, hjust = 0.5),
    axis.text      = element_text(size = 12),
    legend.position = "none"
  )

```









### Boxplots bivariados 

Este tipo de visualización es recomendable para resumir gráficamente la información de la relación entre una *variable categórica* y una *variable cuantitativa*. Concretamente, se visualizan los gráficos de cajas para una variable cuantitativa, separados para cada valor posible de una variable categórica. Por lo tanto, permite ver cómo la variable cuantitativa se distribuye dentro de cada valor de una variable categórica.

Para ello, podríamos ver el ingreso del trabajo (variable continua) por sexo (categórica). Pero, como ya vimos, los boxplots con variables con *tanta dispersión*, se ven feos y *comunican* mal. Ahora bien, algo que es recomendable para comprimir la escala y reducir la asimetría entre los valores para variables con mucha dispersión es aplicarle un logaritmo a la variable de interés. De hecho, esto es algo que se hace bastante con el ingreso. No me interesa profundizar en esto ahora, porque es algo que trateremos cuando lleguemos a regresiones. No obstante, si es de interés, en este punto, el capítulo 2 de @masteringmetrics ofrece una explicación didáctica y profunda. Además, en el apéndice de dicho capítulo, específicamente en el apartado "*Building Models with Logs*" o "Modelos logarítmicos" --en la traducción del libro--, se profundiza específicamente en la utilidad de aplicar logaritmos a variables como el ingreso. 

Veamos cómo queda el gráfico, que además `ggplot2` tiene funciones para realizar transformaciones logarítmicas de las varaibles sin hacer recodificación previa. Primero lo hago con codificación previa ("manual") y luego en ggplot mismo. 







```{r}
#| label: eg-boxplot-bivariado
#| collapse: true
casen2022 |>
  filter(!is.na(ytrabajocor)) |>
  mutate(
    sexo        = factor(sexo, levels = c(1, 2), labels = c("Hombre", "Mujer")),
    log_ingreso = log(ytrabajocor) # transformación manual
  ) |>
  ggplot(aes(x = sexo, y = log_ingreso, fill = sexo)) +
    geom_boxplot(color = "black", outlier.color = "red", alpha = 0.7) +
    scale_y_continuous(
      name   = "Log(Ingreso de trabajo corregido)",
      labels = comma
    ) +
    labs(
      x     = "Sexo",
      title = "Boxplot de ingreso de trabajo corregido (log) por sexo"
    ) +
    theme_minimal(base_family = "serif", base_size = 14) +
    theme(
      plot.title     = element_text(face = "bold", size = 16, hjust = 0.5),
      axis.text      = element_text(size = 12),
      legend.position = "none"
    )

## Transformación logaritmica dentro de ggplot2

casen2022 |>
  filter(!is.na(ytrabajocor)) |>
  mutate(sexo = factor(sexo, levels = c(1, 2), labels = c("Hombre", "Mujer"))) |>
  ggplot(aes(x = sexo, y = ytrabajocor, fill = sexo)) +
    geom_boxplot(color = "black", outlier.color = "red", alpha = 0.7) +
    scale_y_log10(labels = comma) +
    labs(
      x     = "Sexo",
      y     = "Ingreso de trabajo corregido (escala log)",
      title = "Boxplot de ingreso de trabajo corregido por sexo (escala log)"
    ) +
    theme_minimal(base_family = "serif", base_size = 14) +
    theme(
      plot.title     = element_text(face = "bold", size = 16, hjust = 0.5),
      axis.text      = element_text(size = 12),
      legend.position = "none"
    )


```








Como se puede observar, la comparación de ambos *boxplots* deja claro que, al final, la forma de la distribución (posición de cuartiles, mediana y “bigotes”) es exactamente la misma: la transformación logarítmica sólo reescala los datos de manera monotónica, sin alterar su orden ni la detección de *outliers*. De modo general, sobre los gráficos podemos decir: 

- La mediana del ingreso de los hombres está algo por encima de la de las mujeres, lo cual se ve en ambos casos.

- El rango intercuartílico (caja) es ligeramente más amplio en los hombres, indicando mayor dispersión en el 50 % central.

- Aparecen outliers en ambos sexos, sobre todo hacia los ingresos muy bajos y muy altos, que quedan marcados como puntos fuera de los “bigotes”.


Respecto a las diferencias gráficas según transformación logarítmica "manual" o *en* el código de `ggplot2`: 

1. **Boxplot con transformación manual** (`log_ingreso`): 
 - En el eje Y leemos directamente la escala de log(ingreso). Por ejemplo, una mediana alrededor de 12,4 en hombres equivale a un ingreso de $\approx e^{12,4} \approx 2,4 \times^5 $, es decir, 240.000 pesos 
 - Es útil si nuestro interés está en trabajar explícitamente con valores logarítmicos (p. ej. para comparar diferencias porcentuales, como veremos después).

2. **Boxplot con ggplot2** (`scale_y_log10()`): 
  - Aquí el eje Y muestra los valores de ingreso en pesos, pero “espaciados” según su logaritmo. La mediana se lee directamente en unidades monetarias (p. ej. 200.00 vs 180.000), lo que facilita la interpretación sin cálculo inverso.
  - Conserva el lenguaje natural de las unidades en el gráfico (no aparecen logaritmos ni ejes en valores crudos de log), pero controla la asimetría y comprime los outliers en una visual más manejable.

En resumen, ambas aproximaciones equivalen en términos estadísticos. La elección depende de si quieres presentar valores ya en escala log (transformación manual) o prefieres mantener el eje en la unidad original (pesos) pero con la ventaja visual de una escala logarítmica que ofrece el paquete (`scale_y_log10()`).


### Gráficos de dispresión de puntos (*scatter plots*)

Este tipo de gráficos es recomendable para resumir gráficamente la información de **la _asociación_ de dos variables cuantitativas**. Un gráfico de dispersión muestra cada observación como un punto en un sistema de coordenadas para dos variables cuantitativas. Por ejemplo, podríamos graficar la relación entre el ingreso del trabajo y los años de escolaridad.

Ahora bien, un problema común que surge en los gráficos de dispersión es cuando los datos de muchas observaciones se intentan presentar en sólo una figura. En nuestro ejemplo, contamos con la información de más de 200.000 encuestados. En estos casos, es recomendable procesar antes los datos. Se pueden hacer varias cosas. Podrían crearse rangos de ingresos, por ejemplo, o tomar quintiles, deciles, etc., de ingresos. O bien, también podríamos aplicar una transformación logarítmica en los datos como hicimos anteriormente. 

Podemos hacer ambas, pero la verdad es que dependerá mucho de la variable cuál es mejor. Por ejemplo, si estuvieramos viendo puntajes PSU de Matemáticas y a su asociación al Ranking, sería útil hacer grupos. Pero para ingresos no funciona mucho esto. En las clases Estadística I de Engel y Díaz de la FEN se pasa el primer ejemplo. Se presentan primero sin agrupar y luego agrupando: 

<img src="images-sesiones/grafico de dispersión sin procesar.png" 
     alt="Datos sin agrupar" 
     style="display: block; margin: auto;" />

<img src="images-sesiones/grafico de dispersion de puntos procesado.png" 
     alt="Datos ya agrupados" 
     style="display: block; margin: auto;" />

Ahora bien, veamos cómo quedaría con ingreso esto. Primero tomaremos los déciles, registrados por la casen en `dau` (Decil autónomo nacional) que se calculan tomando todos los ingresos del hogar (o sea, ya no ingresos del trabajo de los encuestados encuestado) y se dividen, siguiendo la lógica explicada en las *medidas de posición*, en 10 grupos. En este caso, el grupo 1 es el grupo de ingresos más bajos (el 10% más pobre) y el 10 es el de ingresos más altos (el 10% más rico). Luego, para ver si es un problema de que sean pocos grupos, lo haremos para percentiles (100 grupos). Para la otra variable, en ambos casos, usaremos `esc` que mide los años de escolaridad (solo personas de 15 años en adelante) como variable cuantitativa discreta.







```{r}
#| label: scatter-plot
#| collapse: true

# 1) Scatter: escolaridad vs decil de ingreso del hogar
casen2022 |>
  filter(!is.na(esc), dau %in% 1:10) |>
  ggplot(aes(x = esc, y = dau)) +
    geom_jitter(width = 0.2, height = 0.2, alpha = 0.3, color = "#4E79A7") +
    scale_y_continuous(breaks = 1:10) +
    labs(
      x     = "Años de escolaridad",
      y     = "Decil autónomo nacional",
      title = "Dispersión: Escolaridad vs Decil de ingreso del hogar"
    ) +
    theme_minimal(base_family = "serif", base_size = 14)

# 2) Agrupar ingreso autónomo del hogar en 100 percentiles y graficar
casen2022 |>
  filter(!is.na(esc), !is.na(yautcorh)) |>
  mutate(percentil = ntile(yautcorh, 100)) |>
  ggplot(aes(x = esc, y = percentil)) +
    geom_jitter(width = 0.2, height = 0.2, alpha = 0.3, color = "#4E79A7") +
    scale_y_continuous(breaks = seq(0, 100, by = 10)) +
    labs(
      x     = "Años de escolaridad",
      y     = "Percentil de ingreso autónomo del hogar",
      title = "Dispersión: Escolaridad vs Percentiles de ingreso"
    ) +
    theme_minimal(base_family = "serif", base_size = 14)

```







Ahora bien, en ninguno de los casos se ve bien. Esto es por dos motivos: 1) ambas variables son discretas con muy pocos valores únicos (`esc` y `dau`). Pero, incluso si aumentamos a 100 grupos, y ya no usamos `dau`, tampoco funciona. En realidad, esto nuevamente se debe a la naturaleza de la variable ingreso, que es continua y tiene un rango muy alto. 2) Esto último produce lo que se conoce como *overplotting*, que en nuestro caso, con más de 200.000 observaciones, incluso un pequeño desplazamiento de jitter deja millones de puntos amontonados, de modo que no se distingue ninguna tendencia ni densidad real.

Entonces, para lo que queremos graficar, en este caso, ingresos y su asociación son el nivel de escolaridad, no nos sirve tomar dos variables discretas, aunque sean cuantitativas ambas. Probemos, entonces, si el viejo logaritmo nos ayuda en este caso. 








```{r}
#| label: scatter-plot2
#| collapse: true
casen2022 |>
  filter(!is.na(esc), !is.na(yautcorh)) |>
  ggplot(aes(x = esc, y = yautcorh)) +
    geom_jitter(width = 0.1, alpha = 0.5, color = "#ADD8E6") +
    scale_y_log10(labels = comma) +
    labs(
      x     = "Años de escolaridad",
      y     = "Ingreso autónomo corregido (escala log)",
      title = "Dispersión: Escolaridad vs Ingreso (log)"
    ) +
    theme_minimal(base_family = "serif", base_size = 14)
```







Como se ve, ahora si el gráfico nos comunica algo más comprensible. Además, se puede ver cómo va reduciendo la dispersión en cuanto avanzan los años de escolaridad. Es importante resaltar la cantidad de *outliers* que hay que notienen ingresos autónomos, aún avancen los años de escolaridad. Esto también puede tener que ver con que estamos usando ingresos autónomos del hogar y no de los individuos. Pero, a saber. No voy a profundizar en esto ahora. 

Lo que si me parece más interesante ahora es adelantar algo bacán que se puede hacer con `ggplot2` y los *scatterplots*. Esta tendencia que vemos en los datos, también podemos inmediatamente modelarla. El paquete permite, al graficar la dispersión de los datos, trazar una linea de tendencia con bandas de error estándar mediante un modelo lineal. Como ya advertimos antes, ahora es aún más útil aplicar `log()`, porque dado que los ingresos no se comportan con una distribución normal, transformamos el eje x (`yautcorh`) de una escala linear a una escala logarítmica. Esto nos permitirá un mejor ajuste de la línea de regresión estimada. Luego, cuando lleguemos a regresiones, veremos la utilidad de esto, como ya he ido adelantando. También podemos ajustar los valores de ingres a una notación mas adecuada, en este caso, pesos chilenos.








```{r}
#| label: scatter-plot-regresion
#| collapse: true

#Scatter plot con línea de regresión
ggplot(
  casen2022 |> filter(!is.na(esc), !is.na(yautcorh)),
  aes(x = esc, y = yautcorh)
) +
  geom_point(alpha = 0.4, color = "#36648B") +
  geom_smooth(
    method = "gam",
    se     = TRUE,
    color  = "#8B0000",
    fill   = "#696969",
    size   = 1
  ) +
  scale_y_log10(labels = scales::dollar_format(prefix = "$", suffix = " CLP")) +
  labs(
    x     = "Años de escolaridad",
    y     = "Ingreso autónomo corregido del hogar",
    title = "Nivel de ingreso autónomo corregido según años de escolaridad"
  ) +
  theme_minimal(base_family = "serif", base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
    axis.text  = element_text(size = 12)
  )
```







Entonces, y ahora sí, el gráfico muestra, en escala logarítmica, cómo varía el ingreso autónomo corregido del hogar según los años de escolaridad (para personas de 15 años en adelante). De este *scatter plot* se puede interpretar que hay

1. **Tendencia creciente**. La línea de tendencia sube de izquierda a derecha, lo que indica que a más años de escolaridad corresponde, en promedio, un ingreso mayor; 

2. No obstante, existen **rendimientos decrecientes**. La pendiente es más pronunciada en los primeros años de escolaridad y se aplana conforme avanzamos hacia niveles superiores, sugiriendo que cada año adicional de educación añade menos aumento porcentual al ingreso que el año anterior.

3. **Alta dispersión**. A cualquier nivel de escolaridad hay una enorme variabilidad de ingresos (nubes de puntos muy extendidas). Esto refleja que, aunque el promedio sube con la educación, los ingresos individuales probablemente dependen también de otros factores (sector, experiencia, género, etc., a saber).

4. Respecto a la **confianza de la curva**, se puede observar que la banda gris alrededor de la línea es más estrecha en los niveles de escolaridad donde hay más observaciones (por ejemplo entre 8 y 12 años) y más ancha en los extremos (muy poca gente con 0 ó >20 años), lo que nos habla de mayor incertidumbre al estimar la relación fuera del rango central.


En suma, el gráfico confirma una asociación positiva entre educación e ingreso, con incrementos relativos mayores al inicio de la escolaridad y un efecto suavemente decreciente en los niveles más altos. 

Finalmente, respecto a los gráficos de tipo *scatter plot*, hay que tener ojo qué tipo de variables vamos a visualizar. No basta con asociar dos variables cuantitativas, sino que también hay que fijarse en si son discretas, continuas, en la cantidad de observaciones, etc.; en definitiva, en la naturaleza de las variables asociadas. A menudo, tendremos que realizar transformaciones a nuestras variables para que el gráfico **comunique mejor** el carácter de la asociación de las variables de interés.


