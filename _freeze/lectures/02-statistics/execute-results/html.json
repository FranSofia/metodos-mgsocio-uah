{
  "hash": "90aef1b4708a2c5908c894e7b2d5efbc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Sesión 2: Estadística Descriptiva (II) y Prueba de Hipótesis (I)\"\nsubtitle: \"Taller de Métodos y Técnicas de Investigación I\"\nexecute:\n  freeze: true\n---\n\n\n\n\nEn esta sesión entraremos de lleno en estadística descriptiva, retomando desde contenidos de la sesión pasada, pero con un enfoque más riguroso. Sumado a esto, nos enfocaremos más en esto desde una perspectiva más estadística y usando R aplicadamente, sin deternos tanto en R mismo. Aunque también aprovecharé de mostrarle como crear funciones de usuario para hacer estadísticas descriptivas con una sola función. \n\n\n## Objetivos de la sesión\n\n- Comprender la estadística descriptiva\n- Conocer medidas de la estadística descriptiva\n- Crear visualizaciones avanzadas.\n- Introducción a test de hipótesis y estadística inferencial\n\nPero antes de comenzar, carguemos los paquetes que utilizaremos, donde además les muestro mi forma favorita de cargar y traer paquetes en un solo paso (con `pacman`). \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Forma 1 (Forma clásica)\ninstall.packages(\"tidyverse\") # colección de paquetes, dplyr entre ellos\ninstall.packages(\"dplyr\") # manipular datos\ninstall.packages(\"sjmisc\") # explorar datos\n\n#Forma 2 (Mejor para reproductibilidad)\ninvisible(lapply(c(\"tidyverse\", # colección de paquetes, dplyr entre ellos\n                   \"dplyr\", # manipular datos\n                   \"sjmisc\"), # explorar datos\n                 function(p) \n  if (!requireNamespace(p, quietly = TRUE)) install.packages(p)))\n\n# Llamar librería \nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(sjmisc)\n\n# Forma 3 (Mi favorita. Es `install.packages()` -si no está bajado- más `library`)\n\nif (!requireNamespace(\"pacman\", quietly = TRUE))\n  install.packages(\"pacman\")\npacman::p_load(tidyverse, # colección de paquetes, dplyr entre ellos, pero también haven\n               dplyr, # manipular datos\n               psych, # para estadísticas descriptivas\n               sjmisc, # explorar datos\n               ggplot2, # para visualizar gráficos\n               scales) # para ajustar gráficos\n```\n:::\n\n\n\n\n\n\n\n\n# 1. Estadística descriptiva\n\n## Datos \n\nPara comprender que son las estadísticas descriptivas, y para que las usaremos como futurxs sociólogxs, partamos viendo *qué son los datos*. Los **datos** son valores (o mediciones) de variables que han sido recolectados y organizados para su análisis. La palabra datos se define como información factual (como mediciones o estadísticas) utilizada como base para el razonamiento, para la discusión o para cálculos [(Merrian-Webster)](https://www.merriam-webster.com/dictionary/data). Existen tres tipos de *estrucutas de datos*: \n\n1. **Datos de corte transversal**: son mediciones de un conjunto de variables para un gurpo de unidades (personas, hogares, empresas, países, etc.) en un mismo momento del tiempo. Un ejemplo de esto puede ser los puntajes PSU de un año dado. Una base de datos de datos de corte transversal, e.g., es la CASEN, la ENUT, la ENE, etc. \n\n2. **Datos de series de tiempo**: sucesión de registros de una o varias variables de una única unidad, que son medidos en determinados momento del tiempo en un orden cronológico claro. Es importante resaltar que registra una **única** entidad/unidad a lo largo del tiempo. Un ejemplo podría ser el IMACEC o el PIB de tal a tal periodo. \n\n3. **Datos de panel o longitudinales**: sucesión de registros de un conjunto de variables para un grupo de unidades medidas en varios momentos del tiempo. Los datos de panel/longitudinales combinan las dimensiones de corte transversal y series de tiempo de los datos, pues es una sucesión de registros de un *conjunto de variables* para un *grupo de unidades*, medidas en varios momentos del tiempo con un orden cronológico claro. Un ejemplo de esto podría ser la tasa de crecimiento anual del PIB real para un grupo de países de tal a cual periodo. \n\nA su vez, existen dos tipos de recolección de datos:\n\n1. **Estudios observacionales**: los datos se recopilan por medio de la observación de los valores de las variables de interés sin intervención o influencia en ellos; o \n\n2. **Experimentos**: tras una exposición de ciertas unidades a una intervención, se observan los valores de las variables de interés presentes en los resultados. \n\n## Tipos de variables\n\nUna variable es una característica o atributo que puede tomar valores diferentes para distintas unidades. Las variables tienen dos categorías principales que las distinguen: \n\n1. **Variables categóricas**: puede tomar un número limitado de valores definido sobre la base de alguna característica cualitativa. A su vez, dentro de las variables categóricas se encuentran\n  \n  - *Variable categórica nominal*: es una variable categórica/cualitativa sin ningún orden o jerarquía clara. Por ejemplo, una variable que registre nombres de un curso, comunas, países, etc. Dentro de las variables categóricas nominales, se encuentran las variables *dummy*, las que poseen solo dos valores a los que se les puede asignar un número, generalmente $0$ o $1$, donde $1$ indica presencia del atributo y $0$ ausencia.\n  \n  - *Varible categórica ordinal*: es una variable categórica/cualitativa que tiene un orden o jerarquí establecida entre sus categorías. Por ejemplo, el nivel educacional o las variables en \"Escala Likert\"\n  \nEn R, para crear una variable cualitativa, deben escribirse entre comillas sus categorías, como ya vimos la sesión pasada. Por ejemplo, \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Variables categóricas\n\nvar_nominal <- c(\"Karl Marx\", \"Rosa Luxemburgo\", \"Wooldrigde\")\n\nclass(var_nominal) # Comprobar naturaleza de la variable\n## [1] \"character\"\n\nvar_ordinal_educ<- c(\"Secundaria Incompleta\", \"Secundaria Completa\",\n                     \"Educación Superior Incompleta\", \"Educación Superior Completa\")\nvar_ordinal_likert <- c(\"Muy malo\", \"Malo\", \"Maomeno\",\n                        \"Bueno\", \"Muy bueno\")\n\nclass(var_ordinal_educ) # Comprobar naturaleza de la variable\n## [1] \"character\"\nclass(var_ordinal_likert) # Comprobar naturaleza de la variable\n## [1] \"character\"\n```\n:::\n\n\n\n\nSuele ser útil, en R, asignarles números a las variables cualitativas para un análisis estadísticos posteriores. Por ejemplo, podríamos tomar `var_ordinal_likert` y, *además* de que tenga sus categorías asociadas, también tengan un número. Por ejemplo, que `\"Muy malo\"` sea `1` y `\"Muy bueno\"` sea `5`. Esto podemos hacerlo de dos maneras que mostramos de inmediato. Por cierto, la segunda la hacemos con `haven` que es un paquete dentro de `tidyverse`, y que usualmente usaremos ese paquete para cargar bases de datos, pero que también sirve para recodificar:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1) Versión con un factor ordenado (tienes la categoría y su código interno)\nvar_ordinal_likert <- factor(\n  var_ordinal_likert,\n  levels  = c(\"Muy malo\", \"Malo\", \"Maomeno\", \"Bueno\", \"Muy bueno\"),\n  ordered = TRUE\n)\n# Para ver el código numérico (1=Muy malo … 5=Muy bueno):\nas.integer(var_ordinal_likert)\n## [1] 1 2 3 4 5\n\n\n# 2) Si prefieres un vector numérico “double” con etiquetas (dos niveles),\n#    puedes usar haven::labelled\n\nvar_ordinal_likert <- haven::labelled(\n  as.integer(factor(\n    var_ordinal_likert,\n    levels  = c(\"Muy malo\", \"Malo\", \"Maomeno\", \"Bueno\", \"Muy bueno\"),\n    ordered = TRUE\n  )),\n  labels = c(\n    \"Muy malo\"  = 1,\n    \"Malo\"      = 2,\n    \"Maomeno\"   = 3,\n    \"Bueno\"     = 4,\n    \"Muy bueno\" = 5\n  )\n)\n\n# Comprobamos:\nvar_ordinal_likert\n## <labelled<integer>[5]>\n## [1] 1 2 3 4 5\n## \n## Labels:\n##  value     label\n##      1  Muy malo\n##      2      Malo\n##      3   Maomeno\n##      4     Bueno\n##      5 Muy bueno\n```\n:::\n\n\n\n\n2. **Variables cuantitativas**: son variables medidas en una escala númerica, como la edad, temperatura, estatura, etc. Y, por lo tanto, que el número mismo indica ordinalidad y jerarquía entre los valores ($x<x+1$). También hay dos tipos de variables cuantitativas\n\n  - *Variable cuantitativa discreta*: es aquella que *solo* puede tomar números específicos como valores, sin poder tomar un valor intermedio entre dos valores específicos. Es decir, que se expresan en números enteros. Matemáticamente, simplemente son variables en que sus valores $x \\in \\mathbb{N}$. Un ejemplo, puede ser el número de ventas (no se puede vender -100), la edad de las personas encuestadas (no se puede tener -2), etc. \n  \n  - *Variable cuantiativa continua*: puede tomar como valores un número infinito de posibilidades dentro de un rango de números, por lo que se expresa en números reales: $x\\in \\mathbb{R}$. Un ejemplo de esto puede ser una cuenta corriente, el peso de las personas, la temperatura, etc. \n  \n  En R, las variables cuantitativas tienen por categorías números, sin \"...\". Además, si se quiere dejar como variable cuantitativa discreta (`integer`), se le puede añadir una `L` después del número. Esto es porque en R cualquier literal numérico sin sufijo se interpreta por defecto como tipo `numeric` (variable cuantitativa). Al añadir la `L` se le indica a R que es un `integer` (número entero). No obstante, generalmente para el análisis estadístico no afectará mucho. \n  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Variables cuantitativas discretas\nvar_discreta_hijos <- c(0L, 1L, 2L, 3L, 4L)\nvar_discreta_autos <- c(1L, 0L, 2L, 1L, 3L)\nclass(var_discreta_hijos)   \n## [1] \"integer\"\nclass(var_discreta_autos)   \n## [1] \"integer\"\n\n# Variables cuantitativas continuas\nvar_continua_altura <- c(1.65, 1.72, 1.58, 1.80, 1.75)\nvar_continua_peso   <- c(65.4, 70.2, 58.9, 80.0, 72.5)\nclass(var_continua_altura)  # numeric\n## [1] \"numeric\"\nclass(var_continua_peso)    # numeric\n## [1] \"numeric\"\n```\n:::\n\n\n\n\n\n## Estadísticas descriptivas \n\nLas estadísticas descriptivas son valores fácilmente interpretables y que entregan un \"resumen\" de las características más importantes de un conjunto de datos, además de que utilizan para su cálculo operaciones aritméticas simples. Las estadísticas descriptivas son el primer paso en el análisis exploratorio de datos. Antes del análisis estadístico más riguroso de los datos, es esencial examinar las estadísticas descriptivas de todas las variables para:\n\n- Detectar errores de medición o valores atípicos (*outliers*) en los datos.\n\n- Detectar patrones y tendencias en los datos.\n\n- Evaluar la plausibilidad de los supuestos de trabajo.\n\n- Establecer hipótesis de trabajo.\n\nA su vez, las estadísticas descriptivas se pueden clasificar en tres grupos:\n\n  1. **Medidas de localización o tendencia central**: Se calculan para describir un valor central alrededor del cual se ubican (distribuyen) los datos. En otras palabras, estas medidas determinan el valor donde se ubican mayoritariamente las observaciones\n\n  2. **Medidas de dispersión o variabilidad**: Proporcionan una descripción de la “dispersión de los datos” o qué tan lejos están las observaciones de la tendencia central.\n\n\n  3. **Medidas de posición**: Resumen la posición relativa de valores específicos en los datos.\n  \nAntes de seguir profundizando esto, utilizaremos bases de datos reales y conocidas para ir ejemplificando el contenido con datos observacionales. Partamos con la Encuesta de Caracterización Socioeconómica Nacional (CASEN) más actual, la CASEN 2022. Esta se puede obtener en el [siguiente link](https://observatorio.ministeriodesarrollosocial.gob.cl/encuesta-casen-2022), que cuenta con 202.231 casos (encuestados) y 918 variables. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Obtener ruta \n\ngetwd() # Para obtener nuestro directorio (en este casode nuestro RProject)\n## [1] \"C:/Users/Fran/OneDrive/Escritorio/UAH/MAGISTER ECONOMIA UCH/Ayudantías/Taller métodos/metodos-mgsocio-uah/lectures\"\n\n# Cargar formato sav (SPSS)\ncasen2022<-haven::read_sav(\"data-sesiones/CASEN 2022.sav\")\n\ndim(casen2022)\n## [1] 202231    918\n\n# Cargar formato dta (STATA)\n\n# casen2022<-haven::read_dta(\".../CASEN 2022.dta\")\n\n```\n:::\n\n\n\n\nContinuemos. Antes de pasar a estos tres grupos de estadísticas descriptivas, conviene tener en claro el concepto de estadísticos de orden. Los **estadísticos de orden** es la ordenación de un conjunto de datos u obsrvaciones, dond $y_k$ es el $k-$ésimo menor valor del conjunto de observaciones. Por ejemplo, si tenemos estos cuatro datos: \n$$\nx_1 =9; x_2=3, x_3 = 12; x_3 =1 ; x_5=2\n$$\nLos estadísticos de orden, $y_i$, corresponderían a\n$$\ny_1=1; y_2=2; y_3=3; y_4=9; y_5=12\n$$\nEs decir, la ordenación de $x_i$ de menor a mayor. Dicho esto, pasemos, en primer lugar, a las medidas de localización o tendencia central. \n\n## 1.1. Medidas de localización o tendencia central\n\n#### Media o promedio\n\nLa *media* de $n$ observaciones de una variable, \\(x_1, \\dots, x_n\\), que denotaremos por \\(\\bar{x}\\), está definida por la suma de los valores de todas las observaciones, dividida por el número de observaciones: \n$$\n\\bar{x} = \\frac{x_1 + x_2 + \\dots + x_n}{n} = \\frac{1}{n} \\sum_{i=1}^{n} x_i.\n$$\nPor ejemplo, supongamos que los datos son:\n$$\nx_1 = 0.7; \\quad x_2 = 1.2; \\quad x_3 = 0.9; \\quad x_4 = 0.6; \\quad x_5 = 1.4; \\quad x_6 = 1.8; \\quad x_7 = 2.5.\n$$ \nLa media en este caso viene dada por:\n$$\n\\bar{x} = \\frac{0.7 + 1.2 + 0.9 + 0.6 + 1.4 + 1.8 + 2.5}{7} = 1.3.\n$$\nEn R, la media se obtiene simplemente con el comando `mean()`. Veamos como obtener la media de alguna variable de interés en la CASEN 2022.  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Obtener media mediante mean()\nmean(casen2022$ytrabajocor) # Ingreso del trabajo corregido\n## [1] NA\n                            # Ups! \nmean(casen2022$ytrabajocor,\n     na.rm = TRUE) # ahora si :)\n## [1] 667690.9\n```\n:::\n\n\n\nComo se ve, el salario promedio de los encuestados de la CASEN 2022 es de 667.690CLP. El comando `na.rm = TRUE` es para sacar las observaciones `NA` de la variable, las cuales registran los casos \n  \n#### Mediana  \n\nLa **mediana** es el valor medio de un conjunto de observaciones cuando se les ordena de menor a mayor. Si el número de observaciones es par, entonces la *mediana* es la suma de los dos valores medios, dividida por 2. Si ordenamos las observaciones del ejemplo anterior de menor a mayor obtenemos los **estadísticos de orden** del conjunto de observaciones: \n$$\ny_1 = 0.6, \\quad y_2 = 0.7, \\quad y_3 = 0.9, \\quad y_4 = 1.2, \\quad y_5 = 1.4, \\quad y_6 = 1.8, \\quad y_7 = 2.5.\n$$ \nNotar que los *estadísticos de orden* cumplen con \n$$\ny_1 = \\min(x_1, x_2, \\dots, x_7), \\quad y_7 = \\max(x_1, x_2, \\dots, x_7), \\quad y_1 \\leq y_2 \\leq \\dots \\leq y_7.\n$$ \nEntonces, para calcular la mediana se tiene que ordenar primero la muestra de datos de menor a mayor, tal que se cumpla con los estadísticos de orden. Así, la *mediana* en este ejemplo, que denotamos por $\\tilde{x}$, viene dada por: \n$$\n\\tilde{x} = y_4 = 1.2.\n$$ \nAhora bien, en este caso, tenemos un $n$ impar. Luego, es más fácil establecer la mediana. Supongamos ahora que tenemos un número par de cantidad de datos, un $n=6$, por ejemplo. \n$$\nx_1 = 0.7, \\quad x_2 = 1.2, \\quad x_3 = 0.9, \\quad x_4 = 0.6, \\quad x_5 = 1.4, \\quad x_6 = 1.8.\n$$ \nEs decir, los mismos datos que antes, salvo que hemos excluido el valor más grande. Entonces los estadísticos de orden son: \n$$\ny_1 = 0.6, \\quad y_2 = 0.7, \\quad y_3 = 0.9, \\quad y_4 = 1.2, \\quad y_5 = 1.4, \\quad y_6 = 1.8.\n$$ \nY la mediana es: \n$$\n\\tilde{x} = \\frac{y_3 + y_4}{2} = \\frac{0.9 + 1.2}{2} = 1.05.\n$$ \nPor lo tanto, la mediana en este caso es el promedio simple entre los dos números ``al medio'' del conjunto de datos. Si tenemos que $n=6$, entonces la mediana es el promedio entre $x_3$ y $x_4$, arrojando un valor nuevo. En suma, la mediana es un valor que puede o no puede estar en el conjunto de datos. Si el conjunto de dato es par, entonces será el promedio simple entre los 2 datos del medio y si es impar simplemente será el valor del medio.  \n\nEn R esto se obtiene con `median()`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Obtener mediana a través de median()\nmedian(casen2022$ytrabajocor,\n     na.rm = TRUE) # Ingreso del trabajo corregido\n## [1] 480000\n```\n:::\n\n\n\n  \nComo vemos, la mediana de los ingresos del trabajo (salario), es 480.000 CLP. Lo cual habla bastante de la dispersión de los ingresos, pues el promedio  es de 667.690 CLP. Siguiendo la lógica de los estadísticos de orden, lo que tenemos es que el $50\\%$ de los encuestados gana 480.000 CLP *o menos*. Y, sin embargo, el promedio es de 667.690 CLP, i.e., de 187.690 CLP más. Esto se debe a que hay ingresos muy altos, que elevan el promedio. Y la media es una medida *sensible* a los valores extremos. En breve, profundizaremos sobre esto, pero tiene que ver con robustez de las medidas de localización, lo cual será clave más adelante para para realizas **estimaciones** con estas medidas. \n  \n#### Moda   \n  \nLa *moda* es el valor más frecuente de un conjunto de observaciones. Un conjunto de valores puede tener más de una moda. Por ejemplo, supongamos que los datos son: \n$$\nx_1 = 0.7; \\quad x_2 = 1.2; \\quad x_3 = 1.2; \\quad x_4 = 0.6; \\quad x_5 = 1.4; \\quad x_6 = 1.8; \\quad x_7 = 2.5.\n$$\nEntonces la moda, que denotaremos por $\\tilde{\\tilde{x}}$^[El LaTex se coloca `\\dbtilde{}`. No me deja en Quarto, pero colocar `\\tilde{\\tilde{x}}` es para representar que se denota con doble tilde], viene dada por: \n$$\n\\tilde{\\tilde{x}}= 1.2. \n$$ \nPara un conjunto de valores puede haber más de una moda.\n\nEn R, no hay un \"paquete base\" para extraer la moda. Pero si se puede hacer una función, o bien, usar paquetes. Veamos\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Moda en el caso de datos unimodales discretos\nx <- c(1, 5, 1, 6, 2, 1, 6, 7, 1)\n\n#Función para calcular la moda\nmode <- function(x) {\n   return(as.numeric(names(which.max(table(x)))))\n}\n\nmode(x)\n## [1] 1\n\noptions(scipen = 999) # Para desactivar notación cientifica\n\nmode(casen2022$ytrabajocor)\n## [1] 400000\n\n# Moda con mfv() de modeest\npacman::p_load(modeest)\n\nmodeest::mfv(casen2022$ytrabajocor, na_rm = TRUE)\n## [1] 400000\n```\n:::\n\n\n\nComo se puede observar, la moda de los ingresos del trabajo es de 400.000 CLP. Es decir, que el valor que más se repite entre los salarios de los encuestados es de 400 lucas. \n\n\n### 1.1.1. Robustez de las medidas de localización\n\nAhora bien, **¿cuán robustas son las medidas de localización?** Un tema a considerar es cuál es el impacto de observaciones aberrantes en las medidas de localización, es decir, qué impacto tiene la presencia de una fracción pequeña de observaciones con errores muy grandes en su registro o que son atípicas por otro motivo en el valor de las medidas de localización.\n\nHablaremos indistintamente de observaciones aberrantes, observaciones extrañas y *outliers*. Si el número de observaciones es pequeño, se puede investigar uno por uno los valores aberrantes y corregir, reemplazar o eliminar las observaciones en cuestión. Sin embargo, cuando el número de observaciones es más grande, uno quisiera medidas de localización que sean insensibles a la presencia de una fracción moderada de observaciones extrañas y que, al mismo tiempo, sean un buen resumen numérico de la tendencia central de los datos. Analicemos, pues, cuán robusta son estas medidas. \n  \n#### Robustez de la media\n\nSupongamos que los datos son \n$$\nx_1 = 0.7;\\ x_2 = 1.2;\\ x_3 = 0.9;\\ x_4 = 0.6;\\ x_5 = 1.4;\\ x_6 = 1.8;\\ x_7 = 2.5.\n$$\nComo vimos la media viene dada por $\\bar{x} = 1.3$. Intuitivamente la media **no** es robusta, pues un solo outlier puede tener un efecto devastador sobre su valor. En efecto, si una de las observaciones, digamos la primera, es reportada con un error de medición $e$, de modo que \n$$\nx_1 = 0.7 + e\n$$\nentonces la media, como función del error $e$, será:\n$$\n\\bar{x}(e) = 1.3 + \\frac{e}{7}\n$$ \ny tenemos que\n$$\n\\lim_{e \\to \\infty} \\bar{x}(e) = \\infty, \\quad \\lim_{e \\to -\\infty} \\bar{x}(e) = -\\infty.\n$$ \nEn el caso más general tenemos $\\bar{x}(e) = \\bar{x}(0) + \\frac{e}{n}$, de modo que esta expresión tiende a $\\pm\\infty$ cuando $e$ tiende a $\\pm\\infty$.\n\nEn suma, la media es una medida de localización central **muy** sensible a valores atípicos/*outliers*. En este sentido, a pesar de que indica información, hay que tener cuidado y en general suele ser complementada con otras medidas, tal como vimos con la mediana.  \n  \n#### Robustez de la mediana\n  \nComo vimos, los *estadísticos de orden* de las observaciones anteriores están dadas por: \n$$\ny_1 = 0.6, \\quad y_2 = 0.7, \\quad y_3 = 0.9, \\quad y_4 = 1.2, \\quad y_5 = 1.4, \\quad y_6 = 1.8, \\quad y_7 = 2.5.\n$$ \nVimos también que la mediana, que denotamos por $\\tilde{x}$, viene dada por: \n$$\n\\tilde{x} = y_4 = 1.2.\n$$ \nAhora, suponiendo nuevamente que la primera observación se midió con error $e$, de modo que $x_1 = 0.7 + e$, y denotando por $\\tilde{x}(e)$ la mediana cuando $x_1$ se reemplaza por $x_1 + e$, tenemos \n$$\n\\tilde{x}(e) =\n\\begin{cases}\n    1.2 & \\text{si } e \\leq 0.5, \\\\\n    0.7 + e & \\text{si } 0.5 < e \\leq 0.7, \\\\\n    1.4 & \\text{si } e > 0.7.\n\\end{cases}\n$$ \nEn el caso general con $n = 2m+1$ observaciones, donde elegimos tamaño impar solo para simplificar las expresiones, tendremos que $\\tilde{x}(e) \\in [y_m, y_{m+2}]$. Por tanto, la influencia de un valor aberrante en la mediana está acotada. Es decir, la **mediana es una medida robusta**.   \n\n#### Robustez de la moda  \n\nSupongamos que los datos están dados por:\n$$\nx_1 = 0.7, \\quad x_2 = 1.2, \\quad x_3 = 1.2, \\quad x_4 = 0.6, \\quad x_5 = 1.4, \\quad x_6 = 1.8, \\quad x_7 = 99.9.\n$$ \nEntonces la moda viene dada por $\\tilde{\\tilde{x}} = 1.2$.\n\nSuponiendo nuevamente que la primera observación se midió con error $e$, de modo que $x_1 = 0.7 + e$, y denotando por $\\tilde{\\tilde{x}}(e)$ la moda cuando $x_1$ se reemplaza por $x_1 + e$, tenemos que:\n$$\n\\lim_{e \\to \\infty} \\tilde{\\tilde{x}}(e) = 1.2, \\quad \\lim_{e \\to -\\infty} \\tilde{\\tilde{x}}(e) = 1.2.\n$$ \nEn cambio, si la observación con error es la segunda observación, de modo que se midió $x_2 = 1.2 + e$, para $e = 0.6$, la moda será $1.8$. El cambio en la moda puede ser mucho mayor, existen valores de $e$ para los cuales la moda es $99.9$. En contraste, no existen valores de $e$ para los cuales la mediana cambie tanto.En suma, la moda es **muy** sensible a la presencia de *outliers*, no obstante va depende en qué localización de los datos se encuentre. Entonces, no siempre es muy sensible, pero a veces sí.   \n  \n#### Sintesis \n\nComo se desprende de las expresiones derivadas en los apartados anteriores, el impacto de valores extremos o atípicos es acotado tanto para la mediana como para la moda, mientras que para la media crece sin límite cuando $|e|$ tiende a infinito.Al mismo tiempo, la mediana pareciera ser más robusta que la moda, pues la influencia de errores de medición es menor. La moda pareciera estar entremedio, el impacto de outliers es acotado pero puede ser mucho más grande que su impacto en la mediana. Entonces, **¿media, mediana o moda?**\n\nEn general, la moda se usa poco para resumir en una cifra la localización de los datos. Probablemente, porque es “demasiado localizada”. Por ejemplo, puede que la moda esté en 0, porque un 10% de las observaciones son iguales a cero, pero la mayoría de los datos sean mayores que 100. En casos como este la moda captura muy mal la localización de los datos, pues uno quisiera algún número mayor que 100, no cero. La media es la medida de localización más utilizada en la práctica, seguida de la mediana. Esto nos lleva a considerar las siguientes ventajas y desventajas\n\n- **Mediana**: más robusta.\n\n- **Media**: usa toda la información. Al menos intuitivamente, la robustez de la mediana tiene como contraparte que usa poco la información disponible.\n  \nEntonces, la media entrega más información, pero es menos robusta, en cambio la mediana sufre la ventaja/desventaja opuesta. La digresión anterior motiva considerar medidas de localización más robustas que la media y que usan más información que la mediana. Con ese objetivo consideramos a continuación **las medias podadas**.  \n  \n### Medias podadas \n\nSupongamos que los datos están dado por:\n$$\nx_1 = 0.7, \\; x_2 = 1.2, \\; x_3 = 0.9, \\; x_4 = 0.6, \\; x_5 = 1.4, \\; x_6 = 1.8, \\; x_7 = 2.5,\n$$ \nComo vimos, sus estadísticos de orden correspondientes son:\n$$\ny_1 = 0.6, \\; y_2 = 0.7, \\; y_3 = 0.9, \\; y_4 = 1.2, \\; y_5 = 1.4, \\; y_6 = 1.8, \\; y_7 = 2.5.\n$$\n\nDefinimos la siguiente medida de localización:\n$$\n\\bar{x}^p = \\frac{1}{5}(y_2 + y_3 + \\dots + y_6).\n$$ \nEs decir, primero **podamos** los datos de la muestra ordenada de menor a mayor, removiendo un valor de cada extremo y luego calculamos el promedio de las observaciones podadas. El estadístico descriptivo que resulta se conoce como *media podada*, de nivel $k=1$ porque removimos una observación de cada extremo. Lo denominamos por $\\bar{x}^p$. Para las observaciones en cuestión tenemos que la media podada de nivel 1 es 1,2.\n\nEn síntesis, se eliminan los valores extremos, el más pequeño y el más grande en este caso. Y después calculamos el promedio. Con ello, se le agrega mayor robustez y estaremos usando más información que usando la mediana. Veamos el caso general, no solo para $k=1$\n\n#### Medias podadas: Caso General \n\n:::{.definition}\n Sean $x_1, \\dots, x_n$ las observaciones y $y_1, \\dots, y_n$ los estadísticos de orden correspondientes. Entonces, para $k < n/2$ definimos la media podada de nivel $k$ mediante: \n$$\n    \\bar{x}^p = \\frac{1}{n - 2k} \\sum_{i = k+1}^{n-k} y_i.\n$$\n:::\n  \n\nEn palabras: dadas observaciones $x_1, \\dots, x_n$, la media podada de nivel $k$ será el promedio de los valores que se obtiene luego de remover $k$ observaciones de cada uno de sus extremos (las $k$ más grandes y las $k$ más pequeñas). Las medias podadas de nivel $k = 1$ —se remueve el mínimo y máximo— se utilizan para pasar de las evaluaciones individuales de los jurados en varios deportes, entre ellos nado sincronizado y gimnasia, a la evaluación agregada.\n\nEn R, esto se puede hacer de dos formas, con una función o con `mean()` y especificaciones: \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Datos de ejemplo\nx <- c(0.7, 1.2, 0.9, 0.6, 1.4, 1.8, 2.5)\n\n# 1) Función manual para media podada de nivel k\nmedia_podada <- function(x, k) {\n  n <- length(x)\n  if (2*k >= n) stop(\"k debe ser menor que n/2\")\n  y <- sort(x)\n  mean(y[(k + 1):(n - k)])\n}\n\n# Cálculo de la media podada para k = 1\nmedia_podada(x, 1)\n## [1] 1.2\n\n# 2) Usando la función base mean() con el argumento trim\n#    trim espera la fracción a recortar por cada extremo: k/n\nk <- 1\nn <- length(x)\nmean(x, trim = k / n)\n## [1] 1.2\n\n# Media podada de los ingresos del trabajo\nmean(casen2022$ytrabajocor, \n     trim = k/n,\n     na.rm=TRUE)\n## [1] 521452.3\n```\n:::\n\n\n\n\n### Medias Windorizadas (opcional) \n\nEsto es una medición opcional en el sentido de que no se usa mucho. Pero puede ser útil. Supongamos que los datos están dado por:\n$$\nx_1 = 0.7; \\, x_2 = 1.2; \\, x_3 = 0.9; \\, x_4 = 0.6; \\, x_5 = 1.4; \\, x_6 = 1.8; \\, x_7 = 2.5.\n$$ \nComo vimos, sus estadísticos de orden correspondientes son: \n$$\ny_1 = 0.6, \\, y_2 = 0.7, \\, y_3 = 0.9, \\, y_4 = 1.2, \\, y_5 = 1.4, \\, y_6 = 1.8, \\, y_7 = 2.5.\n$$\nDefinimos la siguiente medida de localización:\n$$\n\\bar{x}^p = \\frac{1}{7} [y_2 + y_2 + y_3 + y_4 + y_5 + y_6 + y_6].\n$$\n\nEs decir, ordenadas las observaciones de menor a mayor, sustituimos un valor de cada extremo por el valor inmediatamente anterior o posterior y luego calculamos el promedio de todos los valores. El estadístico descriptivo que resulta se conoce como **media winsorizada**, de nivel $k = 1$ porque sustituimos una observación de cada extremo. Lo denotamos por $\\bar{x}^w$. Para las observaciones en cuestión tenemos que la media winsorizada de nivel 1 es 1,21 (aproximadamente).\n\n:::{.definition}\n\n**Medias Winsorizadas: Caso General**\n\nSean $x_1, \\dots, x_n$ las observaciones y $y_1, \\dots, y_n$ los estadísticos de orden correspondientes. Entonces, para $k < n/2$ definimos la media winsorizada de nivel $k$ mediante:\n$$\n  \\bar{x}^w = \\frac{k y_{k+1} + \\sum_{i=k+1}^{n-k} y_i + k y_{n-k}}{n}.\n$$\n:::\n\nEn palabras: dadas observaciones $x_1, \\dots, x_n$, la media winsorizada de nivel $k$ será el promedio de los valores que se obtiene luego de sustituir $k$ observaciones de cada uno de sus extremos (las $k$ más grandes y las $k$ más pequeñas) por el valor inmediatamente anterior o posterior. En R, se puede hacer con funciones nuevamente, o con el paquete `DescTools`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Datos de ejemplo\nx <- c(0.7, 1.2, 0.9, 0.6, 1.4, 1.8, 2.5)\n\n# 1) Función winsorizada + argumento na.rm\nmedia_winsorizada <- function(x, k, na.rm = FALSE) {\n  if (na.rm) x <- x[!is.na(x)]\n  n <- length(x)\n  if (2*k >= n) stop(\"k debe ser menor que n/2\")\n  y <- sort(x)\n  # sustituir k extremos\n  y[1:k]           <- y[k + 1]\n  y[(n - k + 1):n] <- y[n - k]\n  mean(y)\n}\n\n# Cálculo de la media winsorizada para k = 1\nmedia_winsorizada(x, 1)\n## [1] 1.214286\n\nmedia_winsorizada(casen2022$ytrabajocor, 1,\n                  na.rm=TRUE)\n## [1] 667567.7\n\n\n# 2) Usando la función Winsorize() del paquete DescTools\npacman::p_load(DescTools)\n\n#  Vector limpio de NAs\nx <- na.omit(casen2022$ytrabajocor)\n\n# 3) Definimos k y calculamos n\nk <- 1\nn <- length(x)\n\n# 4) Calculamos los cuantiles que usarán de umbrales\numbrales <- quantile(x, probs = c(k/n, 1 - k/n), na.rm = TRUE)\n\n# 5) Winsorizamos usando 'val = umbrales'\nx_wins <- Winsorize(x, val = umbrales)\n\n# 6) Y finalmente sacamos la media\nmean(x_wins)\n## [1] 667567.7\n```\n:::\n\n\n\n\n#### Síntesis \n  \n¿Qué hacemos en la práctica?\n\n- Si se desea resumir la localización de los datos, parta por calcular la media, mediana y moda, así como también las medias podadas para algunos valores de $k$ (por ejemplo, $k/n \\simeq 0,05$ y $k/n \\simeq 0,1$)\n\n- Si obtiene valores similares, reportar la media y mediana\n\n- Si hay diferencias importantes, mire los datos con cuidado, entienda de dónde vienen las diferencias y luego decida qué hacer.\n\n## 1.2. Medidas de dispersión\n  \n### La varianza\n\nLas medidas de dispersión lo que buscan es determinar cuán dispersos están los datos con respectos a una medida de tendencia central. Una de ellas es la varianza. La **varianza** es una medida que resume la **dispersión de las observaciones _con respecto_ a su media**.\n  \n:::{.definition}\nDadas $n$ observaciones, $x_1, \\ldots, x_n$, la varianza de este conjunto de datos, que denotaremos por $\\hat{\\sigma}^2$, está dada por:\n$$\n\\begin{aligned}\n\\hat{\\sigma}^2 &= \\frac{(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + \\dots + (x_n - \\bar{x})^2}{n}\\\\\n&= \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\\\\n&= \\frac{1}{n} \\sum_{i=1}^{n} x_i^2 - \\bar{x}^2.\n\\end{aligned}\n$$\n\n:::  \n  \nPor ejemplo, si tuvieramos que\n$$\nx_1 = 0.7; \\quad x_2 = 1.2; \\quad x_3 = 0.9; \\quad x_4 = 0.6; \\quad x_5 = 1.4; \\quad x_6 = 1.8; \\quad x_7 = 2.5,\n$$\nentonces, la varianza de estos datos sería \n$$\n\\begin{aligned}\n  \\hat{\\sigma}^2 = \\frac{(0.7 - 1.3)^2 + (1.2 - 1.3)^2 + (0.9 - 1.3)^2}{7} \\\\\n  + \\frac{(0.6 - 1.3)^2 + (1.4 - 1.3)^2 + (1.8 - 1.3)^2 + (2.5 - 1.3)^2}{7} = 0.39.\n\\end{aligned}\n$$\n  \nOjo,cuando hay muchos datos, la fórmula más simple es el promedio de las observaciones al cuadrado, menos la media al cuadrado. O sea, la última igualdad que pusimos anteriormente, es decir,  \n$$\n  \\hat{\\sigma}^2 =\\frac{1}{n} \\sum_{i=1}^{n} x_i^2 - \\bar{x}^2.\n$$\nEsta, sin embargo, es una demostración de la primera sumatoria, aunque bastante trivial.\n \n:::{.proof}\nSi\n$$\n\\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2 = \\frac{1}{n} \\sum_{i=1}^n x_i^2 - \\bar{x}^2\n$$\nEn efecto,\n$$\n\\begin{aligned}\n\\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2 &= \\frac{1}{n} \\sum_{i=1}^n \\left( x_i^2 - 2x_i \\bar{x} + \\bar{x}^2 \\right)\\\\\n&= \\frac{1}{n} \\sum_{i=1}^n x_i^2 - 2 \\bar{x} \\frac{1}{n} \\sum_{i=1}^n x_i + \\frac{1}{n} \\sum_{i=1}^n \\bar{x}^2\\\\\n&= \\frac{1}{n} \\sum_{i=1}^n x_i^2 - 2 \\bar{x}^2 + \\bar{x}^2\\\\\n&= \\frac{1}{n} \\sum_{i=1}^n x_i^2 - \\bar{x}^2. \n\\end{aligned}\n$$\n\n::: \n  \nEn R es mucho más fácil que estos cálculos tediosos. Esto se hace con el comando de R Base `var()`. No obstante, hay que tener en cuenta que `var()` es por defecto la varianaza muestra. Asumiendo que tenemos datos poblaciales podemos hacer dos cosas^[Esto no lo haré para los datos muestrales de la CASEN 2022. Principalmente, porque para eso usaremos, más adelante, paquetes especializados, como `survey` y `srvyr`]. Veamos cómo: \n  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Datos de ejemplo\nx <- c(0.7, 1.2, 0.9, 0.6, 1.4, 1.8, 2.5)\n\n# Varianza poblacional\nvar_poblacional <- mean((x - mean(x))^2)\n\n# Varianza simplificada\nvar_pobl_simplificada <- mean(x^2) - mean(x)^2\n\n# Varianza muestral\nvar_muestral <- var(x)\n\n# Varianza muestral de casen2022$ytrabajocor\nvar_casen_muestral <- var(casen2022$ytrabajocor, na.rm = TRUE)\n\n# Mostrar resultados\nvar_poblacional\n## [1] 0.3885714\nvar_pobl_simplificada\n## [1] 0.3885714\nvar_muestral\n## [1] 0.4533333\nvar_casen_muestral\n## [1] 665844368478\n```\n:::\n\n\n\n\n### La desviación estándar\n\nUna vez se tiene la varianza, la desviación estándar es muy fácil de calcular, pues es simplemente la raíz cuadrada de la varianza. La *desviación estándar* también es una medida de dispersión de los elementos del conjunto de observaciones con respecto a su media. \n\nDadas $n$ observaciones, $x_1, \\ldots, x_n $, la desviación estándar de este conjunto de datos, que denotaremos por $\\hat{\\sigma}$, está dada por la raíz cuadrada de la varianza:\n$$\n\\hat{\\sigma} = \\sqrt{\\hat{\\sigma}^2} = \\sqrt{\\frac{(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + \\ldots + (x_n - \\bar{x})^2}{n}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}.\n$$\nPor ejemplo, si \n$$\nx_1 = 0.7; \\, x_2 = 1.2; \\, x_3 = 0.9; \\, x_4 = 0.6; \\, x_5 = 1.4; \\, x_6 = 1.8; \\, x_7 = 2.5,\n$$\nentonces, la desviación estándar de estos datos es: $\\hat{\\sigma} = 0.62$. A diferencia de lo que ocurre con la varianza, la desviación estándar está medida en las mismas unidades que el conjunto de observaciones bajo análisis. Como veremos más adelante, esto hace que su interpretación sea más fácil y explica por qué se usa más que la varianza como medida de dispersión. Por lo tanto, la ventaja principal es esa. La desviación estándar está medida en las mismas unidades que las observaciones que estamos trabajando, y no así la varianza. En R es muy simple de hacer, con `sd()`. Al igual que con `var()`, `sd()` calcula por defecto al desviación estándar muestral. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Datos de ejemplo\nx <- c(0.7, 1.2, 0.9, 0.6, 1.4, 1.8, 2.5)\n\n# Desviación estándar poblacional\nsd_poblacional <- sqrt(mean((x - mean(x))^2))\n\n# Desviación estándar simplificada\nsd_simplificada <- sqrt(mean(x^2) - mean(x)^2)\n\n# Desviación estándar muestral\nsd_muestral <- sd(x)\n\n# Desviación estándar muestral de casen2022$ytrabajocor\nsd_casen_muestral <- sd(casen2022$ytrabajocor, na.rm = TRUE)\n\n\n# Mostrar resultados\nsd_poblacional\n## [1] 0.623355\nsd_simplificada\n## [1] 0.623355\nsd_muestral\n## [1] 0.6733003\nsd_casen_muestral\n## [1] 815992.9\n```\n:::\n\n\n\n\nConsiderando la desviación estándar es una medida de dispersión que describe la descripción de los datos en las mismas unidades que el conjunto de observaciones nos sirve para comparar ahora `var_casen_muestral` con `sd_casen_muestral`. En el primer caso, `var_casen_muestral = 665844368478`, significa simplemente que la varianza de los datos es de ese valor, esa es su varianza. En cambio, `sd_casen_muestral = 815992.9` implica que la variación de los datos es de 815.992 CLP, pues la variable `ytrabajocor` mide pesos chilenos. En el caso de la varianza, nos dice cuán dispersos están los datos respecto a la media. En el caso de la desviación estándar, es también una medida de dispersión de los elementos del conjunto de observaciones con respecto a su media, pero que está en las unidades de la variable en cuestión, es decir, que hay una dispersión de los datos de 815.992 CLP respecto de la media (667.690 CLP, como habíamos visto). Por cierto, los valores elevados de dispersión son típicos en los ingresos, pues dada la desigualdad en estos, suelen tener una alta dispersión los datos de ingresos. \n\n\n\n### 1.2.1. Robustez en las medidas de dispersión \n\nAhora bien, **¿cuál es más robusta?** La *desviación estándar muestral* y la *varianza muestral* no son medidas robustas de la dispersión de los datos. Aunque el álgebra es un poco más complejo, el problema es similar al que vimos con la media muestral.\n\nPor ejemplo, denotando por $\\hat{\\sigma}^2(e)$ la varianza muestral cuando $x_1$ se mide con error $e$, se tiene que, usando el resultado de que la varianza muestral es igual a la media muestral de los cuadrados menos el cuadrado de la media muestral:\n\n:::{.proof}\n\n$$\n\\begin{aligned}\n\\hat{\\sigma}^2(e) &= \\frac{1}{n} \\left[ (x_1 + e)^2 + \\sum_{i=2}^{n} x_i^2 \\right] - \\left( \\frac{1}{n} \\left[ (x_1 + e) + x_2 + \\ldots + x_n \\right] \\right)^2 \\\\\n&= \\frac{1}{n} \\sum_{i=1}^{n} x_i^2 + \\frac{x_1 e}{n} + \\frac{e^2}{n} - \\left( \\bar{x} + \\frac{e}{n} \\right)^2 \\\\\n&= \\frac{1}{n} \\sum_{i=1}^{n} x_i^2 + \\frac{x_1 e}{n} + \\frac{e^2}{n} - \\bar{x}^2 - 2 \\bar{x} \\frac{e}{n} - \\frac{e^2}{n^2} \\\\\n&= \\hat{\\sigma}^2(0) + \\frac{2}{n}(x_1 - \\bar{x})e + \\frac{n-1}{n^2}e^2.\n\\end{aligned}\n$$\nDe donde se concluye que \n$$\n\\lim_{e \\to \\pm \\infty} \\hat{\\sigma}^2(e) = +\\infty,\n$$\ny, por lo tanto, también que \n$$\n\\lim_{e \\to \\pm \\infty} \\hat{\\sigma}(e) = +\\infty.\n$$\n\n::: \n\nEn simple, las dos medidas de dispersión, la varianza y la desviación estándar, tienden a infinito positivo cuando el término de error tiende - o + infinito. Por lo tanto, ambas son medidas que no son robustas y, por lo tanto, sensibles a la presencia de valores extremos, de *outliers*. \n\n### El rango\n\nEl *rango* es la diferencia entre los valores **más grande** y **más pequeño** de un conjunto de observaciones de una variable. Al tomar el valor máximo y mínimo de los datos, permite obtener una idea de la *dispersión* de los datos. Así, cuanto mayor es el rango, aún más dispersos están los datos. Por ejemplo, si\n$$\nx_1 = 0.7; \\quad x_2 = 1.2; \\quad x_3 = 0.9; \\quad x_4 = 0.6; \\quad x_5 = 1.4; \\quad x_6 = 1.8; \\quad x_7 = 2.5,\n$$\nentonces, como ya vimos:\n$$\ny_1 = 0.6, \\quad y_2 = 0.7, \\quad y_3 = 0.9, \\quad y_4 = 1.2, \\quad y_5 = 1.4, \\quad y_6 = 1.8, \\quad y_7 = 2.5.\n$$\nEntonces el rango está dado por:\n$$\ny_7 - y_1 = 2.5 - 0.6 = 1.9.\n$$\n\nComo ya se puede intuir a estas alturas, el rango tampoco es una medida de dispersión robusta, pues basta un *outlier* en los valores mínimos o máximos para que el rango se distorsiones. Así, aunque el rango de un conjunto de datos es simple de calcular, su valor no es muy útil porque depende fuertemente de valores extremos y, por tanto, no es robusto a la presencia de *outliers*. Formalmente, si tomamos el ejemplo anterior, tendríamos que \n$$\n\\begin{aligned}\ny_1 = 0.6, \\quad y_2 = 0.7, \\quad y_3 = 0.9, &\\quad y_4 = 1.2, \\quad y_5 = 1.4, \\quad y_6 = 1.8, \\quad y_7 = 2.5.\\\\\ny_7 - y_1 = &2.5 - 0.6 = 1.9.\n\\end{aligned}\n$$\nAhora bien, suponiendo nuevamente que la primera observación se midió con error $e$, de modo que $x_1 = 0.7 + e$, y denotando por $y_7(e)$ y $y_1(e)$ al valor máximo y mínimo cuando $x_1$ se reemplaza por $x_1 + e$, respectivamente, tenemos que:\n$$\n\\lim_{e \\to -\\infty}(y_7(e) - y_1(e)) = \\infty, \\quad \\lim_{e \\to +\\infty}(y_7(e) - y_1(e)) = \\infty.\n$$\nPor lo que valores aberrantes o *outliers* tienen efectos brutales en el rango. En R es muy fácil de hacer, se puede \"manualmente\" o con `range()` y `diff()`. Lo mostramos y luego pasamos a la primera medida de dispersión robusta: el rango intercuartil. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\n# Para obtener el rango (max–min) usa diff(range(x, na.rm=TRUE))\nx <- c(0.7, 1.2, 0.9, 0.6, 1.4, 1.8, 2.5)\n\n# Rango con max–min\nr1 <- max(x) - min(x)\n\n# Rango con diff(range())\nr2 <- diff(range(x, na.rm = TRUE)) # range(x, na.rm=TRUE) devuelve c(min, max), no la diferencia\n\n# En tu variable de casen2022\nr_casen_1 <- max(casen2022$ytrabajocor, na.rm = TRUE) - \n             min(casen2022$ytrabajocor, na.rm = TRUE)\nr_casen_2 <- diff(range(casen2022$ytrabajocor, na.rm = TRUE))\n\n# Mostrar resultados\nr1\n## [1] 1.9\nr2\n## [1] 1.9\nr_casen_1\n## [1] 50966585\nr_casen_2\n## [1] 50966585\n```\n:::\n\n\n\n\n### El rango intercuartil\n\nEl **primer cuartil** de un conjunto de observaciones de una variable, que denotaremos por $q_1$, es el valor para el cual el 25\\% de las observaciones son más pequeñas y el 75\\% son más grandes. En otras palabras, el primer cuartil es la mediana del 50\\% más pequeño de los datos.\n\nEl **segundo cuartil**, $q_2$, es el mismo valor que aquel de la mediana (50\\% son más pequeños, 50\\% son más grandes).\n\nEl **tercer cuartil**, $q_3$, es el valor para el cual el 25\\% de las observaciones son más grandes y el 75\\% son más pequeñas. En otras palabras, el tercer cuartil es la mediana del 50\\% más grande de los datos.\n\nEl **rango intercuartil**, que denotaremos \\textbf{RIC}, está dado por: $q_3 - q_1$.\n\nNotar que el RIC es el **rango** del conjunto de observaciones con los cuartos más pequeño y más grande eliminados (*podados*), similar en espíritu a las observaciones que se consideran para calcular la media podada.Entonces, formalicemos su robustez.\n\nPor ejemplo, supongamos que las observaciones (ordenadas de menor a mayor) son:\n$$\n43, 48, 50, 50, 52, 53, 56, 58, 59, 60, 62, 65, 66, 68, 70, 71, 74, 76, 78, 80,\n$$\nentonces los cuartiles de este conjunto de observaciones son: \n$$\nq_1 = 52.5, \\quad q_2 = 61, \\quad q_3 = 70.5\n$$\ny el RIC viene dado por $q_3 - q_1 = 18$\n\nAhora bien, suponiendo que el valor 43 se midió con un error $e$, de modo que su valor es $43 + e$, y denotando por $RIC(e)$ el valor del rango intercuartil cuando el valor 43 se reemplaza por $43 + e$, tenemos que: \n$$\nRIC(e) \\in [16,18]\n$$ \nDe modo que **el RIC es una medida de dispersión de los datos robusta a la presencia de _outliers_**. Para calcular cuartiles en R se usa `quantile()`. Lo importante es especificar adecuadamente el cuantil de interés (50%, 75%, etc.) A su vez, entre los paquete de R base, específicamente en `stats`, se tiene `IQR()` nos permite calcular rangos intercuartiles. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Datos de ejemplo\nx <- c(43, 48, 50, 50, 52, 53, 56, 58, 59, 60, 62, 65, 66, 68, 70, 71, 74, 76, 78, 80)\n\n# Calcular cuartiles\nq1   <- quantile(x, 0.25)\nq2   <- median(x)\nq3   <- quantile(x, 0.75)\n\n# Rango intercuartil\nric  <- q3 - q1\n\n# Para casen2022$ytrabajocor\nx_casen    <- casen2022$ytrabajocor\nq1_casen   <- quantile(x_casen, 0.25, na.rm = TRUE)\nq2_casen   <- median(x_casen, na.rm = TRUE)\nq3_casen   <- quantile(x_casen, 0.75, na.rm = TRUE)\nric_casen  <- q3_casen - q1_casen\n\n# Alternativa con función base\nric_casen2 <- IQR(x_casen, na.rm = TRUE)\n\n# Mostrar resultados\nq1; q2; q3; ric\n##   25% \n## 52.75\n## [1] 61\n##   75% \n## 70.25\n##  75% \n## 17.5\nq1_casen; q2_casen; q3_casen; ric_casen; ric_casen2\n##    25% \n## 337500\n## [1] 480000\n##    75% \n## 763333\n##    75% \n## 425833\n## [1] 425833\n```\n:::\n\n\n\n\n\n## 1.3. Medidas de posición\n\n### Los percentiles \n\nEl percentil de un conjunto de observaciones es una medida de posición que indica, una vez ordenados los datos de menor a mayor, el valor por debajo del cual se encuentra un porcentaje dado de observaciones. En general, el percentil $k$-ésimo, que denotaremos por $p_k$, es un valor para el cual un $k\\%$ por ciento de las observaciones son menores que ella.\n\nLa definición anterior es un tanto vaga: ¿menor o igual?, ¿qué pasa si no hay valores que cumplan?, ¿qué pasa si hay varios? Se pueden adoptar varios criterios, todos los cuales arrojan resultados parecidos cuando se aplican con conjuntos grandes de observaciones.\n\nNotar que, en general, $p_{25} = q_1$, $p_{50}$ es la mediana y $p_{75} = q_3$. Por ejemplo, si las observaciones (ordenadas de menor a mayor) son: \n$$\n43, 48, 50, 50, 52, 53, 56, 58, 59, 60, 62, 65, 66, 68, 70, 71, 74, 76, 78, 80,\n$$\ny exigimos que los percentiles sean observaciones y la definición es con \"menor o igual\", tenemos: \n$$\np_{10} = 48, \\quad p_{25} = 52, \\quad p_{50} = 60, \\quad p_{75} = 68, \\quad p_{100} = 76.\n$$\nEn R no hay una distinción entre cuantiles y percentiles a nivel de comando. Ahora bien, para obtener los percentiles en la posición que nos interesan podemos hacer lo siguiente: \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Datos de ejemplo\nx <- c(43, 48, 50, 50, 52, 53, 56, 58, 59, 60, 62, 65, 66, 68, 70, 71, 74, 76, 78, 80)\n\n# Definimos percentiles deseados\np <- c(0.10, 0.25, 0.50, 0.75, 1.00)\n\n# Calculamos percentiles como observaciones (type = 1)\npercentiles <- quantile(x, probs = p, na.rm = TRUE, type = 1)\np10   <- percentiles[1]\np25   <- percentiles[2]\np50   <- percentiles[3]\np75   <- percentiles[4]\np100  <- percentiles[5]\n\n# Para casen2022$ytrabajocor\nx_casen <- casen2022$ytrabajocor\npercentiles_casen <- quantile(x_casen, probs = p, na.rm = TRUE, type = 1)\np10_casen  <- percentiles_casen[1]\np25_casen  <- percentiles_casen[2]\np50_casen  <- percentiles_casen[3]\np75_casen  <- percentiles_casen[4]\np100_casen <- percentiles_casen[5]\n\n# Mostrar resultados\npercentiles\n##  10%  25%  50%  75% 100% \n##   48   52   60   70   80\npercentiles_casen\n##      10%      25%      50%      75%     100% \n##   150000   337500   480000   763333 50966668\n```\n:::\n\n\n\n\n## 1.4. Transformaciones lineales de los datos descriptivos \n\nUna transformación lineal de un conjunto de datos es aquella en la que cada observación se multiplica por una constante y se le suma otra constante. Concretamente, dadas $n$ observaciones, $x_1, x_2, \\dots, x_n$, una transformación lineal de ellas está dada por \n$$\nax_1 + b, ax_2 + b, \\dots, ax_n + b,\n$$ \ncon $a$ y $b$ constantes cualesquiera. \n\nDe tal modo, las transformaciones lineales permiten transformar los datos de las estadísticas descriptivas transformadas linealmente con las estadísticas descriptivas originales. Un ejemplo de esto, podría ser la transformación de los datos de precios a los mismos precios en UF de algún día dado. Con ello, podríamos ver, e.g., el ingreso en pesos y queremos verlo en UF. De tal modo, podríamos comprar el ingreso original en pesos con el ingreso en UF sin distorsionar la proporción mediante esta multiplicación con constantes (ponderar en la misma cantidad básicamente). En nuestro ejemplo, de CLP $\\to$ UF tendríamos que la constante $a$ sería el valor de la UF y la constante $b$ simplemente sería 0.  \n\nLo que se sigue de esto, es determinar la relación entre las medidas que hemos visto hasta hora, en sus estadísticas descriptivas originales, y compararlas con sus respectivas transformaciones lineales. \n\n- Si denotamos por $\\bar{x}$ a la *media* de los valores de las observaciones originales y por $\\bar{x}_t$ a la media de los datos después de aplicarles la transformación lineal, tenemos que \n$$\n\\bar{x}_t = \\frac{ax_1 + b + ax_2 + b + \\dots + ax_n + b}{n} = a\\bar{x} + b.\n$$\n\n- Si denotamos por $\\hat{\\sigma}^2$ a la *varianza* de los valores de las observaciones originales y por $\\hat{\\sigma}^2_t$ a la varianza de los datos después de aplicarles la transformación lineal, tenemos que\n$$\n\\hat{\\sigma}^2_t = \\frac{(ax_1 + b - \\bar{x}_t)^2 + (ax_2 + b - \\bar{x}_t)^2 + \\dots + (ax_n + b - \\bar{x}_t)^2}{n} = a^2 \\hat{\\sigma}^2.\n$$\n- De modo que para el caso de la *desviación estándar*, tenemos que: \n$$\n\\hat{\\sigma}_t = |a| \\hat{\\sigma}.\n$$\n- Si denotamos por $\\tilde{\\tilde{x}}$ a la *moda* de los valores originales y por $\\tilde{\\tilde{x}}_t$ a la moda de los datos transformados, tenemos que: \n$$\n\\tilde{\\tilde{x}}_t = a \\tilde{\\tilde{x}} + b.\n$$\n- Si denotamos por $y_1, y_2, \\dots, y_n$ a los *estadísticos de orden* de los valores originales de los datos, tenemos que los estadísticos de orden de los datos transformados linealmente están dados por $a y_1 + b, a y_2 + b, \\dots, a y_n + b$ si $a \\geq 0$, o por $a y_n + b, a y_{n-1} + b, \\dots, a y_1 + b$ si $a < 0$.\n- Si denotamos por $\\bar{x}$ a la *mediana* de las observaciones originales y por $\\tilde{x}_t$ a la mediana de los datos después de aplicarles la transformación lineal, tenemos que: \n$$\n\\tilde{x}_t = a \\tilde{x} + b.\n$$\n- El rango de los datos transformados está dado por $\\left| a \\right| (y_n - y_1)$.\n- El rango intercuartil de los datos después de aplicarles la transformación lineal está dado por $\\left| a \\right| (q_3 - q_1)$, donde $q_3$ y $q_1$ son los cuartiles 3 y 1 de los datos originales, respectivamente.\n\n\n# 2. Estadística descriptiva y visualización de los datos\n\n\nLa **visualización de datos** es otro paso importante en el *análisis exploratorio de datos* y, por tanto, para la *estadística descriptiva*. La **visualización de datos** es el proceso de resumir gráficamente la información, por lo que suelen ser fundamentales para las estadísticas descriptiva. Su utilidad radica en que nos ayuda a interpretar los datos disponibles y a detectar patrones, tendencias y anomalías en ellos. Además, la visualización de datos es una herramienta que nos permite comunicar claramente ideas complejas de una forma atractiva. Concretamente, el **gráfico** presenta lo que los números no pueden comunicar por sí mismos, y lo transmite de una manera visible y más fácil de asimilar y de recordar. Un **(buen) gráfico** vale más que mil palabras. Y el propósito principal de graficar los datos es **comunicar** información de los datos.\n\nEn R, existe el comando `plot()` en los paquetes bases. Pero se suele graficar con el paquete `ggplot2`. También hay paquetes basados en `ggplot2` pero que incorporan otro tipos de gramáticas, como `tidyplots` que se especializa en gráficos para artículos científicos. Por no ser un apunte sobre cómo usar paquetes, usaré `ggplot2`. También usaré `scales` que complementará `ggplot2` ajustando escalas. No obstante, ambos paquetes tienen múltiples páginas webs, tutoriales, etc., sobre cómo usarlos. También cargamos la CASEN 2022 nuevamente\n\n\n\n## Tipos de gráficos \n\nLa elección de la **visualización de datos* depende de varios elementos, pero principalmente de \n\n- Qué tipo de variable se quiere investigar/presentar.\n\n- Lo que esperamos mostrar. \n\nHay gráficos que son más adecuados para presentar ciertos tipos de datos, al igual que hay gráficos que no sirven para visualizar algunas variables según su naturaleza. Al mismo tiempo, dependerá de dónde y a quién se quiera presentar los gráficos. Dado que nuestra intención es *comunicar* información de los datos de manera gráfica, es importante tener en consideración si los gráficos se expondran en un artículo científico, una presentación en un congreso, en una afiche divulgativo, para una clase, etc. No obstante, aquí dejo una pauta general de qué gráficos usar según los tipos de variables y cuántas variables queramos comunicar. \n\n1. Los **gráficos univariados**: muestran la distribución básica de una variable. Para ello utilizamos \n  a. Gráficos de barras,\n  b. Histogramas y\n  c. Gráficos de caja (boxplots)\n  \n2. Los **gráficos bivariados**: muestran cómo dos variables están *relacionadas* entre sí. Cuál usar depende del tipo de variables bajo análisis:\n  a. Dos variables categóricas: gráficos de barras.\n  b. Una categórica y una cuantitativa: gráficos de cajas.\n  c. Dos variables cuantitativas: gráficos de dispersión de puntos.\n  \n  \nVeamos cómo graficar estos tipos de gráficos en R usando `ggplot2` y datos reales de la CASEN 2022.   \n\n## Gráficos univariados\n\n### Gráficos de barras\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Vemos datos de nuestra variable de interés\n\nsjmisc::frq(casen2022$sexo) # ups! no funciona en RMarkdown\n## Sexo (x) <numeric> \n## # total N=202231 valid N=202231 mean=1.53 sd=0.50\n## \n## Value |     Label |      N | Raw % | Valid % | Cum. %\n## -----------------------------------------------------\n##     1 | 1. Hombre |  95656 | 47.30 |   47.30 |  47.30\n##     2 |  2. Mujer | 106575 | 52.70 |   52.70 | 100.00\n##  <NA> |      <NA> |      0 |  0.00 |    <NA> |   <NA>\n\n\n# Gráfico de barras de la variable sexo\nggplot(casen2022, aes(x = factor(sexo, labels = c(\"Hombre\", \"Mujer\")))) +\n  geom_bar() +\n  labs(\n    x = \"Sexo\",\n    y = \"Frecuencia\",\n    title = \"Distribución de la variable Sexo\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/eg-barplots-1.png){width=672}\n:::\n:::\n\n\n\n\nSi queremos agregarle más colores^[Yo uso las paletas de colores de [aquí](https://r-charts.com/es/colores/)], cambiar el tamaño de las letras, ponerle caption, etc., solo hay que usar comandos de `ggplot2` que sirvan para ello y seguir su gramática \"en capas\" adecuadamente \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Gráfico de barras de la variable sexo más bonito\nggplot(casen2022, aes(x = factor(sexo, labels = c(\"Hombre\", \"Mujer\")), \n                      fill = factor(sexo))) +\n  geom_bar(width = 0.7, color = \"black\") +\n  scale_fill_manual(values = c(\"#8F8F8F\", \"#8B0000\")) +\n  labs(\n    x = \"Sexo\",\n    y = \"Frecuencia\",\n    title = \"Distribución de Sexo en CASEN 2022\", \n    caption = \"Elaboración propia a partir de Encuesta CASEN 2022\"\n  ) +\n  theme_minimal(base_family = \"Fira Sans\", base_size = 12) +\n  theme(\n    plot.title     = element_text(size = 18, face = \"bold\", hjust = 0.5),\n    axis.title     = element_text(size = 14),\n    axis.text      = element_text(size = 12),\n    panel.grid.minor = element_blank(),\n    legend.position = \"none\"\n  )\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/eg-barplots2-1.png){width=672}\n:::\n:::\n\n\n\n  \nMás allá de \"enchular\" el gráfico, lo importante es que el gráfico es más fácil de comprender rápidamente que una tabla de frecuencias, como vimos con `frq()`. Y cuando se agrega más categorías, facilita aún más. Por ejemplo, veamos Clasificación Internacional Normalizada de Educación (CINE-F). Para ello usaremos `cinef13_area` de la CASEN 2022, pero excluiremos las categorías sin datos \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Exploramos nuestra variable de interés\n\nfrq(casen2022$cinef13_area)\n## Clasificación Internacional Normalizada de Educación (CINE-F). Campo amplio (x) <numeric> \n## # total N=202231 valid N=49628 mean=4.13 sd=6.48\n## \n## Value |                                                Label |      N | Raw %\n## -----------------------------------------------------------------------------\n##     1 |                                    Salud y Bienestar |   9383 |  4.64\n##     2 |                 Ingeniería, Industria y Construcción |  10833 |  5.36\n##     3 |                                            Educación |   7819 |  3.87\n##     4 |                                            Servicios |   2806 |  1.39\n##     5 |                 Administración de Empresas y Derecho |  10933 |  5.41\n##     6 |          Ciencias Sociales, Periodismo e Información |   1917 |  0.95\n##     7 |        Ciencias naturales, matemáticas y estadística |    654 |  0.32\n##     8 |       Agricultura, Silvicultura, Pesca y Veterinaria |   1080 |  0.53\n##     9 | Tecnología de la Información y la Comunicación (TIC) |   1843 |  0.91\n##    10 |                                  Artes y Humanidades |   2107 |  1.04\n##    11 |                        Doctorado en Ciencias Básicas |      0 |  0.00\n##    88 |                                             Sin dato |    253 |  0.13\n##  <NA> |                                                 <NA> | 152603 | 75.46\n## \n## Value | Valid % | Cum. %\n## ------------------------\n##     1 |   18.91 |  18.91\n##     2 |   21.83 |  40.74\n##     3 |   15.76 |  56.49\n##     4 |    5.65 |  62.14\n##     5 |   22.03 |  84.17\n##     6 |    3.86 |  88.04\n##     7 |    1.32 |  89.35\n##     8 |    2.18 |  91.53\n##     9 |    3.71 |  95.24\n##    10 |    4.25 |  99.49\n##    11 |    0.00 |  99.49\n##    88 |    0.51 | 100.00\n##  <NA> |    <NA> |   <NA>\n\n\n# Como vemos Doctorado en Ciencias Básicas no tiene casos\n# y Sin dato no nos interesa. Tampoco NA\n# Usamos dplyr\n\narea_labels <- c(\n  \"Salud y Bienestar\",\n  \"Ingeniería, Industria y Construcción\",\n  \"Educación\",\n  \"Servicios\",\n  \"Administración de Empresas y Derecho\",\n  \"Ciencias Sociales, Periodismo e Información\",\n  \"Ciencias naturales, matemáticas y estadística\",\n  \"Agricultura, Silvicultura, Pesca y Veterinaria\",\n  \"Tecnología de la Información y la Comunicación (TIC)\",\n  \"Artes y Humanidades\"\n)\n\ndf_area <- casen2022 |> \n  filter(!cinef13_area %in% c(11, 88, NA)) |> \n  count(cinef13_area) |> \n  mutate(area = factor(cinef13_area, levels = 1:10, labels = area_labels))\n\nggplot(df_area, aes(x = area, y = n, fill = area)) +\n  geom_col(color = \"white\", width = 0.8) +\n  labs(\n    x = \"Área CINE-F\",\n    y = \"Frecuencia\",\n    title = \"Distribución de Áreas CINE-F (códigos 1–10)\"\n  ) +\n  theme_minimal(base_family = \"Fira Sans\", base_size = 12) +\n  theme(\n    axis.text.x    = element_text(angle = 45, hjust = 1),\n    plot.title     = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    legend.position = \"none\"\n  )\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/eg-barplots3-1.png){width=672}\n:::\n:::\n\n\n\n  \nEsto es mucho más amable que una tabla en la que comparamos números, porque permite una comparación visual entre categorías. Entonces, la ganancia del gráfico de barra se da en la medida que haya más categorías a analizar. \n  \n\n### Histogramas (y kernels)\n\nEste tipo de visualizaciones es recomendable para resumir gráficamente la información de una **variable cuantitativa**. Un histograma muestra la distribución empírica de las observaciones de un variable cuantitativa. Muchas veces, por ejemplo, se usa para observar si una variable sigue o no una distribución normal. \n\nEn un histograma, la altura de cada barra representa cuántas observaciones en el conjunto de datos caen dentro de un cierto rango (intervalo) de la variable cuantitativa bajo análisis. Un elemento fundamental al momento de hacer un histograma es la definición del número de rangos (intervalos) de la variable cuantitativa que se considerarán para agrupar los datos. Si no se elige el número correcto (ancho adecuado) para dichos intervalos, se corre el riesgo de que el histograma no refleje las características esenciales del conjunto de datos, llevándonos a malinterpretar cómo se distribuyen las observaciones. \n  \nVeamos la distribución de los ingresos del trabajo en un histograma: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(casen2022, aes(x = ytrabajocor)) +\n  geom_histogram(bins = 30, fill = \"#4E79A7\", color = \"white\", na.rm = TRUE) +\n  scale_x_continuous(labels = scales::comma) + # para sacar notación científica\n  labs(\n    x = \"Ingreso de trabajo corregido\",\n    y = \"Frecuencia\",\n    title = \"Histograma de ytrabajocor\"\n  ) +\n  theme_minimal(base_family = \"Fira Sans\", base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    axis.title = element_text(size = 14),\n    axis.text  = element_text(size = 12)\n  )\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/eg-histogram-1.png){width=672}\n:::\n:::\n\n\n\n\nComo se ve, la desigualdad de los ingresos nuevamente ataca. Al haber pocas personas con ingresos muy altos, y muchas personas con ingresos bajos, ni se alcanzan a ver las personas de ingresos altos. Ciertamente, esta forma de elefante dentro de una serpiente hacia un lado del gráfico es habitual verla en gráficos de distribución en los ingresos. Podemos para ver cómo varía esto, por ejemplo, usar percentiles, deciles, quintiles, etc., para filtar casos y observar la distribución empírica de los ingresos, e.g., para el 90% de los ingresos (excluyendo al 10% más alto):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncasen2022 |> \n  filter(!is.na(ytrabajocor), dau < 10) |> \n  ggplot(aes(x = ytrabajocor)) +\n    geom_histogram(bins = 30, fill = \"#4E79A7\", color = \"white\") +\n    scale_x_continuous(labels = comma) +\n    labs(\n      x = \"Ingreso de trabajo corregido\",\n      y = \"Frecuencia\",\n      title = \"Histograma de ytrabajocor (deciles 1–9)\"\n    ) +\n    theme_minimal(base_family = \"Fira Sans\", base_size = 12) +\n    theme(\n      plot.title     = element_text(face = \"bold\", size = 16, hjust = 0.5),\n      axis.title     = element_text(size = 14),\n      axis.text      = element_text(size = 12)\n    )\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/eg-histogram2-1.png){width=672}\n:::\n:::\n\n\n\n\n¿Y si queremos, por ejemplo, para el 80%? Solo basta modificar `dau < x` en el código, pues `dau` es la variable del Decil autónomo nacional dentro de la CASEN 2022. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncasen2022 |> \n  filter(!is.na(ytrabajocor), dau < 9) |> \n  ggplot(aes(x = ytrabajocor)) +\n    geom_histogram(bins = 30, fill = \"#4E79A7\", color = \"white\") +\n    scale_x_continuous(labels = comma) +\n    labs(\n      x = \"Ingreso de trabajo corregido\",\n      y = \"Frecuencia\",\n      title = \"Histograma de ytrabajocor (deciles 1–8)\"\n    ) +\n    theme_minimal(base_family = \"Fira Sans\", base_size = 12) +\n    theme(\n      plot.title     = element_text(face = \"bold\", size = 16, hjust = 0.5),\n      axis.title     = element_text(size = 14),\n      axis.text      = element_text(size = 12)\n    )\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/eg-histogram3-1.png){width=672}\n:::\n:::\n\n\n\n\n\nAlgo que también se puede hacer, similar a los histogramas, es observar esto con *kernels*, que son como histogramas pero con densidad y continuos (no en barras). Esto se hace con `geom_density()`. Además, esto nos facilita (para visualizar la distribución empírica) ver nuestra variable de interés agrupada según otra variable que nos interese. Por ejemplo, podríamso ver el ingreso del trabajo según sexo. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\ncasen2022 |> \n  filter(!is.na(ytrabajocor), dau < 10) |> \n  mutate(sexo = factor(sexo, levels = c(1, 2), labels = c(\"Hombre\", \"Mujer\"))) |> \n  ggplot(aes(x = ytrabajocor, fill = sexo)) +\n    geom_density(alpha = 0.7,adjust =2, color = \"black\") +\n    scale_x_continuous(labels = comma) +\n    labs(\n      title = \"Distribución de ingreso de trabajo corregido por sexo\",\n      x     = \"Ingreso de trabajo corregido\",\n      y     = \"Densidad\",\n      fill  = \"Sexo\"\n    ) +\n    theme_minimal(base_family = \"serif\", base_size = 12) +\n    theme(\n      plot.title     = element_text(face = \"bold\", size = 16, hjust = 0.5),\n      legend.position = \"bottom\",\n      axis.text       = element_text(size = 12)\n    )\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/eg-kernels-1.png){width=672}\n:::\n:::\n\n\n\n\n\nOtra cosa que puede resultarnos util es ver donde está mediana en el gráfico. Aprovechamos de mostrar como se verían los gráficos ya no superpuestos, sino al lado: \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\ndf <- casen2022 |>\n  filter(!is.na(ytrabajocor), dau < 10) |>\n  mutate(sexo = factor(sexo, levels = c(1, 2), labels = c(\"Hombre\", \"Mujer\")))\n\n# Extraemos medianas\nmedianas <- df |>\n  group_by(sexo) |>\n  summarise(mediana = median(ytrabajocor))\n\n# Graficamos \nggplot(df, aes(x = ytrabajocor, fill = sexo, color = sexo)) +\n  geom_density(alpha = 0.3, adjust = 1, size = 1) +\n  geom_vline(data = medianas, aes(xintercept = mediana, color = sexo),\n             linetype = \"dashed\", size = 1) +\n  facet_wrap(~ sexo) +\n  scale_fill_manual(values = c(\"Hombre\" = \"#C51517\", \"Mujer\" = \"#BAB3EB\")) +\n  scale_color_manual(values = c(\"Hombre\" = \"#9C0824\", \"Mujer\" = \"#312271\")) +\n  scale_x_continuous(labels = comma) +\n  labs(\n    title = \"Distribución de ingreso de trabajo corregido por sexo\",\n    x     = \"Ingreso de trabajo corregido\",\n    y     = \"\",\n    fill  = \"Sexo\",\n    color = \"Sexo\"\n  ) +\n  theme_minimal(base_family = \"serif\", base_size = 12) +\n  theme(\n    plot.title     = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    legend.position = \"bottom\",\n    axis.text       = element_text(size = 12)\n  )\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/eg-kernels2-1.png){width=672}\n:::\n:::\n\n\n\n\n### Gráfico de cajas (boxplots)\n\nCuando son gráficos univariados este tipo de visualizaciones tipo *boxplot* es recomendable para resumir gráficamente la información de una **variable cuantitativa** (continua o discreta). Un grafico de caja o *boxplot* muestra las **características de la distribución** de las observaciones de una variable cuantitativa: “mínimo”, primer cuartil ($q_1$), mediana, tercer cuartil ($q_3$), “máximo”. También etiqueta posibles valores atípicos como puntos separados en el gráfico. En un gráfico de caja, pues, se tiene que:\n\n- El “mínimo” corresponde a $q_1 - 1,5RIC$ (no es el valor más pequeño entre las observaciones).\n\n- El “máximo” corresponde a $q_3 + 1,5RIC$ (no es el valor más grande entre las observaciones).\n\nEn esta herramienta gráfica, las observaciones etiquetadas como posibles valores atípicos (*outliers*) son aquellas que se ubican fuera del intervalo dado por:\n$$\n[q_1 - 1,5RIC, q_3 + 1,5RIC]\n$$\n\nVeamos como hacer esto con `ggplot2`. Usaremos otra variable cuantitativa, y ya no ingresos del trabajo porque, dada su dispersión se verá así: \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Para ingresos del trabajo\n\n\nggplot(casen2022, aes(y = ytrabajocor)) +\n  geom_boxplot(fill = \"#4E79A7\", color = \"black\", outlier.color = \"red\", na.rm = TRUE) +\n  scale_y_continuous(labels = comma) +\n  labs(\n    y     = \"Ingreso de trabajo corregido\",\n    title = \"Boxplot de ingreso de trabajo corregido\"\n  ) +\n  theme_minimal(base_family = \"Fira Sans\", base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    axis.text  = element_text(size = 12)\n  )\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/eg-boxplot-1.png){width=672}\n:::\n:::\n\n\n\n\nProbemos con otra variable que sea interesante, por ejemplo con la varriable `tot_per_h` que registra el \"Total de personas en el hogar\" de los hogares encuestados por la CASEN\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\n# Para Total de personas en el hogar\n\ncasen2022 |>\n  filter(!is.na(tot_per_h)) |>\n  ggplot(aes(y = tot_per_h)) +\n    geom_boxplot(fill = \"#CAE1FF\", color = \"black\", outlier.color = \"red\") +\n    scale_y_continuous(breaks = 1:13, limits = c(1, 13)) +\n    labs(\n      y     = \"Total de personas en el hogar\",\n      title = \"Boxplot de total de personas en el hogar\"\n    ) +\n    theme_minimal(base_family = \"Fira Sans\", base_size = 12) +\n    theme(\n      plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n      axis.text  = element_text(size = 12)\n    )\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/eg-boxplot2-1.png){width=672}\n:::\n:::\n\n\n\n\nLa intepretación del boxplot, entonces, debe incluir lo siguiente: \n\n- La mediana está en 3 miembros, lo que indica que la mitad de los hogares tiene 3 o menos personas y la otra mitad 3 o más.\n\n- El primer cuartil ($Q1$) se sitúa en 2 personas y el tercer cuartil ($Q3$) en 4 personas. Esto nos dice que el 50% central de los hogares (entre $Q1$ y $Q3$) agrupa tamaños de 2 a 4 miembros.\n\n- El rango intercuartílico ($RIC = Q3 – Q1$) es 2 personas, reflejando una variabilidad moderada en el tamaño típico de los hogares.\n\n- Los “bigotes” se extienden desde 1 hasta 7 miembros aproximadamente, marcando los límites dentro de $1,5·RIC$.\n\n- Más allá de 7 personas aparecen puntos aislados (*outliers*), correspondientes a hogares muy numerosos (8 a 13 personas), que representan casos excepcionales en la distribución.\n\nEn conjunto, el gráfico revela que la mayoría de los hogares chilenos encuestados están compuestos por entre 2 y 4 personas, con un hogar típico de 3 miembros, y que los tamaños extremos (hogares muy grandes) son poco frecuentes.\n\n\n## Gráficos bivariados\n\n### Gráficos de barras de dos variables\n\nEsta visualización es recomendable para resumir gráficamente la información sobre la relación entre **dos variables categóricas**. Específicamente, este gráfico destaca cómo una variable categórica puede diferir entre las categorías de otra variable categórica. Por ejemplo, podríamos mostrar cómo la matrícula en la educación superior difiere por género.\n\nPara ello, tomaré la variable `e6a_asiste` que responde a la pregunta ¿Cuál es el nivel educacional al que asiste?, dado que supone que asiste, al momento de ser encuestado/a, a un establecimiento educacional o no. En este caso, solo tomaremos la categoría `13` que es \"13. Profesional (Carreras 4 o más años)\". Aunque no es una variable sobre la matricula como tal, es un buen *proxy*^[Para ello, hay muchas bases de datos que si registran esta información. Además, el MINEDUC una buena plataforma de datos abiertos, pero para centrarnos, en este capítulo, en la CASEN] \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\n# Creamos un df ad hoc\ndf_asiste_prof <- casen2022 |>\n  filter(e6a_asiste == 13) |>\n  count(sexo) |>\n  mutate(sexo = factor(sexo, levels = c(1, 2), labels = c(\"Hombre\", \"Mujer\")))\n\n# Ploteamos\nggplot(df_asiste_prof, aes(x = sexo, y = n, fill = sexo)) +\n  geom_col(width = 0.6, color = \"white\") +\n  scale_y_continuous(labels = comma) +\n  labs(\n    x     = \"Sexo\",\n    y     = \"Número de estudiantes en carreras profesionales (4+ años)\",\n    title = \"Asistencia a la universidad (Profesional) por sexo\"\n  ) +\n  theme_minimal(base_family = \"Fira Sans\", base_size = 12) +\n  theme(\n    plot.title     = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    axis.text      = element_text(size = 12),\n    legend.position = \"none\"\n  )\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/eg-barplot-bivariado-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n### Boxplots bivariados \n\nEste tipo de visualización es recomendable para resumir gráficamente la información de la relación entre una *variable categórica* y una *variable cuantitativa*. Concretamente, se visualizan los gráficos de cajas para una variable cuantitativa, separados para cada valor posible de una variable categórica. Por lo tanto, permite ver cómo la variable cuantitativa se distribuye dentro de cada valor de una variable categórica.\n\nPara ello, podríamos ver el ingreso del trabajo (variable continua) por sexo (categórica). Pero, como ya vimos, los boxplots con variables con *tanta dispersión*, se ven feos y *comunican* mal. Ahora bien, algo que es recomendable para comprimir la escala y reducir la asimetría entre los valores para variables con mucha dispersión es aplicarle un logaritmo a la variable de interés. De hecho, esto es algo que se hace bastante con el ingreso. No me interesa profundizar en esto ahora, porque es algo que trateremos cuando lleguemos a regresiones. No obstante, si es de interés, en este punto, el capítulo 2 de @masteringmetrics ofrece una explicación didáctica y profunda. Además, en el apéndice de dicho capítulo, específicamente en el apartado \"*Building Models with Logs*\" o \"Modelos logarítmicos\" --en la traducción del libro--, se profundiza específicamente en la utilidad de aplicar logaritmos a variables como el ingreso. \n\nVeamos cómo queda el gráfico, que además `ggplot2` tiene funciones para realizar transformaciones logarítmicas de las varaibles sin hacer recodificación previa. Primero lo hago con codificación previa (\"manual\") y luego en ggplot mismo. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncasen2022 |>\n  filter(!is.na(ytrabajocor)) |>\n  mutate(\n    sexo        = factor(sexo, levels = c(1, 2), labels = c(\"Hombre\", \"Mujer\")),\n    log_ingreso = log(ytrabajocor) # transformación manual\n  ) |>\n  ggplot(aes(x = sexo, y = log_ingreso, fill = sexo)) +\n    geom_boxplot(color = \"black\", outlier.color = \"red\", alpha = 0.7) +\n    scale_y_continuous(\n      name   = \"Log(Ingreso de trabajo corregido)\",\n      labels = comma\n    ) +\n    labs(\n      x     = \"Sexo\",\n      title = \"Boxplot de ingreso de trabajo corregido (log) por sexo\"\n    ) +\n    theme_minimal(base_family = \"serif\", base_size = 14) +\n    theme(\n      plot.title     = element_text(face = \"bold\", size = 16, hjust = 0.5),\n      axis.text      = element_text(size = 12),\n      legend.position = \"none\"\n    )\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/eg-boxplot-bivariado-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n## Transformación logaritmica dentro de ggplot2\n\ncasen2022 |>\n  filter(!is.na(ytrabajocor)) |>\n  mutate(sexo = factor(sexo, levels = c(1, 2), labels = c(\"Hombre\", \"Mujer\"))) |>\n  ggplot(aes(x = sexo, y = ytrabajocor, fill = sexo)) +\n    geom_boxplot(color = \"black\", outlier.color = \"red\", alpha = 0.7) +\n    scale_y_log10(labels = comma) +\n    labs(\n      x     = \"Sexo\",\n      y     = \"Ingreso de trabajo corregido (escala log)\",\n      title = \"Boxplot de ingreso de trabajo corregido por sexo (escala log)\"\n    ) +\n    theme_minimal(base_family = \"serif\", base_size = 14) +\n    theme(\n      plot.title     = element_text(face = \"bold\", size = 16, hjust = 0.5),\n      axis.text      = element_text(size = 12),\n      legend.position = \"none\"\n    )\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/eg-boxplot-bivariado-2.png){width=672}\n:::\n:::\n\n\n\n\n\nComo se puede observar, la comparación de ambos *boxplots* deja claro que, al final, la forma de la distribución (posición de cuartiles, mediana y “bigotes”) es exactamente la misma: la transformación logarítmica sólo reescala los datos de manera monotónica, sin alterar su orden ni la detección de *outliers*. De modo general, sobre los gráficos podemos decir: \n\n- La mediana del ingreso de los hombres está algo por encima de la de las mujeres, lo cual se ve en ambos casos.\n\n- El rango intercuartílico (caja) es ligeramente más amplio en los hombres, indicando mayor dispersión en el 50 % central.\n\n- Aparecen outliers en ambos sexos, sobre todo hacia los ingresos muy bajos y muy altos, que quedan marcados como puntos fuera de los “bigotes”.\n\n\nRespecto a las diferencias gráficas según transformación logarítmica \"manual\" o *en* el código de `ggplot2`: \n\n1. **Boxplot con transformación manual** (`log_ingreso`): \n - En el eje Y leemos directamente la escala de log(ingreso). Por ejemplo, una mediana alrededor de 12,4 en hombres equivale a un ingreso de $\\approx e^{12,4} \\approx 2,4 \\times^5 $, es decir, 240.000 pesos \n - Es útil si nuestro interés está en trabajar explícitamente con valores logarítmicos (p. ej. para comparar diferencias porcentuales, como veremos después).\n\n2. **Boxplot con ggplot2** (`scale_y_log10()`): \n  - Aquí el eje Y muestra los valores de ingreso en pesos, pero “espaciados” según su logaritmo. La mediana se lee directamente en unidades monetarias (p. ej. 200.00 vs 180.000), lo que facilita la interpretación sin cálculo inverso.\n  - Conserva el lenguaje natural de las unidades en el gráfico (no aparecen logaritmos ni ejes en valores crudos de log), pero controla la asimetría y comprime los outliers en una visual más manejable.\n\nEn resumen, ambas aproximaciones equivalen en términos estadísticos. La elección depende de si quieres presentar valores ya en escala log (transformación manual) o prefieres mantener el eje en la unidad original (pesos) pero con la ventaja visual de una escala logarítmica que ofrece el paquete (`scale_y_log10()`).\n\n\n### Gráficos de dispresión de puntos (*scatter plots*)\n\nEste tipo de gráficos es recomendable para resumir gráficamente la información de **la _asociación_ de dos variables cuantitativas**. Un gráfico de dispersión muestra cada observación como un punto en un sistema de coordenadas para dos variables cuantitativas. Por ejemplo, podríamos graficar la relación entre el ingreso del trabajo y los años de escolaridad.\n\nAhora bien, un problema común que surge en los gráficos de dispersión es cuando los datos de muchas observaciones se intentan presentar en sólo una figura. En nuestro ejemplo, contamos con la información de más de 200.000 encuestados. En estos casos, es recomendable procesar antes los datos. Se pueden hacer varias cosas. Podrían crearse rangos de ingresos, por ejemplo, o tomar quintiles, deciles, etc., de ingresos. O bien, también podríamos aplicar una transformación logarítmica en los datos como hicimos anteriormente. \n\nPodemos hacer ambas, pero la verdad es que dependerá mucho de la variable cuál es mejor. Por ejemplo, si estuvieramos viendo puntajes PSU de Matemáticas y a su asociación al Ranking, sería útil hacer grupos. Pero para ingresos no funciona mucho esto. En las clases Estadística I de Engel y Díaz de la FEN se pasa el primer ejemplo. Se presentan primero sin agrupar y luego agrupando: \n\n<img src=\"images-sesiones/grafico de dispersión sin procesar.png\" \n     alt=\"Datos sin agrupar\" \n     style=\"display: block; margin: auto;\" />\n\n<img src=\"images-sesiones/grafico de dispersion de puntos procesado.png\" \n     alt=\"Datos ya agrupados\" \n     style=\"display: block; margin: auto;\" />\n\nAhora bien, veamos cómo quedaría con ingreso esto. Primero tomaremos los déciles, registrados por la casen en `dau` (Decil autónomo nacional) que se calculan tomando todos los ingresos del hogar (o sea, ya no ingresos del trabajo de los encuestados encuestado) y se dividen, siguiendo la lógica explicada en las *medidas de posición*, en 10 grupos. En este caso, el grupo 1 es el grupo de ingresos más bajos (el 10% más pobre) y el 10 es el de ingresos más altos (el 10% más rico). Luego, para ver si es un problema de que sean pocos grupos, lo haremos para percentiles (100 grupos). Para la otra variable, en ambos casos, usaremos `esc` que mide los años de escolaridad (solo personas de 15 años en adelante) como variable cuantitativa discreta.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1) Scatter: escolaridad vs decil de ingreso del hogar\ncasen2022 |>\n  filter(!is.na(esc), dau %in% 1:10) |>\n  ggplot(aes(x = esc, y = dau)) +\n    geom_jitter(width = 0.2, height = 0.2, alpha = 0.3, color = \"#4E79A7\") +\n    scale_y_continuous(breaks = 1:10) +\n    labs(\n      x     = \"Años de escolaridad\",\n      y     = \"Decil autónomo nacional\",\n      title = \"Dispersión: Escolaridad vs Decil de ingreso del hogar\"\n    ) +\n    theme_minimal(base_family = \"serif\", base_size = 14)\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/scatter-plot-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# 2) Agrupar ingreso autónomo del hogar en 100 percentiles y graficar\ncasen2022 |>\n  filter(!is.na(esc), !is.na(yautcorh)) |>\n  mutate(percentil = ntile(yautcorh, 100)) |>\n  ggplot(aes(x = esc, y = percentil)) +\n    geom_jitter(width = 0.2, height = 0.2, alpha = 0.3, color = \"#4E79A7\") +\n    scale_y_continuous(breaks = seq(0, 100, by = 10)) +\n    labs(\n      x     = \"Años de escolaridad\",\n      y     = \"Percentil de ingreso autónomo del hogar\",\n      title = \"Dispersión: Escolaridad vs Percentiles de ingreso\"\n    ) +\n    theme_minimal(base_family = \"serif\", base_size = 14)\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/scatter-plot-2.png){width=672}\n:::\n:::\n\n\n\n\nAhora bien, en ninguno de los casos se ve bien. Esto es por dos motivos: 1) ambas variables son discretas con muy pocos valores únicos (`esc` y `dau`). Pero, incluso si aumentamos a 100 grupos, y ya no usamos `dau`, tampoco funciona. En realidad, esto nuevamente se debe a la naturaleza de la variable ingreso, que es continua y tiene un rango muy alto. 2) Esto último produce lo que se conoce como *overplotting*, que en nuestro caso, con más de 200.000 observaciones, incluso un pequeño desplazamiento de jitter deja millones de puntos amontonados, de modo que no se distingue ninguna tendencia ni densidad real.\n\nEntonces, para lo que queremos graficar, en este caso, ingresos y su asociación son el nivel de escolaridad, no nos sirve tomar dos variables discretas, aunque sean cuantitativas ambas. Probemos, entonces, si el viejo logaritmo nos ayuda en este caso. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncasen2022 |>\n  filter(!is.na(esc), !is.na(yautcorh)) |>\n  ggplot(aes(x = esc, y = yautcorh)) +\n    geom_jitter(width = 0.1, alpha = 0.5, color = \"#ADD8E6\") +\n    scale_y_log10(labels = comma) +\n    labs(\n      x     = \"Años de escolaridad\",\n      y     = \"Ingreso autónomo corregido (escala log)\",\n      title = \"Dispersión: Escolaridad vs Ingreso (log)\"\n    ) +\n    theme_minimal(base_family = \"serif\", base_size = 14)\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/scatter-plot2-1.png){width=672}\n:::\n:::\n\n\n\n\nComo se ve, ahora si el gráfico nos comunica algo más comprensible. Además, se puede ver cómo va reduciendo la dispersión en cuanto avanzan los años de escolaridad. Es importante resaltar la cantidad de *outliers* que hay que notienen ingresos autónomos, aún avancen los años de escolaridad. Esto también puede tener que ver con que estamos usando ingresos autónomos del hogar y no de los individuos. Pero, a saber. No voy a profundizar en esto ahora. \n\nLo que si me parece más interesante ahora es adelantar algo bacán que se puede hacer con `ggplot2` y los *scatterplots*. Esta tendencia que vemos en los datos, también podemos inmediatamente modelarla. El paquete permite, al graficar la dispersión de los datos, trazar una linea de tendencia con bandas de error estándar mediante un modelo lineal. Como ya advertimos antes, ahora es aún más útil aplicar `log()`, porque dado que los ingresos no se comportan con una distribución normal, transformamos el eje x (`yautcorh`) de una escala linear a una escala logarítmica. Esto nos permitirá un mejor ajuste de la línea de regresión estimada. Luego, cuando lleguemos a regresiones, veremos la utilidad de esto, como ya he ido adelantando. También podemos ajustar los valores de ingres a una notación mas adecuada, en este caso, pesos chilenos.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Scatter plot con línea de regresión\nggplot(\n  casen2022 |> filter(!is.na(esc), !is.na(yautcorh)),\n  aes(x = esc, y = yautcorh)\n) +\n  geom_point(alpha = 0.4, color = \"#36648B\") +\n  geom_smooth(\n    method = \"gam\",\n    se     = TRUE,\n    color  = \"#8B0000\",\n    fill   = \"#696969\",\n    size   = 1\n  ) +\n  scale_y_log10(labels = scales::dollar_format(prefix = \"$\", suffix = \" CLP\")) +\n  labs(\n    x     = \"Años de escolaridad\",\n    y     = \"Ingreso autónomo corregido del hogar\",\n    title = \"Nivel de ingreso autónomo corregido según años de escolaridad\"\n  ) +\n  theme_minimal(base_family = \"serif\", base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    axis.text  = element_text(size = 12)\n  )\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/scatter-plot-regresion-1.png){width=672}\n:::\n:::\n\n\n\n\nEntonces, y ahora sí, el gráfico muestra, en escala logarítmica, cómo varía el ingreso autónomo corregido del hogar según los años de escolaridad (para personas de 15 años en adelante). De este *scatter plot* se puede interpretar que hay\n\n1. **Tendencia creciente**. La línea de tendencia sube de izquierda a derecha, lo que indica que a más años de escolaridad corresponde, en promedio, un ingreso mayor; \n\n2. No obstante, existen **rendimientos decrecientes**. La pendiente es más pronunciada en los primeros años de escolaridad y se aplana conforme avanzamos hacia niveles superiores, sugiriendo que cada año adicional de educación añade menos aumento porcentual al ingreso que el año anterior.\n\n3. **Alta dispersión**. A cualquier nivel de escolaridad hay una enorme variabilidad de ingresos (nubes de puntos muy extendidas). Esto refleja que, aunque el promedio sube con la educación, los ingresos individuales probablemente dependen también de otros factores (sector, experiencia, género, etc., a saber).\n\n4. Respecto a la **confianza de la curva**, se puede observar que la banda gris alrededor de la línea es más estrecha en los niveles de escolaridad donde hay más observaciones (por ejemplo entre 8 y 12 años) y más ancha en los extremos (muy poca gente con 0 ó >20 años), lo que nos habla de mayor incertidumbre al estimar la relación fuera del rango central.\n\n\nEn suma, el gráfico confirma una asociación positiva entre educación e ingreso, con incrementos relativos mayores al inicio de la escolaridad y un efecto suavemente decreciente en los niveles más altos. \n\nFinalmente, respecto a los gráficos de tipo *scatter plot*, hay que tener ojo qué tipo de variables vamos a visualizar. No basta con asociar dos variables cuantitativas, sino que también hay que fijarse en si son discretas, continuas, en la cantidad de observaciones, etc.; en definitiva, en la naturaleza de las variables asociadas. A menudo, tendremos que realizar transformaciones a nuestras variables para que el gráfico **comunique mejor** el carácter de la asociación de las variables de interés.\n\n## 3. Estadística inferencial: una introducción\n\nRetomemos algunas cosas y veamos algunas cosas con las que deberíamos haber comenzado todo, pero que por cuestiones del curso no alcanzan. Partamos por qué es una *variable aleatoria*.\n\n### Variables aleatorias: funciones y distribución de probabilidad\n\nLa probabilidad es una medida que nos entrega un grado de *certidumbre* de que ocurra un evento *incierto*. La teoría de la probabilidad es la base de toda estadístíca y, por tanto, de la econometría. Y la base de la probabilidad es la teoría de conjuntos y la matemática. Teníamos que la probabilidad intenta *asignar* a cada evento de un experimento un valor $(\\in \\mathbb{R})$ para capturar la frecuencia del evento si el experiemento fuese repetido muchas veces. De ello, deducimos los axiomas de la probabilidad y otras propiedades que eran consecuencias de ellos. Finalmente, examinamos los tipos de probabilidades (conjunta, margianl y condicional), poniendo un énfasis especial en la probabilidad condicional y la independencia estadística entre eventos. Con todo eso, veamos qué es una variable aleatoria. \n\nUna variable aleatoria es \"una función de valor real para la cual el dominio es un espacio \nmuestral\" ([Wackerly, et al., 2010, p. 76](https://www.cimat.mx/ciencia_para_jovenes/bachillerato/libros/[Wackerly,Mendenhall,Scheaffer]Estadistica_Matematica_con_Aplicaciones.pdf)). Una variable aleatoria (v.a.), así, toma valores que son determinados por un proceso aleatorio, que muchas veces puede ser un proceso natural estocástico.^[Un proceso estocástico es aquel cuyo resultado no puede preverse con certeza de antemano. En cambio, un proceso determinista produce siempre el mismo resultado cuando se parte de una misma condición inicial. Por ejemplo, lanzar una moneda es un proceso estocástico, mientras que sumar dos números concretos es un proceso determinista. Luego lo usaremos como homólogo de proceso aleatorio]. De tal modo, una v.a. no es más que una representación numérica de los resultados potenciales de un experimento. Formalmente,\n\n:::{.callout-note}\n### Definición\nSea $\\mbox{S}$ el espacio muestral de un experimento. Una función de valor real definida en $\\mbox{S}$ se denomina *variable aleatoria* (DeGroot y Schervish, 2014, p. 93)\n:::\n\nDe tal modo, dado que una variable aleatoria es un resumen numérico de algún resultado de interés. Y que, formalmente, es una función que asocia un número real a cada elemento de $\\mbox{S}$, podemos representarla matemáticamente como\n$$\n\\begin{aligned}\n    X&:\\mbox{S} \\longrightarrow \\mathbb{R}\\\\\n    &u \\longrightarrow X(u).\n\\end{aligned}\n$$\nPor ejemplo, imaginemos que lanzamos una moneda 2 veces, tal que su  espacio muestral es $S = \\{cc, cs, sc, ss\\}$. Definimos, pues, la variable aleatoria $X$ que es el número de caras en los dos lanzamientos.\n$$\n\\begin{aligned}\n    X(cc) &= 2 \\\\\n    X(cs) &= 1 \\\\\n    X(sc) &= 1 \\\\\n    X(ss) &= 0\n\\end{aligned}\n$$\nEn el fondo, para cada caso que saliera cara, $c$, la función arrojaba cuántos $c$ hay en el espacio muestral en función de cómo la evaluáramos. Por ende, $X$ puede tomar los valores 0, 1 y 2, solamente. En efecto, la principal virtud de las variables aleatorias es que nos permiten describir eventos de forma sencilla. Tomando el caso anterior,\n\n- $\\{X = 0\\} \\iff$ Todos los elementos en $S$ que no tienen caras  = $\\{ss\\}$. Por lo tanto,\n$$\n    \\Pr(X = 0) = \\Pr(\\{ss\\}) = \\frac{1}{4}.\n$$\n\n- $\\{X = 1\\} \\iff$ Todos los elementos en $S$ que tienen una cara = $\\{cs, sc\\}$. Por lo tanto,\n$$\n\\Pr(X = 1) = \\Pr(\\{cs, sc\\}) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}.\n$$\n\n- $\\{X = 2\\} \\iff$ Todos los elementos en $S$ que tienen dos caras = $\\{cc\\}$. Por lo tanto,\n$$\n\\Pr(X = 2) = \\Pr(\\{cc\\}) = \\frac{1}{4}.\n$$\n\nAhora bien, a partir de un espacio muestral podemos crear diversas variables aleatorias. En el ejemplo anterior creamos el número de caras, pero también podemos denotar por $Y$ la v.a. que cuenta el número de sellos.\n$$\n\\begin{aligned}\nY(cc) &= 0 \\\\\nY(cs) &= 1 \\\\\nY(sc) &= 1 \\\\\nY(ss) &= 2\n\\end{aligned}\n$$\nA partir de un espacio muestral podemos crear diversas variables aleatorias. En el ejemplo anterior creamos el número de caras, pero también podemos denotar por $Y$ la v.a. que cuenta el número de sellos. Finalmente, notemos que por definición\n$$\nX + Y = 2.\n$$\n#### Tipos de variables aleatorias y distribuciones de probabilidad asociada\n\n\nQuizás como ya podrían intuir, hay dos grandes grupos de variables aleatorias: las discretas y las continuas. En este curso, no profundizaremos en esto, pero si mencionaremos brevemente algunas cuestiones importantes antes de llegar al estadística inferencial. \n\n#### Variables aleatorias discretas\n\nUna variable aleatoria discreta es aquella que toma un número finito o numerable de valores. Por ejemplo, el el resultado de lanzar un dado. Formalmente, diremos que \n\n:::{.callout-note}\n### Definición\nUna v.a. $X$ es \\textit{discreta} si existe un conjunto finito $A = \\{a_1, a_2, ..., a_n\\}$ tal que \n$$\n\\Pr(X \\text{ toma valores en } A) = 1.\n$$\n:::\n\n\nTenemos, entonces, que la variable aleatoria captura la información contenida en todo el espacio muestral del experimento en una cantidad numérica. Ahora bien, el valor del experimento, por definición, no se conoce, dada su naturaleza aleatoria. Lo que conocemos, en cambio, es $\\mathbb{S}$. Es decir, que conocemos todos los resultados potenciales del experimento aleatorio. Del lanzamiento de un dado, no sabemos qué va salir exactamente, pero sí sabemos que solo puede salir entre 1 y 6. No puede salir 7, ni Spiderman, ni Conchalí. Porque esos son valores que no pertenecen a $\\mathbb{S}$. Entonces, tenemos que volver ahora al concepto de función de probabilidad, incorporando este nuevo conocimiento. \n\n:::{.callout-note}\n### Definición\nConsideremos una v.a. discreta $X$ definida sobre un espacio muestral $\\mathbb{S}$. Entonces la **función de probabilidad** (f.p.) de $X$ se define como: \n$$\nf(x) = \\Pr(X = x).\n$$ \n:::\n\nLa expresión $f(x) = \\Pr(X = x)$ nos dice que la fdp de $X$ evaluada en un valor particular de $x$ se corresponde con la probabilidad de que la v.a. $X$ tome algún valor de $x$. Es decir, que $X$ corresponde a la v.a., mientras que $x$ corresponde algún valor particular de $X$. Además, se tiene que $f(x)>0$, i.e., es estrictamente positivo para $x$ en el soporte de $X$ y $f(x)=0$, i.e., vale 0 cualquier otro caso, o sea,  en caso contrario. \n\nPor ejemplo, si se lanza una moneda honesta 2 veces. Entonces\n\n- Variable aleatoria $X$: número de caras en los 2 lanzamientos.\n\n- Variable aleatoria $Y$: número de sellos en los 2 lanzamientos.\n\n¿Cuál es la f.p. de $X$? Ya deberíamos saber que\n$$\nf(0) = \\frac{1}{4}, \\quad f(1) = \\frac{1}{2}, \\quad f(2) = \\frac{1}{4}, \\quad f(x) = 0 \\quad \\text{para cualquier otro valor de } x.\n$$\npero ahora además le agregamos que $f(x)=0$ es cualquier otro valor. ¿Y cuál es la f.p. de $Y$? Por simetría, ya veíamos que era exactamente la misma probabilidad que la f.p. de $X$. Ahora bien, a pesar de que $X$ e $Y$ tienen la misma **función de probabilidad**, *no* son la misma v.a. Por ejemplo, cuando salen 2 caras, tenemos que $X(cc)=2$ e $Y(cc)=0$. Veamos ejemplos de variables aleatorias discretas y sus distribuciones de probabilidad. \n\n\n##### Distribución uniforme discreta\n\nUna distribución uniforme discreta (UD) corresponde a una función de probabilidad donde todos los eventos tienen la misma probabilidad, como el lanzamiento de un dado. Si se tiene que $m$ y $n$ son números enteros tales que $m \\leq n$, que $X$ es una v.a. discreta, $X$ tendrá una distribución uniforme discreta entre los eventos $m$ y $n$, o $X\\sim UD(m,n)$ si se cumple que\n$$\nf(x) =\n\\begin{cases}\n\\frac{1}{n-m+1} & x=m, m+1, \\ldots, n\\\\\n0 & \\text{en caso contrario}\n\\end{cases}\n$$\nNotemos, además, que la distribución de probabilidad de una v.a. discreta suma 1.\n\n##### Distribución de Bernoulli \n\nUna variable aleatoria $X$ sigue una **distribución de Bernoulli** si solo toma dos valores, específicamente $0$ o $1$. Es decir que $X$ sigue una distribución de Bernoulli de parámetro $p\\in [0,1]$ si $X$ toma valores $0$ o $1$ y si $\\Pr(X=1)=p$. Esto lo denotamos como $X\\sim \\mbox{Bern}(p)$\n\nEntonces, si $X \\sim \\mbox{Bern}(p)$ la función de probabilidad, denotada por $f(x)$ es\n$$\nf(x) =\n\\begin{cases}\n    p & \\text{si } x = 1, \\\\\n    1 - p & \\text{si } x = 0, \\\\\n    0 & \\text{si } x \\notin \\{0, 1\\}\n\\end{cases}\n$$\nEs habitual para experimentos con solo dos resultados posibles que se interpreten los dos resultados posibles como éxito y fracaso, i.e., $X=1$ es considerado como éxito y $X=0$ como fracaso. Por ejemplo, si al lanzar una moneda, definimos $X$ según: si sale cara, $X = 1$; y si sale sello, $X = 0$. Entonces, \n$$\np = \\frac{1}{2}, \\quad X \\sim \\text{Ber}(0,5). \n$$\n\n##### Distribución Binomial: Ensayos de Bernoulli y coeficiente binomial\n\nAntes de ver distribución binomial, conviene ver un concepto previo, los famosos \"**Ensayos de Bernoulli**\". Una *secuencia de ensayos de Bernoulli* es una secuencia $X_1, X_2, \\dots, X_n$ de variables aleatorias independientes con distribución común $\\mbox{Bern}(p)$, donde $0 \\leq p \\leq 1$. Es decir, una secuencia de $n$ ensayos de Bernoulli es una secuencia de ensayos que cumple lo siguiente:\n\n1. Solo dos resultados posibles en cada ensayo: 1 (éxito) y 0 (fracaso).\n\n2. Ensayos *independientes*.\n\n3. La misma probabilidad de éxito en cada ensayo.\n\nDe tal modo, formalizando, diremos que una **secuencia de ensayos de Bernoulli** es una seguidilla de variables aleatorias independientes, tal que\n$$\nX_1 \\sim \\mbox{Bern}(p), X_2 \\sim \\mbox{Bern}(p), \\ldots , X_n \\sim \\mbox{Bern}(p)\n$$\ndonde $0\\leq p \\leq 1$. En otras palabras, una secuencia de $n$ ensayos de Bernoulli cumple co que se realizan $n$ experimentos donde hay dos resultados posibles (éxito o fracaso, 1 o 0), que son independientes y donde la probabilidad de éxito es la misma para cada ensayo. Ahora bien, cuando hablemos de la *distribución* de una secuencia de $n$ ensayos de Bernoullli con parámetro $p$ o probabilidad de éxito $p$, estamos hablando de una **distribución binomial**. \n\nUna variable aleatoria $X$ sigue una **distribución binomial** con parámetros $n$ y $p$ si $X$ tiene una distribución discreta con función de probabilidad:\n$$\nf(x) = \n\\begin{cases}\n\\binom{n}{x} p^x (1-p)^{n-x} & \\text{para } x = 0, 1, 2, \\dots, n, \\\\\n0 & \\text{si no.}\n\\end{cases}\n$$\nEl parámetro $n$ es un entero mayor o igual que uno, mientras que $p$ pertenece al intervalo $\\{0,1\\}$. Escribimos $X \\sim \\text{Bin}(n, p)$ y decimos \"**$X$ sigue una Binomial con parámetros $n$ y $p$**\".\n\nAlgo que se suele ver en conteo y combinatoria, y que no incluyo en este apunte, es el **coeficiente binomial**. Este se escribe como\n$$\n\\binom{n}{x} = \\frac{n!}{x!(n-x)!}\n$$\ny en combinatoria representa el número de formas de escoger (sin importar el orden) un subconjunto de $x$ objetos extraídos de un conjunto de $n$ objetos distintos. Por ejemplo, si se tiene $n=5$ frutas distintas y queremos saber de cuántas maneras puede elegir $x=2$ de ellas, se hace lo siguiente: \n$$\n\\binom{n}{x} = \\frac{n!}{x!(n-x)!} = \\frac{5!}{2!3!} = \\frac{120}{2 \\cdot 6} = 10\n$$\nEs decir, hay 10 pares distintos que se pueden formar. Esto tiene las siguiente propiedades clave (que usaremos para un teorema de sobre la distribución binomial y ensayos de Bernoulli, por eso lo coloco): \n\n- **Simetría**  \n  $$\\binom{n}{x} = \\binom{n}{n - x}$$  \n  Elegir \\(x\\) de \\(n\\) es lo mismo que elegir los \\(n - x\\) que quedan fuera.\n\n- **Valor en los extremos**  \n  $$\\binom{n}{0} = \\binom{n}{n} = 1$$  \n  Sólo hay una forma de no escoger nada o de escoger todo.\n\n- **Relación recursiva (Triángulo de Pascal)**  \n  $$\\binom{n}{x} = \\binom{n - 1}{x - 1} + \\binom{n - 1}{x}$$\n\nAdemás, de las propieidades, una **Aplicación en el Teorema del Binomio** es la siguiente: en la expansión de $(a + b)^n$, los coeficientes vienen dados por:\n\n$$\n(a + b)^n = \\sum_{x = 0}^{n} \\binom{n}{x}\\,a^x\\,b^{\\,n - x}.\n$$\n\nRespecto a la **interpretación probabilística** del coeficiente binomial, tenemos que fijarnos en lo siguiente. En un experimento de Bernoulli (éxito o fracaso) con probabilidad de éxito \\(p\\) en cada ensayo, la probabilidad de obtener exactamente \\(x\\) éxitos en \\(n\\) ensayos es:\n\n$$\nP(X = x) = \\binom{n}{x}\\,p^x\\,(1 - p)^{\\,n - x}.\n$$\n\nEn resumen, $\\displaystyle \\binom{n}{x}$ cuenta de cuántas maneras puedes elegir $x$ elementos de un total de $n$, y aparece de forma natural en combinatoria, en el desarrollo de potencias binomiales y en distribuciones de probabilidad.\n\nCon todo esto, veremos un Teorema, que nos permite relacionar la distribución binomial con los ensayos de Bernoulli. Con ello, sabremos bien para qué nos sirve este tipo de distribución. \n\n::: {.callout-note}\n### Teorema: Distribución Binomial y ensayos de Bernoulli. \n\nSea $X_1, X_2, \\dots, X_n$ variables aleatorias independientes, con distribuciones Bernoulli de parámetro $p$. Diremos, por tanto, que se trata de $n$ ensayos de Bernoulli con probabilidad de éxito $p$, donde se sobreentiende que son independientes. Denotamos por $S$ el número de éxitos en los $n$ ensayos, es decir\n$$\nS = X_1 + X_2 + \\dots + X_n.\n$$\nEntonces $S \\sim \\mbox{Bin}(n, p)$.\n:::\n\n\n#### Distribución Geométrica\n\nUna variable aleatoria $X$ sigue una distribución geométrica de parámetro $p$, $0 < p < 1$, si $X$ tiene una distribución discreta con función de probabilidad:\n$$\nf(x) =\n    \\begin{cases} \n    pq^x & \\text{para } x = 0, 1, 2, \\dots \\\\\n    0 & \\text{si no.}\n    \\end{cases}\n$$\ndonde $q = 1 - p$. En este caso, escribimos $X \\sim \\mbox{Geo}(p)$\n\n::: {.callout-note}\n### Teorema: Distribución Geómetrica y ensayos de Bernoulli. \n\nConsideremos $X_1,X_2,X_3, \\ldots$ v.a. $\\mbox{Ber}(p)$ independientes. Denotemos, pues, $Y$ por el número de fracasos antes del primer éxito de la secuencia anterior. Entonces, $X$ sigue una distribución geométrica de parámetro $p$.\n:::\n\n#### Relación Bernoulli, Binomial y Geométrica\n\nComo ya se podría intuir, este tres tipos de distribuciones de variables aleatorias discretas, están relacionadas. Y lo están de la siguiente manera: \n\n- Cuando realizamos un experimento que tiene **solo dos resultados posibles**, estamos ante un ensayo Bernoulli\n\n- Si tenemos una secuencia $X_1, X_2, \\dots, X_n$ de variables aleatorias independientes con distribución común $\\mbox{Bern}(p)$, la suma de ``éxitoa'' tendrá una distribución $\\mbox{Bin}(n,p)$. Es básicamente, la suma una secuencia de experimentos con distribución $\\mbox{Bern}(p)$\n\n- Si en esta misma secuencia denotamos como $Y$ el número de ``fracasos'' antes del primer \"éxito\", $Y$ tendrá una distribución $\\mbox{Geo}(p)$\n\n\nPor ejemplo, si lanzamos una moneda en la que definimos $X=1$ si sale cara y $X=0$ si sale sello. Supongamos que el resultado fue el siguiente en 8 lanzamientos:\n$$\nS = \\{\\text{s, s, c, s, c, c, s, s}\\}\n$$ \nTenemos, pues que \n\n1. El lanzamiento particular de cada moneda tiene $\\mbox{Ber}(0,5)$. \n\n2. Si definimos $Y$ como el número de caras si lanzamos la moneda 8 veces, tenemos que el número de caras fue $c=3$. La distribución de los lanzamientos, así, es de $\\mbox{Bin}(n=8, p=0,5$).\n\n3. Si definimos $Z$ como el número de fracasos antes del primer éxito, dado que hubo dos sellos $X=0$ antes de cara $X=1$, entonces, tenemos, dado que la v.a., sigue una distribución tal qeu $\\mbox{Geo}(0,5)$, entonces, que va tener un $\\Pr(Y)=q^2p$\n\nGráficamente puede ser aún más claro. Veamos este ejemplo en R. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(dplyr, ggplot2, scales, purrr, tidyr)\n\nset.seed(2025)\nN <- 10000\np <- 0.5\nn <- 8\n\n# 1) Simulaciones\nsim_data <- tibble(\n  Ber = rbinom(N, size = 1, prob = p),\n  Bin = rbinom(N, size = n, prob = p),\n  Geo = rgeom(N, prob = p)\n) |> \n  pivot_longer(everything(), names_to = \"Distrib\", values_to = \"x\")\n\n# 2) PMF empírica\ndf_pmf <- sim_data |> \n  count(Distrib, x) |> \n  group_by(Distrib) |> \n  mutate(prob = n / sum(n)) |> \n  ungroup()\n\n# 3) Graficar\nggplot(df_pmf, aes(x = factor(x), y = prob, fill = Distrib)) +\n  geom_col(color = \"black\", width = 0.7, position = position_dodge(width = 0.8)) +\n  facet_wrap(~ Distrib, scales = \"free_x\", strip.position = \"top\") +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  scale_fill_manual(values = c(\"Ber\" = \"#4E79A7\", \"Bin\" = \"#2CA02C\", \"Geo\" = \"#E15759\")) +\n  labs(\n    x     = \"Valor de la variable aleatoria\",\n    y     = \"Probabilidad empírica\",\n    title = \"Relación entre Bernoulli, Binomial y Geométrica (p = 0.5, n = 8)\"\n  ) +\n  theme_minimal(base_family = \"serif\", base_size = 14) +\n  theme(\n    plot.title    = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    strip.text    = element_text(face = \"bold\", size = 14),\n    axis.text     = element_text(size = 12),\n    legend.position = \"none\"\n  )\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/relacion-distribuciones-1.png){width=672}\n:::\n:::\n\n\n\n\n\n##### Distribución de Poisson \n\nLa distribución de Poisson es algo que podríamos colocar más adelante. Pero al ser también una distribución propia de variables aleatorias discretas decido dejarla aquí. La distribución de Poisson permite modelar el número de éxistos en un determinado intervalo de *tiempo*. Por ejemplo, si en 1 hora vendo 20 chocolates promedio, puedo modelar esto con Poisson con parámetro 20. Formalmente tenemos lo siguiente.\n\nSea $\\lambda>0$. La v.a. $X$ tiene una distribución de Poisson de parámetro $\\lambda$, lo cual denotamos como $X\\sim \\mbox{Poi}(\\lambda)$, si su función de probabilidad satiface\n$$\nf(x) = \\frac{e^{-\\lambda}\\lambda^x}{x!}, \\qquad x= 0, 1, 2, \\ldots \n$$\nAquí es donde conviene haber visto algunas cosas antes, particularmente los términos Esperanza y Varianza en este contexto. Pero da igual porque lo pondré en el apartado siguiente. Solo quería seguir la lógica de colocar las distribuciones juntas. Una vez visto esta distribución, esperanza y varianza (siguiente sección), pasaremos a las variables aleatorias continuas y sus distribuciones. Lo que tenemos que ver ahora, respecto a la distribución de Poisson, son sus propiedades. \n\n**Propiedades de la Distribución de Poisson**\n\n1. Si $X\\sim \\mbox{Poi}(\\lambda)$ entonces\n$$\n\\mbox{E}(X) = \\lambda, \\qquad \\mbox{Var}(X)=\\lambda\n$$\n\n2. Si $X\\sim \\mbox{Poi}(\\lambda_1)$, $Y\\sim \\mbox{Poi}(\\lambda_2)$, y $X$ e $Y$ son independientes, entonces se tiene que\n$$\nX+Y \\sim \\mbox{Poi}(\\lambda_1 +\\lambda_2).\n$$\nEs decir, la suma de las v.a. independientes Poisson es Poisson, y el parámetro de las sumas es la suma de los parámetros de las variables individuales. \n\nComo ya mencionamos, la distribución de Poisson es principalmente utilizada para contar el número de éxitos en un determinado intervalo de tiempo o región del espacio, pues una Poisson es una **muy buena _aproximación_ para una binomial** con un $n$ que tiende a infinito y un parámetro $p$ muy pequeño. En efecto, si se tiene qeu $X_n \\sim \\mbox{Bin}(n,p)$ y $n$ tiende a infinito, entonces $X_n$ *converge* a Poisson de parámetro $\\lambda = np$, i.e., \n$$\n\\lim_{n\\to \\infty} (\\Pr(X_n=x)) = \\frac{e^{-\\lambda}\\lambda^x}{x!}\n$$\nPor lo tanto, es recomentable usar este tipo de distribución en el caso que nos enfrentemos a una distribución binomial con un número de ensayos $n$ bastante grande y una probabilidad de éxito $p$ pequeña. \n\n\n#### Distribuciones de variables aleatorias continuas\n\n diferencia de las v.a. discretas, las v.a. continuas son aquellas que pueden tomar **cualquier valor** de un intervalo de una recta real. Es decir, cualquier número real. Un ejemplo puede ser el tiempo, los ingresos, distancias, etc. De tal modo, las v.a. continuas describen el resultado de situaciones de probabilidades en donde lso valores de la v.a. forman un *continuum* de valores y, por tanto, no es posible *listar* valores. De tal modo, conviene aprehenderlos mediante intervalos. Formalmente, tenemos lo siguiente: \n\n:::{.callout-note}\n### Definición \nLa v.a. $X$ tiene una **distribución continua** si existe una función $f$ definida sobre la recta real, que toma valores mayores o iguales que 0, tal que cualquier para intervalo la probabilidad de que $X$ tome valores en el intervalo que va de $a$ a $b$ es igual a la integral de $f$ sobre el intervalo. Matemáticamente,\n$$\n\\Pr(a<X<b) = \\int^b_a f(x)dx\n$$\n:::\n\nCiertamente los intervalos dependerán de lo que queramos modelar. Si $a\\leq b$, entonces,\n$$\n\\Pr(a\\leq X\\leq b) = \\int^b_a f(x)dx\n$$\nSi, e.g., $b=+\\infty$, entonces\n$$\n\\Pr(a\\geq X) = \\int^\\infty_a f(x)dx\n$$\nO, si $a=-\\infty$, entonces,\n$$\n\\Pr(X\\leq b) = \\int^b_{-\\infty} f(x)dx\n$$\nDiremos, ademñas, que $f(x)$ es la **función de _densidad_ de probabilidad** (o f.d.p.), y ya no solo la f.p., de la v.a. $X$. El conjunto de valores de $x$ donde $f(x)>0$, se conoce como el **soporte** de $X$. Un ejemplo de esto, lo podríamos observar con lo siguiente. Sea la v.a. continua $X$, cuya f.d.p. está dad por\n$$\nf(x)=\n\\begin{cases}\n  \\frac{x}{2} & \\text{si } \\quad 0 \\leq x \\leq 2,\\\\\n  0 & \\text{si no}\n\\end{cases}\n$$\nDeberíamos calcular, entonces, $\\Pr(1\\leq X \\leq 1,5)$. Y siguiendo la definición, tendríamos que\n$$\n\\Pr(1\\leq X \\leq 1,5) = \\int^{1,5}_1 \\frac{x}{2} dx\n$$\nSi resolvemos la integral:\n$$\n\\begin{aligned}\n  \\int^{1,5}_1 \\frac{x}{2} dx &= \\frac{1}{2}\\int^{1,5}_1 xdx \\quad || \\text{ regla de la potencia}\\\\\n  &= \\frac{1}{2}\\frac{x^2}{2} \\quad || \\text{ simplificamos}\\\\\n  &= 0,5[0,5x^2]^{1,5}_1 \\quad || F(b)- F(a)\\\\\n  &| \\lim _{x\\to \\:1+}\\left(0.5x^2\\right)=0,5 \\\\\n  &| \\lim _{x\\to \\:1,5-}\\left(0.5x^2\\right)=1,125 \\\\\n  &= 1,125-0,5\\\\\n  &= 0,625\n\\end{aligned}\n$$\n**Propiedades de una v.a. continua**. Para garantizar que se cumple con los **axiomas de probabilidad**, la f.d.p. $f(x)$ de la v.a. continua $X$ debe cumplir con:\n\n1. Ser positiva:\n$$\nf(x) \\geq 0 \\text{ para todo } - \\infty \\leq x \\leq \\infty\n$$\n\n2. La probabilidad del soporte de $X$ igual a uno\n$$\n\\int^\\infty_{-\\infty} f(x)dx =1\n$$\n##### Función de distribución cumulativa\n\nOtro concepto relevante que surge una vez conocemos las v.a. continuas, es el de función de distribución cumulativa. Ahora bien, también tenemos cumulativa para v.a. discretas. Veremos ambas y las propiedades de todas las funciondes cumulativas, además de especialmente fijarnos en el caso de v.a. continuas. \n\n:::{.callout-note}\n### Definición\nLa **función de distribución cumulativa (f.d.c.) de una v.a. $X$ es la función\n$$\nF(x) = \\Pr(X \\leq x)\n$$\nAbreviaremos la f.d.c. como función de distribución, como función cumulativa o cumulativa. La cumulativa está definida para toda v.a. (discreta o continua), y la denotamos con mayúscula $F(\\cdot)$ para distinguirla de la f.d.p y f.p. \n:::\n\n##### Distribución uniforme para el caso de v.a. continuas\n\nPartimos con una distribución que ya vimos, pero ahora para el caso de las v.a. continuas. Arrancamos por su definición \n\n:::{.callout-note}\n### Definición\nDecimos que la v.a. $X$ sigue una distribución uniforme en el intervalo $[a,b]$, donde $a$ y $b$ son números reales con $a<b$, si $X$ toma valores entre $a$ y $b$, y la probabilidad de que $X$ tome valores en cualquier subintervalo de $[a,b]$ es proporcional al largo del subintervalo. Si esto es así, escribimos $X\\sim \\mbox{U}(a,b)$. \n:::\n\nPondré solo un ejemplo. **Cumulativa de una v.a. discreta**: supongamos que $X\\sim \\mbox{Bin}(4, 1/2)$, de modo que su f.p. está dadar por \n$$\nf(x)= \n\\begin{cases}\n\\frac{4}{x}(1/2)^4, & \\quad x=0, 1, 2, 3, 4,\\\\\n0, & \\quad \\text{en caso contrario}\n\\end{cases}\n$$\nY queremos calcular $F(2)$. Esto es, \n$$\n\\begin{aligned}\n  F(2) &= \\Pr(X\\leq 2)\\\\\n  &=\\Pr(X=0) + \\Pr(X=1)+\\Pr(X=2)\\\\\n  &=0,5^4+4\\cdot 0,5^4+6\\cdot 0,5^4\\\\\n  &= \\frac{11}{16}\n\\end{aligned}\n$$\n\nSi hicieramos esto con todos los valores que toma $x$, y no solo $2$, se puede construir la función cumulativa completa. En R, en el paquete `stats` hay funciones específicas para distribuciones binomiales que usamos antes. Pero vale la pena especificar que `dbinom()`, proporciona la densidad, `pbinom()` proporciona la función de distribución, `qbinom()` proporciona la función cuantílica y `rbinom()` genera desviaciones aleatorias. Gráficamente es más facil verlo, así que lo hacemos en R podemos graficarlo así: \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\npacman::p_load(dplyr, ggplot2, \n               patchwork) # Para combinar gráficos.\n\n# Preparar datos para Binomial(4, 0.5)\ndf <- tibble(\n  x        = 0:4,\n  pmf      = dbinom(x, size = 4, prob = 0.5),\n  cdf      = pbinom(x, size = 4, prob = 0.5)\n) |> \n  mutate(prev_cdf = lag(cdf, default = 0))\n\n# Gráfico PMF\np_pmf <- ggplot(df, aes(x = x, y = pmf)) +\n  geom_segment(aes(xend = x, y = 0, yend = pmf)) +\n  geom_point(size = 3) +\n  scale_x_continuous(breaks = 0:4) +\n  scale_y_continuous(limits = c(0, max(df$pmf) * 1.1)) +\n  labs(x = \"x\", y = \"f(x)\", title = \"PMF de Binomial(4, 0.5)\") +\n  theme_minimal(base_family = \"sans\", base_size = 14) +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5))\n\n# Gráfico CDF\np_cdf <- ggplot(df, aes(x = x)) +\n  geom_step(aes(y = cdf), direction = \"hv\") +\n  geom_point(aes(y = cdf), size = 3) +\n  geom_point(aes(y = prev_cdf), shape = 21, fill = \"white\", size = 3) +\n  scale_x_continuous(breaks = 0:4) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.25)) +\n  labs(x = \"x\", y = \"F(x)\", title = \"CDF de Binomial(4, 0.5)\") +\n  theme_minimal(base_family = \"sans\", base_size = 14) +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5))\n\n# Mostrar ambos juntos\np_pmf + p_cdf\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/eg-fdc-discreta-1.png){width=672}\n:::\n:::\n\n\n\n\nPuse un ejemplo por lo siguiente: en la primera figura se muestra la función de probabilidad de $X$, donde se centra en 2. En la siguiente figura, se muestra la función cumulativa de $X$. Como se ve, hay saltos y tiene forma de escalera. Esto ocurre porque, en la medida que nos movemos hacia la derecha, $0,1, \\ldots, 4$, cada valor del soporte de $X$ añade a su propia probabilidad de ocurrencia a la probabilidad que la función ya venía acumulando. Por esto mismo, **el \"tamaño\" del salto para un valor particualr de $X$ es igual a la probabilidad de que $X$ tome dicho valor**. Por ejemplo, $\\Pr(X=2)$ se expresa la linea vertical (el \"salto\") del punto blanco al negro en el valor $2$ de las abscisas. \n\n**Cumulativa de una v.a. continua**: Para definir esto, partamos con una **propiedad** de toda cumulativa de una v.a. continua. Sea $X$ una v.a. continua con cumativa $F$ y f.d.p. $f$. Entonces,\n$$\nF(x)= \\int^x_{-\\infty}f(u)du\n$$\nAquí la demostración es directa, pues\n$$\nF(x)= \\Pr (X \\leq x) = \\int^x_{-\\infty}f(u)du\n$$\nlo único que cambia respecto a lo que ya sabíamos es que la variable de integración *no puede ser* $x$ dado que $x$ es el límite superior de la integral. Por eso se usa $u$, como variable auxiliar.\n\n\n##### Distribución exponencial\n\nCon la distribución exponencial, explicaremos lo mismo que explicamos antes con la uniforme, i.e., f.d.p., la cumulativa, sus propiedades, esperanza y varianza. La distribución exponencial es útil para modelar el **tiempo de espera** hasta el primer éxito en un determinado experimento, donde se sabe que los éxitos llegan a una tasa de $\\lambda$ éxitos *en promedio* por unidad de tiempo^[Se podría denominar esta distribució como la contraparte continua de una v.a. geométrica, pues se utiliza frecuentemente para representar tiempo de espera antes del éxito. En rigor, la distribución exponencial es un caso particular de la distribución gamma.]. \n\n:::{.callout-note}\n### Definición\nUna v.a. continua $X$ tiene una **distribución exponencial** con parámetro $\\lambda>0$ si su densidad de probabilidad es de la forma\n$$\nf(x)=\n\\begin{cases}\n  \\lambda e^{-\\lambda x}, & \\text{ si } x \\geq 0,\\\\\n  0 & \\text{ en caso contrario.}\n\\end{cases}\n$$\nEn tal caso, escribimos $X\\sim \\mbox{Exp}(\\lambda)$\n:::\n\nGráficamente, podemos visualizar esto (para distintos parámetros de $\\lambda$) de la siguiente manera: \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\npacman::p_load(dplyr, ggplot2, scales)\n\nlambdas <- c(0.5, 1.0, 1.5)\n\ndf_expo <- expand.grid(\n  x      = seq(0, 6, length.out = 1000),\n  lambda = lambdas\n) |> \n  as_tibble() |> \n  mutate(\n    f     = lambda * exp(-lambda * x),\n    label = sprintf(\"λ = %.1f\", lambda)\n  )\n\nggplot(df_expo, aes(x = x, y = f, color = label)) +\n  geom_line(size = 1) +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = comma) +\n  scale_color_manual(\n    breaks = c(\"λ = 0.5\", \"λ = 1.0\", \"λ = 1.5\"),\n    values = c(\n      \"λ = 0.5\" = \"#1F77B4\",\n      \"λ = 1.0\" = \"#7F7F7F\",\n      \"λ = 1.5\" = \"#2CA02C\"\n    )\n  ) +\n  labs(\n    x     = \"x\",\n    y     = expression(f[X](x)),\n    title = \"F.d.p. de la distribución Exponencial para distintos λ\",\n    color = \"Parámetro λ\"\n  ) +\n  theme_minimal(base_family = \"serif\", base_size = 14) +\n  theme(\n    plot.title   = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    legend.title = element_text(size = 12),\n    legend.text  = element_text(size = 12),\n    axis.text    = element_text(size = 12)\n  )\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/dist-exponencial-fdp-1.png){width=672}\n:::\n:::\n\n\n\n\n\n### Distribución normal estándar\n\nSobre esta distribución, nos detendremos un poco más. Veremos, no solo su función de densidad y cumulativa, sino que también los percentiles de una normal estándar y propiedes importantes. Esta será nuestro vinculo directo con el otro aspecto que habíamos mencionado como relevante para la inferencia estadística y causal además de la Ley de los Grandes Números: el Teorema del Límite Central. \n\nAdemás, la distribución normal estándar es bastante importante para las ciencias sociales y naturales. Esta distribución provee buen ajuste de los datos para muchos fenómenos naturales, económicos y sociales. Por ejemplo, los datos de altura suelen distribuirse de manera normal estándar, o al menos aproximarse como tal. Por otro lado, podríamos pensar que, e.g., la altura es la suma de multiples factores: edad, genética, sexo, alimentación, etc. Justamente el TLC muestra que el **promedio** de un número lo *suficientemente grande* de v.a. independientes sigue una distribución *muy cecana* a una normal. Así,  cuando una v.a. resulta de sumar **muchas v.a. aleatorias independientes**, la distribución normal es una *buena aproximación* de la v.a. que resulta. En ello reside el vínculo de esta distribución con el TLC y, lo que posteriormente veremos en inferencia estadística y modelamiento de fenómenos de interés.\n\n:::{.callout-note}\n### Definición\nUna variable aleatoria continua sigue una distribución normal estándar si la función de densidad viene dada por\n$$\n\\phi(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}x^2}, \\qquad -\\infty < x < + \\infty \n$$\nNotemos, además, que el soporte de la distribución normal estándar son todos los $\\mathbb{R}$, i.e., $-\\infty < x < + \\infty$.\n\nPor otro lado, se puede probar que $\\int^{+\\infty}_{-\\infty} \\phi (X) dx =1$, lo cual está unido al hehco de que $\\phi(x)>0$ para todo $x$ establece que $\\phi(x)$ efectivamente define una densidad de probabilidad\n:::\n\nGráficamente, la densidad de una distribución normal estándar se ve así:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(dplyr, ggplot2, grid)\n\n# Crear datos para la densidad de la normal estándar\ndf_norm <- data.frame(x = seq(-4, 4, length.out = 1000)) |>\n  mutate(phi = dnorm(x))\n\n# Graficar densidad y marcar intervalos de 68% y 95%\nggplot(df_norm, aes(x = x, y = phi)) +\n  geom_line(size = 1, color = \"#104E8B\") +\n  geom_vline(xintercept = c(-1, 1), linetype = \"dashed\", color = \"#24693D\") +\n  geom_vline(xintercept = c(-2, 2), linetype = \"dashed\", color = \"#8B1A1A\") +\n  # Flecha para 68%\n  geom_segment(aes(x = -1, y = 0.25, xend = 1, yend = 0.25),\n               arrow = arrow(ends = \"both\", length = unit(0.2, \"cm\")),\n               color = \"#24693D\", size = 1) +\n  annotate(\"text\", x = 0, y = 0.27, label = \"68%\", color = \"#24693D\", size = 4) +\n  # Flecha para 95%\n  geom_segment(aes(x = -2, y = 0.06, xend = 2, yend = 0.06),\n               arrow = arrow(ends = \"both\", length = unit(0.2, \"cm\")),\n               color = \"#8B1A1A\", size = 1) +\n  annotate(\"text\", x = 0, y = 0.08, label = \"95%\", color = \"#8B1A1A\", size = 4) +\n  labs(\n    x     = \"x\",\n    y     = expression(phi(x)),\n    title = \"Densidad de la normal estándar\"\n  ) +\n  theme_minimal(base_family = \"serif\", base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    axis.text  = element_text(size = 12)\n  )\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/fdp-normal-estandar-1.png){width=672}\n:::\n:::\n\n\n\n\nComo se puede observar, la distribución aquí está centrada en una media de $0$. La *probabilidad* de que nuestra variable aleatoria tome valores entre $[-1, 1]$ es del 68%. A su vez, que la v.a. tome valores $[-2, 2]$ es del 95%. Por otro lado, la *forma* de la densidad de la distribución es similar a una campana. Por ello, cuando una distribución toma dicha *forma*, se le suele decir que toma forma de **campana de Gauss**. \n\nComo en todas las otras distribuciones, a partir de la f.d.p. podemos definir su f.d.c., que denotaremos por $\\Phi$. Definamosla: \n\n\n:::{.callout-note}\n### Definición\nLa función cumulativa correspondiente a $\\phi(x)$, que se denota por $\\Phi(x)$, está dada por\n$$\n\\Phi(x) = \\int^x_{-\\infty} \\phi(x)du\n$$\nUn punto importante, es que la función cumulativa de una normal estándar no tiene una expresión cerrada, por lo cual se usan tablas (con Excel o similares) para calcular sus valores. \n\nPor otro lado, una consecuencia directa de la **simetría** antes mencionada (y graficada) para $\\phi(x)$ es que su cumulativa $\\Phi(\\cdot)$ evaluada en $-x$ es \n$$\n\\Phi(-x) = 1 - \\Phi(x)\n$$\n:::\n\n#### Propiedades de la Distribución Normal Estándar\n\nPor último, dejamos expresadas las propiedades de una distribución normal estándar. No las demostraré, luego que es más útil ver algunas cuestiones antes, particularmente al ver familia de distribuciones normales.  \n\n::: {.callout-note}\n### Propiedades de la Distribución Normal Estándar\n\nSupongamos que la v.a. $Z$ sigue una distribución normal estándar, que denotamos como $Z \\sim \\mathcal{N}(0,1)$. Entonces, \n\n1. Se tiene que $\\mbox{E}(Z)=0,$\n\n2. Se tiene que $\\mbox{Var}(Z)=1,$\n\n3. Se tiene que $\\mbox{E}(Z^3)=0,$\n\n4. Se tiene que $\\mbox{Var}(Z^4)=3$\n:::\n\n\n## 4. Inferencia estadística: idea general e introducción a los test de hipótesis\n\n\nEntonces, ahora sí. Hsata aquí, tenemos que existen variables aleatorias discretas o continuas, que tienen una distribución de probabilidad asociada, que se puede calcular mediante sus funciones de densidad (cumulativa). Ahora bien, es justamente esa distribución de las v.a. que fundamentan la *inferencia estadística*. No obstante, en estadística ocurre algo increíble que nos permitirá tratar distribuciones como si fueran la misma, permitiéndonos a su vez predecir mejor valores probables dentro de esta distribución. Para ello, veremos dos conceptos fundamentales: la Ley de los Grandes Números y el Teorema del Límite Central.\n\n\n### Ley de los grandes números (LGN)\n\nPara comprender la ley de los grandes números, introducimos los conceptos de muestras aleatorias, media de los datos y media muestral. Hasta ahora, no hemos distinguido claramente entre muestra y población. Esta distinción, en conjunto con propiedades y leyes, como la Ley de los Grandes Números (LGN) y el Teorema del Límite Central (TLC), son la base esencial de la inferencia estadística (Canavos, 1988; DeGroot y Schervish, 2014).\n\nCuando hablamos de población en términos estadísticos, nos referimos al universo completo del fenómeno a medir, tal cual es. Si nuestra población en nuestro estudio son los chilenos, entonces, la población serán todos los individuos de la población chilena. La única forma de obtener esto \"en bruto\" es encuestando a cada uno de los chilenos, que es lo que pretende el censo (y aún así nunca se logra esto a cabalidad). Obviamente, esto sería no solo muy costoso, sino que además es muy complejo de lograr. Va ser dificil tener plena certeza de que mediante un censo podremos obtener a *toda* la población, dado que hay personas en situación de calle, migrantes irregulares, etc., que intervienen directamente en relaciones que nos interesan investigar. \n\nEn cambio, una muestra es una porción de *ese* universo que nos interesa investigar. Más concretamente, es una colección de datos que se obtienen al llevar a cabo repetidos ensayos de un experimento para lograr una evidencia representativa acerca de la población. Una muestra representativa de la población chilena, por ejemplo, es la CASEN, la ENUT, la ENE, etc., que a través de encuestas solo una porción de la población objetivo, con un diseño muestral adecuado^[Por ejemplo, la CASEN utilliza un diseño probabilístico, estratificado y bietápico; y es representativa a nivel nacional, áreas geográficas urbana y rural y regional. Pero no entra en ese este apunte ni muestreo ni diseños de investigación], **infieren** información de la población. Por lo tanto, la idea central que queremos tratar de las muestras, es que con ellas podemos *inferir* estadísticamente de una población específica. Esto conlleva ciertos grados de error, como vimos anteriormente, pero errores conocidos. Por lo tanto, no son *equivocaciones*, sino que un error estadístico medible.  \n\n#### Muestras aleatorias, medias de los datos y media muestral \n\n:::{.callout-note}\n### Definición\nConsideremos una distribución de probabilidad dada en la recta real que puede representarse mediante una función de probabilidad (f.p.) o una función de densidad de probabilidad (f.d.p) $f$. Se dice que $n$ variables aleatorias $X_1,\\ldots,X_n$ forman una **muestra aleatoria** de esta distribución si estas variables aleatorias son *independientes* y la función de probabilidad marginal o la función de densidad de probabilidad de cada una de ellas es $f$. También se dice que estas variables aleatorias son **independientes e idénticamente distribuidas**, lo que se abrevia como i.i.d. Nos referimos al número *n* de variables aleatorias como el tamaño de la muestra [@probability-statistics]\n:::\n\nEntonces, cuando hablemos de **muestra aleatoria**, nos referimos a esa colección de variables $X_1, \\ldots, X_n$, con $n$ observaciones y que entrega datos $x_1, \\ldots, x_n$ que no son más que *la realización* de dichas v.a. Si esas v.a. de la muestra aleatoria son i.i.d., entonces comparten una función de probabilidad $f(x)$ y una esperanza $\\mu$. El promedio de los datos provenientes de las realizaciones de *cada* v.a. **convergerá** a $\\mu$ si $n$ es lo suficientemente grande, tal que\n$$\n\\lim_{n\\to \\infty} \\frac{1}{n}\\sum^n_{i=1} x_i \\approx \\mu\n$$\nY este promedio de los datos, pues, corresponde a la **media de los datos**. Veamos esta convergencia en lanzamientos de un dado, que, obviamente es una muestra aleatoria. Veamos como, en función de que aumenten los experimentos, y con ello la realización de nuestra v.a. $X$, es decir que $n$ crezca por cada lanzamiento de dado, la \"esperanza $\\mu$ empírica\" (i.e., la media muestral) se va acercando cada vez más a la \"teórica\". Sabemos ya que la esperanza del lanzamiento de un dado es $3,5$. Veamos si se va a acercando o no en una simulacion de R. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\npacman::p_load(dplyr, ggplot2, scales)\n\nset.seed(2025)\nn_max <- 100000\nrolls <- sample(1:6, size = n_max, replace = TRUE)\n\ndf_lgn <- tibble(\n  N     = seq_len(n_max),\n  media = cumsum(rolls) / N\n)\n\nggplot(df_lgn, aes(x = N, y = media)) +\n  geom_line(color = \"#4E79A7\") +\n  geom_hline(yintercept = 3.5, linetype = \"dashed\", color = \"#E15759\") +\n  scale_x_log10(labels = comma) +\n  labs(\n    x     = \"Número de lanzamientos\",\n    y     = \"Media muestral\",\n    title = \"Ley de los Grandes Números: convergencia de la media muestral\",\n    subtitle = \"Media teórica del dado = 3,5\"\n  ) +\n  theme_minimal(base_family = \"serif\", base_size = 14) +\n  theme(\n    plot.title    = element_text(face = \"bold\", size = 16, hjust = 0.5),\n    plot.subtitle = element_text(size = 12, hjust = 0.5),\n    axis.text     = element_text(size = 12)\n  )\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/eg-simulacion-dado-LGN-1.png){width=672}\n:::\n:::\n\n\n\n\nEntonces, como vimos, nuestra variable aleatoria $X$, que en este caso es un dado, al realizarse, al lanzar el dado, genera un dato, un $x_1$. Ese dato es el resultado del experimento. Por ejemplo, que salga 5. Si sale 5, el promedio muestral, **hasta ahora**, que denotaremos como $\\bar{x}_1$ de esa primera realización es 5, $\\bar{x}_1=5$. Si sale para $x_2=2$, entonces $\\bar{x}_2=4,5$. Y así. Como vemos en la simulación, los primeros lanzamientos arrojan un promedio distinto de 3,5. No obstante, mientras más lanzamos el dado, mientras más realizaciones de nuestra v.a. hay, y por tanto, el tamaño muestral $n$ crece, más se asemeja a la esperanza, al valor esperado, que en este caso $\\mu=3,5$. Ello es tal, porque nuestras v.a. tienen una *distribución* en común, que denotamos como (es decir, son i.i.d.), cuyo valor esperado es $\\mbox{E}(X_i)=\\mu$. Y vimos que se cumple empíricamente que   \n$$\n\\lim_{n\\to \\infty} \\frac{1}{n}\\sum^n_{i=1} x_i \\approx \\mu\n$$\npara este caso. Pero además, veremos que esto es así siempre y cuando tengamos una muestra aleatoria adecuada. Ya sabemos qué es la media muestral y con este ejemplo queda claro. Pero formalicemoslo para este caso. \n\n:::{.callout-note}\n### Definición\nLa **media muestral** $(\\bar{X}_n)$ corresponde a una v.a. que, por definición, es el promedio de las v.a. de una muestra aleatoria. Si nuestra muestra aleatoria se compone por v.a. $X_1, X_2, \\ldots, X_n$ i.i.d., con $\\mbox{E}(X_i) = \\mu$ y $\\mbox{Var}(X_i)= \\sigma^2$. Entonces, tendremos que\n$$\n\\begin{aligned}\n  \\bar{X}_n &= \\frac{1}{n} \\sum^n_{i=1}X_i\\\\\n  \\mbox{E}(\\bar{X}_n) &= \\mu\\\\\n  \\mbox{Var}(\\bar{X}_n) &= \\frac{\\sigma^2}{n}\\\\\n  \\mbox{sd}(\\bar{X}_n) &= \\frac{\\sigma}{n}\n\\end{aligned}\n$$\nDado que $\\bar{X}_n$ es una v.a. del promedio de las v.a. de la muestra, es lógico que tendrá el mismo valor esperado, pero una dispersión bastante menor. El valor observado o realización de la variable aleatoria $\\bar{X}_n$ corresponde al dato $\\bar{x}_n$. \n:::\n\n#### Ley de los grandes números\n\nSabiendo todo esto, ya podemos enunciar la Ley de los grandes números (LGN). Y lo haremos en relación con la media muestral \n\n\n:::{.callout-note}\n### Definición\nSean $X_1, \\ldots, X_n$ variables aleatorias independientes e idénticamente distribuidas con esperanza común $\\mu$ y varianza común $\\sigma^2$, ambas finitas. Entonces la media muestral, $\\bar{X}_n$, converge a $\\mu$ a medida que $n$ aumenta. \n:::\n\nCiertamente la LGN aplica en varios aspectos y es uno de los pilares que sustenta la **consistencia** de muchísimos otros estimadores y métodos de probabilidad y estadística. Mantiene proporciones muestrales, e.g., si $X_1, \\ldots, X_n$ son variables Bernoulli ($p$) la proporción \n$$\n\\hat{p}_n = \\frac{1}{n}\\sum^n_{i=1} X_i\n$$\nconverge al $p$ verdadero (poblacional), lo cual es clave para el trabajo en encuestas, calidad y demografía. Sirve para estimadores basados en momentos, que veremos más adelante al ver método de momentos. Entrega consistencia a la varianza muestral. Y así un sin fin de aspectos donde veremos presente la LGN. No obstante, me interesa seguir profundizando más en esto más adelante. Por ahora, puede ser útil revisar el apéndice \"*Asymptotic Theory*\" [@microeconometrics], el cual que ofrece un buen resumen de los elementos más centrales de teoría asintótica aplicada a estimadores econométricos. \n\nPor último, la demostración de la definición que dimos de la LGN, solo hay  que usar la definición de convergencia. Esto es, $\\bar{X}_n$ converge a $\\mu$ dado que \n$$\n\\lim_{n\\to\\infty} \\mbox{E}(\\bar{X}_n - \\mu)^2= 0\n$$\nPues si descomponemos, llegaremos a $\\lim_{n\\to\\infty}\\sigma^2/n$, y como $n\\to \\infty$, entonces nos dará $0$. Que demostramos tal que\n$$\n\\begin{aligned}\n  \\lim_{n\\to\\infty} \\mbox{E}(\\bar{X}_n - \\mu)^2&= \\lim_{n\\to\\infty} \\mbox{Var}(\\bar{X}_n)\\\\\n  &= \\lim_{n\\to\\infty} \\frac{\\sigma^2}{n} \\\\\n  &=0\n\\end{aligned}\n$$\nMás allá de lo formal, la intuición detrás del resultado nos indica que la media mustral **pierde su aleatoriedad** cuando $n\\to \\infty$, ergo, se centra en el parámetro $\\mu$. Esa es, justamente, la intuición detrás de la LGN. Y es lo que vimos con la simulación del dado. \n\n### Teorema Central del Límite (TCL)\n\nFinalmente, para casi finalizar el capítulo por fin, veremos el teorema del limite central. Dejo esto al final porque, junto con la Ley de los Grandes Números, y todo lo que hemos visto hasta ahora, tendremos una base sólida para meternos en estadística inferencial como tal. Hagamoslo mediante la siguiente motivación. \n\nConsideremos la siguientes funciones de probabilidad y función de densidad de probabilidad: Poisson con $\\lambda=0,8$, Bernoulli con $p=0,8$ y Exponencial de $\\lambda=0,8$. Como ya sabemos, son v.a. con distribuciones muy distintas, partiendo porque dos son discretas y la otra continua. Pero incluso las discretas son distintas entre sí, dado que la Bernoulli solo puede tomar dos valores. Además, gráficamente, se ven así: \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\npacman::p_load(dplyr, ggplot2, patchwork, scales)\n\n# 1) Poisson(λ = 0.8)\ndf_pois <- tibble(x = 0:6,\n                  f = dpois(x, lambda = 0.8))\np_pois <- ggplot(df_pois, aes(x = x, y = f)) +\n  geom_segment(aes(xend = x, y = 0, yend = f),\n               size = 1, color = \"#9467BD\") +\n  geom_point(size = 3, color = \"#9467BD\") +\n  labs(title = \"Poisson (λ = 0.8)\", x = \"x\", y = \"f(x)\") +\n  scale_y_continuous(labels = comma) +\n  theme_minimal(base_family = \"serif\", base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5))\n\n# 2) Bernoulli(p = 0.8)\ndf_bern <- tibble(x = c(0, 1),\n                  f = dbinom(x, size = 1, prob = 0.8))\np_bern <- ggplot(df_bern, aes(x = x, y = f)) +\n  geom_segment(aes(xend = x, y = 0, yend = f),\n               size = 1, color = \"#9467BD\") +\n  geom_point(size = 3, color = \"#9467BD\") +\n  labs(title = \"Bernoulli (p = 0.8)\", x = \"x\", y = \"f(x)\") +\n  scale_y_continuous(labels = comma, limits = c(0, 1)) +\n  theme_minimal(base_family = \"serif\", base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5))\n\n# 3) Exponencial(λ = 0.8)\ndf_exp <- tibble(x = seq(0, 1, length.out = 1000),\n                 f = dexp(x, rate = 0.8))\np_exp <- ggplot(df_exp, aes(x = x, y = f)) +\n  geom_line(size = 1, color = \"#9467BD\") +\n  labs(title = \"Exponencial (λ = 0.8)\", x = \"x\", y = \"f(x)\") +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = comma) +\n  theme_minimal(base_family = \"serif\", base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5))\n\n# Mostrar los tres gráficos lado a lado\np_pois + p_bern + p_exp + plot_layout(ncol = 3)\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/poisson-bernoulli-exp-graph-1.png){width=672}\n:::\n:::\n\n\n\n\nEntonces, ¿qué tienen en común estas variables aleatorias con distinta distribución? Por sí solas, nada, o al menos no a primera vista. Pero, si sumaramos cada vez más v.a. de esas mismas distribuciones, y graficaramos la suma de cada distribución al irse sumando más v.a. Poisson, Bernoulli y Exponencial, veríamos que sí tienen algo en común: \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\npacman::p_load(dplyr, ggplot2, purrr, patchwork)\n\nset.seed(1917)\n\n# Parámetros y tamaños de muestra\nlambda_pois <- 0.8\np_bern      <- 0.8\nrate_exp    <- 0.8\nns          <- c(1, 50, 320)\nn_rep       <- 10000\n\n# Etiquetas y colores\nlbls <- c(pois = \"Poisson\", bern = \"Bernoulli\", expo = \"Exponencial\")\ncols <- c(pois = \"#1F77B4\", bern = \"#7F7F7F\", expo = \"#2CA02C\")\n\n# Función para simular y graficar un solo histograma\nplot_suma <- function(dist, n) {\n  xvals <- switch(dist,\n    pois = replicate(n_rep, sum(rpois(n, lambda_pois))),\n    bern = replicate(n_rep, sum(rbinom(n, size = 1, prob = p_bern))),\n    expo = replicate(n_rep, sum(rexp(n, rate = rate_exp)))\n  )\n  ggplot(data.frame(x = xvals), aes(x)) +\n    geom_histogram(aes(y = after_stat(density)),\n                   bins = 30,\n                   fill = cols[dist], alpha = 0.6,\n                   color = cols[dist]) +\n    labs(\n      title = paste0(\"Σ Xᵢ con n = \", n, \" (\", lbls[dist], \")\"),\n      x     = \"Suma de variables\",\n      y     = \"Densidad\"\n    ) +\n    theme_minimal(base_family = \"serif\", base_size = 10) +\n    theme(\n      plot.title   = element_text(face = \"bold\", size = 11, hjust = 0.5),\n      axis.text   = element_text(size = 10)\n    )\n}\n\n# Para cada n, generar una fila de tres gráficos\nplots_por_n <- map(ns, function(n) {\n  map(c(\"pois\",\"bern\",\"expo\"), ~ plot_suma(.x, n)) %>% \n    wrap_plots(nrow = 1)\n})\n\n# Mostrar todas las filas apiladas\nwrap_plots(plots_por_n, ncol = 1)\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/simulacion-poisson-bern-exp-graph-TLC-1.png){width=672}\n:::\n:::\n\n\n\n\nQuizás tanto gráfico marea, pero de izquierda a derecha, vemos como al sumar las varaibles con distintas distribuciones, $\\sum X_i$, todas van pareciéndose cada vez más a una distribución normal. Esto no es para nada obvio si vemos que en $\\sum X_i$ con $n=1$, en todos los casos, la distribución es muy distinta. Pero cuando ya hay un $n=50$ se parecen aún más. Al seguir creciendo, se parecen aún más. \n\nEste resultado es **muy potente**. Pues nos indica que la distribución de la suma de v.a., *independiente* de la distribución original de cada v.a. sumada, se *aproxima* a una distribución normal. A esta idea, por tanto, es lo que le llamamos el Teorema del Límite Central. \n\n\nLo que vimos recién es una forma de *presentar* el TLC. Pero no es la única. La que vimos ahora, podemos formalizarla así: \n\n\n:::{.callout-note}\n### Definición\nLa suma de variables aleatorias i.i.d. se distribuye *aproximadamente* como una normal, cuya media y varianza son resultado de la suma de las v.a., tal que\n$$\n\\sum^n_{i=1} X_i  \\overset{a}{\\sim} \\mathcal{N}(n\\mu, n \\sigma^2)\n$$\ndonde $\\overset{a}{\\sim}$ significa \"*se distribuye aproximadamente como*\". A esta primera definición, la denominaremos como la primera formulación de TLC\n:::\n\nUna segunda formulación, como habíamos adelantado anteriormente, es respecto al promedio. Lo cual tiene sentido, porque al final si la suma de $X_i$ tiende a una normal, ¿por qué no el promedio, que es una suma ponderada? Siguiendo la lógica anterior, \n\n:::{.callout-note}\n### Definición\nEl promedio de $n$ variables aleatorias i.i.d. se distribuye *aproximadamente* como una normal, cuya media y varianza son iguales a la esperanza y a la varianza del promedio muestral, tal que\n$$\n\\bar{X}_n \\overset{a}{\\sim} \\mathcal{N} \\left(\\mu,  \\frac{\\sigma^2}{n}\\right)\n$$\n:::\n\nSi nos damos cuenta, segunda formulación no es más que la multiplicación de la primera por $1/n$. \n\nUna tercera formulación, que probablemente tratemos más adelante, reside en lo siguiente. En algunos casos, la parametrización de la distribución normal queremos que no dependa de $n$. En esto casos, hacemos lo que se llama como \"estandarización\". El proceso es el siguiente: si a la segunda formulación, le restamos $\\mu$, nos queda como\n$$\n\\bar{X}_n \\overset{a}{\\sim} \\mathcal{N} \\left(\\mu,  \\frac{\\sigma^2}{n}\\right) - \\mu = \\bar{X}_n-\\mu \\overset{a}{\\sim} \\mathcal{N} \\left(0,\\frac{\\sigma^2}{n}\\right)\n$$\nLuego dividimos por la desviación estándar: \n$$\n\\bar{X}_n-\\mu \\overset{a}{\\sim} \\mathcal{N} \\left(0,\\frac{\\sigma^2}{n}\\right) \\div \\mbox{sd}(\\bar{X}_n) = \\frac{\\bar{X}_n-\\mu}{\\mbox{sd}(\\bar{X}_n)} \\overset{a}{\\sim}\\mathcal{N}(0,1)\n$$\nLo que es igual a $\\sigma/\\sqrt{n}$. Es decir,\n$$\n\\frac{\\bar{X}_n-\\mu}{\\mbox{sd}(\\bar{X}_n)} \\overset{a}{\\sim}\\mathcal{N}(0,1) =\\frac{\\bar{X}_n-\\mu}{\\sigma/\\sqrt{n}} \\overset{a}{\\sim}\\mathcal{N}(0,1)\n$$\nY así llegamos a una tercera formulación. Solo hay que restar la media y dividir por la desviación estándar y, al final, tomar esta última expresión y aplicamos la regla de 3, o dicho más bonito y como dicen los profes, \"con un poco de algebra\", llegamos a que\n$$\n\\frac{\\sqrt{n}(\\bar{X}_n-\\mu)}{\\sigma} \\overset{a}{\\sim}\\mathcal{N}(0,1)\n$$\nY a ese proceso le decimos **estandarización**. \n\nVeamos esto gráficamente. Apliquemosle a una variable aleatoria Poisson de parámetro $\\lambda=5$ la primera, la segunda y la tercera formulación. Partamos por la primera\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(ggplot2)\n\nset.seed(1917)\nlambda <- 5\nn      <- 1000\nn_rep  <- 10000\n\n# 1) Primera formulación: suma de n v.a. Poisson(λ=5)\nsuma <- replicate(n_rep, sum(rpois(n, lambda)))\nggplot(data.frame(x = suma), aes(x)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 bins = 100,\n                 fill = \"#EEC9E5\", color = \"#7C4D79\", alpha = 0.6) +\n  labs(\n    title = \"Primera formulación: Σ Xᵢ con n = 1000 (Pois(λ=5))\",\n    x     = \"x\",\n    y     = \"Densidad\"\n  ) +\n  theme_minimal(base_family = \"serif\", base_size = 12) +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5))\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/primera-formulacion-1.png){width=672}\n:::\n\n```{.r .cell-code}\n\n\n# Segunda formulación: promedio de n v.a. Poisson(λ=5)\nmedia <- replicate(n_rep, mean(rpois(n, lambda)))\nggplot(data.frame(x = media), aes(x)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 bins = 100,\n                 fill = \"#EEC9E5\", color = \"#7C4D79\", alpha = 0.6) +\n  labs(\n    title = \"Segunda formulación: Σ Xᵢ/n con n = 1000 (Pois(λ=5))\",\n    x     = expression(bar(X)[n]),\n    y     = \"Densidad\"\n  ) +\n  theme_minimal(base_family = \"serif\", base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 12, hjust = 0.5),\n    axis.text  = element_text(size = 10)\n  )\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/primera-formulacion-2.png){width=672}\n:::\n\n```{.r .cell-code}\n\n# Tercera formulación: variable estandarizada\nstdz <- replicate(n_rep, {\n  m <- mean(rpois(n, lambda))\n  (m - lambda) / sqrt(lambda / n)\n})\nggplot(data.frame(x = stdz), aes(x)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 bins = 100,\n                 fill = \"#EEC9E5\", color = \"#7C4D79\", alpha = 0.6) +\n  labs(\n    title = expression(\"Tercera formulación: \" *\n      frac(sqrt(n) * (bar(X)[n] - mu), sigma) *\n      \" con \" * X[i] ~ \"~Pois(\" * lambda == 5 * \")\"),\n    x = expression(frac(sqrt(n) * (bar(X)[n] - mu), sigma)),\n    y = \"Densidad\"\n  ) +\n  theme_minimal(base_family = \"serif\", base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 12, hjust = 0.5),\n    axis.text  = element_text(size = 10)\n  )\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/primera-formulacion-3.png){width=672}\n:::\n:::\n\n\n\n\n\nComo vemos, no cambia mucho la forma. Todas siguen la campana de Gauss característica de las distribuciones normales. No obstante, cambian los valores de las abscisas y las ordenadas, pues justamente hay trasnformaciones en su \"escala\". Por ejemplo, la primera tiene un eje $X$ con valores entre 4700 y 5200. La segunda de 4,7 a 5,2, porque está dividido por $n$ siguiendo la segunda formulación. Siguiendo la tercera, es decir, un proceso de *estandarización*, el eje de las abscisas queda como una normal estándar :). Entonces, formalicemos todo esto\n\n::: {.callout-note}\n### Teorema Central del Límite\n\nSean $X_1, X_2, \\ldots, X_n$ variables aleatorias i.i.d. con media $\\mu$ y varianza $\\sigma^2$. Denotamos por $\\bar{X}_n$ la media meustral de las primera $n$ variables aleatorias. Entonces,\n$$\n\\forall y  \\quad \\lim_{n\\to \\infty} \\Pr\\left(\\frac{\\sqrt{n}(\\bar{X}_n-\\mu)}{\\sigma} \\leq y \\right) = \\Phi(y)\n$$\nEl TCL es un resultado de convergencia en distribución de probabilidad. La distribución de la v.a. dada por \n$$\n\\frac{\\sqrt{n}(\\bar{X}_n-\\mu)}{\\sigma}\n$$\nconverge a la distribución normal estándar cuando $n$ tiende a infinito. \n:::\n\nHay que insistir en la importancia de este resultado. Extrememos el caso. Imaginemos que no tenemos idea de la distribución de nuestras variables aleatorias originales. Mientras trabajemos con la suma (primera formulación) o el promedio (segunda) de un $n$ grande de v.a., entonces podemos usar la distribución normal para *aproximar* la distribución de la suma o promedio. Con este resultado, pues, podremos aproximar todas las distribuciones que querramos. No lo haré aquí. Pero sin duda tendremos que hacerlo al ir avanzando en cuestiones estadísticas y econométricas\n\nPor otro lado, hasta ahora, hemos visto la Ley de los Grandes Números (LGN) afirma que, para $n$ grande, la media muestral es *aproximadamente* una constante (la experanza $\\mu$). Acabamos de ver, además, que el TCL afirma que, para $n$ grande, la media muestral se distribuye *aproximadamente* normal con media $\\mu$ y varianza $\\sigma^2/n$. Visto así, podría parecer contradictorio que la media muestral, al mismo tiempo, sea aproximadamente constante y distribuirse aproximadamente normal. Pero no hay contradicción, porque como ya podríamos intuir, la varianza de $\\bar{X}_n$ tiende a cero cuando $n$ tiende a infinito. \n\n\n## 5. Test de hipotesis: una introducción\n\nLa estadística es una ciencia, disciplina o rama de las matemáticas que estudia la recolección, análisis, interpretación y presentación de masas de información numérica^[Es discutible si está dentro de la matemática o tiene autonomía disciplinar. Lo coloco justamente porque es una discusión, y que no llevaré a cabo aquí. Mi opinión es que es una disciplina autónoma de las matemáticas y que, más bien, se fundamenta la teoría matemática, particularmente en la de la probabilidad. Pero no lo veo como una *rama* de la matemática. Ver discusiones de esto en (Wackerly, et al., 2010; DeGroot y Schervish, 2014)]. Ante todo, la \"*estadística  es una teoría de la información, siendo la inferencia su objetivo*\" ([Wackerly, et al., 2010, p. 2](https://www.cimat.mx/ciencia_para_jovenes/bachillerato/libros/[Wackerly,Mendenhall,Scheaffer]Estadistica_Matematica_con_Aplicaciones.pdf)). En esta línea, al ser una ciencia que tiene como fin principal *inferir* acerca de una población, con base a información contenida en una muestra de esa población, he enfatizado ello en la decisión de dedicarle un capítulo a la estadística inferencial como simplemente estadística. \n\n### Estimación e introducción al test de hipótesis\n\nComo ya hemos dicho, el objetivo de la estadística es usar la información contenida en una muestra para hacer inferencias para sacar conclusiones sobre la población de la que proviene. Dado que las poblaciones se describen mediante valores numéricos —los llamados *parámetros*—, gran parte del trabajo estadístico consiste en estimar uno o varios de esos parámetros a partir de la muestra. Para ello, las distribuciones que hemos visto resultan fundamentales en el diseño y la justificación de los métodos de estimación.\n\nEn este sentido, la información de la muestra se puede emplear para calcular el valor de una estimación puntual, una estimación de intervalo o ambas. En cualquier caso, la estimación real se logra con el uso de un *estimador* del **parámetro objetivo** ([Wackerly, et al., 2010](https://www.cimat.mx/ciencia_para_jovenes/bachillerato/libros/[Wackerly,Mendenhall,Scheaffer]Estadistica_Matematica_con_Aplicaciones.pdf)). Definamos, entonces, que es un estimador.\n\n:::{.callout-note}\n### Definición\nSiguiendo a Wackerly, et al. ([2010](https://www.cimat.mx/ciencia_para_jovenes/bachillerato/libros/[Wackerly,Mendenhall,Scheaffer]Estadistica_Matematica_con_Aplicaciones.pdf)]), un estimador es una regla, expresión matemática o fórmula, que indica **cómo calcular** el valor de una estimación con base a las mediciones contenidas en una muestra. Por ejemplo, para la media muestral, \n$$\n\\bar{Y}=\\frac{1}{n} \\sum^n_{i=1}Y_i\n$$\nse puede calcular un estimador *puntual* de la media poblacional, $\\mu$. \n:::\n\nLuego profundizaremos en los tipos de estimadores, pero antes de ello es conveniente detallar el concepto de test de hipótesis. Un **test de hipótesis** (o prueba de hipótesis) es un procedimiento sistemático que, partiendo de los datos de una muestra, nos ayuda a evaluar si una afirmación concreta sobre un parámetro poblacional (la hipótesis nula) es consistente con la evidencia empírica, o si deberíamos aceptar una afirmación alternativa (la hipótesis alternativa). En esencia, un test de hipótesis mide la compatibilidad de la muestra con una suposición inicial, cuantificando el grado de discrepancia mediante una estadística de prueba y un nivel de significación.\n\nVeremos qué es un test de hipótesis a partir de 4 conceptos nuevos: modelo probabilístico, hipótesis nula e hipótesis alternativa, estadístico de prueba y valor-p ($p-$*value*). Pero veamoslo mediante un ejemplo. Y usemos bases de datos reales. Supongamos que nos interesa saber si el hecho de ser mujer tiene un efecto en realizar o no trabajo doméstico no remunerado. Para plantear esto de manera tal que nos permita verificarlo empíricamente, tenemos dos hipótesis posibles: afecta ser mujer o no afecta ser mujer en participar en el trabajo doméstico no remunerado (TDNR, desde ahora). Estas dos hipótesis son llamadas formalmente como Hipótesis Nula ($H_0$) e Hipótesis alternativa ($H_A$ o $H_1$). Generalmente, aunque no es necesariamente así, la hipótesis nula es aquella en la que se implica ausencia de efecto, mientras que la alternativa implica que sí lo hay. Tenemos entonces, \n\n- $H_0:$ Ser mujer no tiene un efecto en participar en el TDNR\n\n- $H_1:$ Ser mujer si tiene un efecto en participar en el TDNR \n\nRevisemos los datos, pues, en los que testearemos nuestras hipótesis empíricamente. Para ello, utilizaremos la Encuesta de Uso del Tiempo (ENUT) 2023. Esta encuesta, realizada entre el 14 de septiembre y el 29 de diciembre de 2023 por el Instituto Nacional de Estadísticas (INE),  permite analizar cómo las personas distribuyen su tiempo en las actividades de la vida cotidiana, enfocándose principalmente en el trabajo no remunerado, el trabajo en la ocupación y otras actividades personales. La ENUT recopila información en los principales centros urbanos de cada región del país, mediante entrevistas directas a personas de 12 años en adelante [INE, 2025](https://www.ine.gob.cl/docs/default-source/uso-del-tiempo-tiempo-libre/metodologias/ii-enut/documento-metodologico-ii-enut-2023.pdf?sfvrsn=c4a74cc4_6). En ella, se encuentra la variable `p_tdnr_dt` que registra la Participación en día tipo en Trabajo Doméstico No Remunerado para el Propio Hogar en un día tipo de los encuestados, a partir de las categorías $0:$ No y $1:$ Sí.\n\nCargamos los datos entonces y planteamos nuestro modelo. \n\n\n\n\n\n::: {.cell}\n\n```\n## [1] 48020  1089\n```\n:::\n\n\n\n\nUna vez tenemos nuestro datos, tenemos que primero obtener nuestro grupo de interés para plantear la hipótesis. En nuestro caso, nos interesan las mujeres, que se registran en la variable `sexo` y si participan o no en el TDNR, registrado en `p_tdnr_dt`^[Aquí usaré el operador `%>%` de `magrittr` para mostrarles cómo funciona. Es muy parecido y a mi me gusta más el nuevo de R base, pero para que vean simplemente.]. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\npacman::p_load(dplyr, knitr, \n               magrittr) #Para el operador pipeline (%>%) \n\n# Filtrar solo mujeres\nenut_muj <- enut %>% \n  dplyr::filter(haven::as_factor(sexo, \n                                 levels = \"labels\") == \"Mujer\")\n\n#Comprobamos filtrado\nfrq(enut_muj$sexo)\n## Sexo (x) <numeric> \n## # total N=25411 valid N=25411 mean=2.00 sd=0.00\n## \n## Value |         Label |     N | Raw % | Valid % | Cum. %\n## --------------------------------------------------------\n##     1 |        Hombre |     0 |     0 |       0 |      0\n##     2 |         Mujer | 25411 |   100 |     100 |    100\n##    96 | Valor Perdido |     0 |     0 |       0 |    100\n##  <NA> |          <NA> |     0 |     0 |    <NA> |   <NA>\n\n# Calcular conteos y porcentajes de participación en TDNR\ntabla_td_muj <- enut_muj %>%\n  filter(p_tdnr_dt %in% c(0, 1)) %>%\n  count(p_tdnr_dt) %>%\n  mutate(\n    Participación   = if_else(p_tdnr_dt == 1, \"Sí\", \"No\"),\n    `Porcentaje (%)` = n / sum(n) * 100\n  ) %>%\n  select(Participación, Conteo = n, `Porcentaje (%)`)\n\n# Mostrar tabla\nknitr::kable(tabla_td_muj, digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n|Participación | Conteo| Porcentaje (%)|\n|:-------------|------:|--------------:|\n|No            |    350|           2.11|\n|Sí            |  16270|          97.89|\n\n\n:::\n:::\n\n\n\n\nComo vemos, el 97.89% de las mujeres sí participa en el TDNR en un día tipo. Entonces, ¿se puede concluir que ser mujer afecta en participar en el TDNR? Todavía no. Por ejemplo, por ahora, no sabemos qué porcentaje es el de los hombres. Y en todo caso, eso sería evaluar dos grupos distintos -lo cual probablemente es más interensante plantear esto para un problema de investigación, pero estamos usando esto como ejemplo solamente. Después usaremos el mismo ejemplo para dos grupos y comparar magnitudes/efectos-. Inclusive, podríamos buscar el porcentaje de los hombres, pero aun así no podríamos afirmarlo con plena certeza solamente usando estadísticas descriptivas de nuestra muestra por másm grande que sea. Vamos construyendo nuestro modelo *probabilístico* para poder afirmar esto con mayor rigurosidad y certeza. En nuestra tabla, vemos que hay $350+16270=16620$ mujeres. Diremos que cada una de ellas se representa como observaciones $i$, donde $i=1,2, \\ldots, 16620$. Con ello, definimos \n$$\nx_i \n\\begin{cases}\n  1, &\\quad \\text{ si participa en TDNR}\\\\\n  0, &\\quad \\text{ si no}\n\\end{cases}\n$$\nPor lo tanto, observamos los datos $x_1, x_2, \\ldots, x_{16620}$, los cuales son una **realización** de una **muestra aleatoria** $X_1, X_2, \\ldots, X_{16620}$. Como cada $x_i$ solo puede tomar 2 valores, 0 y 1, el candidato natural para modelar la distribución de nuestros $X_i$ será una Bernoulli, como ya deberíamos saber. En este caso, entonces, diremos que $X_i \\sim \\mbox{Bern}(p)$. Ahora bien, ¿cuál debería ser el valor de $p$? Depende exclusivamente de nuestra hipótesis. En nuestro caso, nuestra hipótesis nula es $H_0:$ Ser mujer **no** tiene un efecto en participar en el TDNR. Eso se traduciría formalmente en que bajo $H_0$, nuestros datos $X_1, \\ldots, X_{16620}$ son i.i.d.^[Son independientes e idénticamente distribuidas porque son obtenidos de una muestra aleatoria. En este caso, en principio si la ENUT fue bien construida, y ninguna respuesta de cada persona encuesta afecta en la otra (son independientes entre sí) y no tienen una distribución sesgada, podemos asumir que contamos con datos i.i.d.] cuya distribució común es $\\mbox{Bern}(0)$. Ahora, para nuestra hipótesis alternativa, sería asumir que ser mujer si tiene un efecto en participar en el TDNR. Como simplemente queremos comprobar la existencia de un efecto en el caso de la hipótesis alernativa, da igual qué tan grande este sea, simplemente tendríamos que plantear que $X_i \\overset{H_1}{\\sim} \\mbox{Bern}(p), \\quad p\\neq 0$. \n\nFormalicemos. En este caso, diremos que  $X_i \\sim \\mathrm{Bern}(p)$, donde $p$ es la proporción verdadera de mujeres que participan en TDNR. Planteamos, entonces, las siguiente hipótesis\n\n- Hipótesis nula:  \n$$\nH_0\\colon p = p_0\n$$  \ndonde bajo $H_0$, los $X_i$ son i.i.d. $\\mathrm{Bern}(p_0)$.\n\n- Hipótesis alternativa:  \n$$\n  H_1\\colon p \\neq p_0\n$$\n\n### Estadístico de prueba\n\nUna vez ya tenemos las hipótesis bien planteadas, tenemos que proseguir con el **estadístico de prueba**, el cual recibe este nombre porque se usa para **probar** (en el sentido de *testear*) cuál de las dos hipótesis es validada por la evidencia. El **estadístico de prueba** es una variable aleatoria, que denotaremos por $T$, que depende de nuestra muestra aleatoria, i.e., $X_1, X_2, \\ldots, X_{16620}$, cuyas realizaciones $x_1, x_2, \\ldots, x_{16620}$ observamos, y que sirve para testear si la evidencia permite rechazar $H_0$ en favor de $H_1$ o no. Dado que $T$ es una v.a., **tiene una distribución asociada**. Y por ello tuvimos que revisar las distribuciones previo a la inferencia estadística. Luego, la distribución de $T$ siempre tiene que conocerse. \n\n\nPara nuestro caso, consideremos el siguiente estadístico de prueba \n$$\nT \\;=\\; \\sum_{i=1}^{n} X_i,\n$$  \ncon $n = 16620$ y $T_{\\mathrm{obs}} = 16270$. Bajo $H_0: p = p_0$, se tiene que\n$$\nT = \\sum_{i=1}^n X_i \\sim \\text{Binomial}(n, p_0)\n$$\n\nBajo $H_1: p \\neq p_0$:\n$$\nT = \\sum_{i=1}^n X_i \\sim \\text{Binomial}(n, p)\\quad (p \\neq p_0)\n$$\nNo debería ser necesario explicarlo, pero como $X_i \\sim \\mathrm{Bern}(p)$, entonces el estadístico de prueba, $T = \\sum_{i=1}^{n} X_i$ es Binomial, y no Bernoulli. \n\nAhora, para ver mejor esto, hagamos un gráfico de distribución y ver qué queremos calcular. Para el gráfico de la distribución de $T$ bajo $H_{0}$ emplearemos  \n$$\np_{0} = 0.5\n$$\nUsaremos $p_0=0.5$ pues en un test de proporción de una sola muestra y sin otro grupo de comparación, $p_{0}=0.5$ funciona como punto medio neutro que refleja ausencia de preferencia o efecto en la participación (en nuestro caso, que no afecta ser mujer respecto a ser hombre en el fondo). Con ello, bajo $H_{0}\\colon p = 0.5$  el estadístico $T$ queda como \n$$\n  T = \\sum_{i=1}^{n}X_{i} \\sim \\mathrm{Binomial}(n,0.5)\n$$  \nDe este modo, al graficar la función de densidad de esa binomial, podemos  visualizar qué tan alejada está la proporción observada ($T_{\\mathrm{obs}}/n \\approx 0.9789$)  del 50 %, y así evaluar de forma intuitiva el posible rechazo de $H_{0}$.  \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(dplyr, ggplot2, scales)\n\n# Preparar n y Tobs para mujeres\nenut_muj <- enut %>% filter(sexo == 2, p_tdnr_dt %in% c(0,1))\nn        <- nrow(enut_muj)\nTobs     <- sum(enut_muj$p_tdnr_dt == 1)\np0       <- 0.5\n\n# Media y desviación bajo H₀\nmu0 <- n * p0\nsd0 <- sqrt(n * p0 * (1 - p0))\n\n# Grilla para la densidad normal aproximada\nx_vals <- seq(mu0 - 4*sd0, mu0 + 4*sd0, length.out = 1000)\ndf0     <- data.frame(x = x_vals, dens = dnorm(x_vals, mu0, sd0))\n\n# Graficar\nggplot(df0, aes(x = x, y = dens)) +\n  geom_line(color = \"#1F77B4\", size = 1) +\n  geom_vline(\n    xintercept = Tobs,\n    linetype   = \"dashed\",\n    color      = \"#D62728\",\n    size       = 1\n  ) +\n  annotate(\n    \"text\",\n    x = Tobs + sd0*0.5,\n    y = max(df0$dens)*0.9,\n    label = \"T_obs\",\n    color = \"#D62728\",\n    hjust = 0\n  ) +\n  scale_x_continuous(labels = comma) +\n  labs(\n    title = \"Distribución de T bajo H₀: p = 0.5\",\n    x     = expression(T == sum(X[i])),\n    y     = \"Densidad\"\n  ) +\n  theme_minimal(base_family = \"serif\", base_size = 12) +\n  theme(\n    plot.title    = element_text(face = \"bold\", hjust = 0.5),\n    axis.text     = element_text(size = 10)\n  )\n```\n\n::: {.cell-output-display}\n![](02-statistics_files/figure-html/tsimulacion-tdnr-test-1.png){width=672}\n:::\n:::\n\n\n\n\nEl gráfico muestra que, bajo $H_{0}\\colon p=0.5$, la distribución de  \n$$\nT = \\sum_{i=1}^{n}X_{i}\n$$  \nestá centrada en la media  \n$$\n\\mu_{0} = n \\times 0.5 \\approx 8\\,310\n$$  \ncon desviación estándar  \n$$\n\\sigma_{0} = \\sqrt{n \\times 0.5 \\times 0.5}\\approx 64.\n$$  \nEl valor observado  \n$$\nT_{\\mathrm{obs}} = 16\\,270\n$$  \nqueda a más de 100 desviaciones estándar por encima de la media, de modo que  \n$$\n\\Pr\\bigl(T \\ge T_{\\mathrm{obs}}\\mid H_{0}\\bigr)\\approx 0.\n$$  \nPor tanto, es extremadamente improbable observar este resultado si $H_{0}$  \nfuera cierto, y debemos rechazar $H_{0}\\colon p=0.5$. Si bien recordamos, las distribuciones normales, cuya distribución aparece en nuestro caso dado el $n$ grande, se podía dividir en regiones las cuales nos permitía calcular la probabilidad de que se encuentre tal valor en tal región. En este caso, planteamos la hipótesis nula de que no afectara ser mujer en la participación del TDNR. Y nos arrojó un lugar de la curva demasiado alejado de la media. \n\nEsta probabilidad, en este caso *muy improbable*, es la intuición que está detrás de valor$-p$, o $p-$*value*. El $p-$*value* los que nos indica es la **probabilidad de observar un valor del estadístico de prueba tanto o más contarios que el observado _si_** $H_0$ **es cierta**. La intuición detrás es señalar que tan probable es encontrar un valor que lleve a rechazar la hipótesis nula, ergo, aceptar la alternativa. Y esto, el hecho de que sean valores \"más contrarios que el observado si $H_0$ es cierta\", será razonable cuando los valores de $T$ sean más grandes que $t_{obs}$. \n\nDe tal modo, podemos plantear el cálculo de de la f.p. de $T$, como\n$$\np_{obs} = \\Pr(T \\geq t_{obs})\n$$\nEn nuestro caso, podríamso calcualr esto en R como\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(dplyr)\n\n# Filtrar solo mujeres con respuestas válidas\nenut_muj <- enut %>% \n  filter(sexo == 2, p_tdnr_dt %in% c(0, 1))\n\n# Calcular n y estadístico observado\nn     <- nrow(enut_muj)\nt_obs <- sum(enut_muj$p_tdnr_dt == 1)\n\n# Valor bajo H0\np0 <- 0.5\n\n# p-valor de una cola: P(T ≥ t_obs) para T ~ Binomial(n, p0)\np_obs <- pbinom(t_obs - 1, size = n, prob = p0, lower.tail = FALSE)\n\np_obs\n## [1] 0\n```\n:::\n\n\n\n\nComo vemos, es tan improbable que no tenga ningún efecto el hecho de ser mujer en participar o no en el TDNR, que la calculadora nos arroja derechamente 0. Ciertamente, debe ser un número ínfimo y no exactamente cero, pero es tan pequeño que es 0,000...% probable. De tal modo, cuando $T$ sea más grande que $t_{obs}$, entonces será razonable rechazar nuestra hipótesis nula. En nuestro caso, es \"muy\" razonable rechazarla, y por tanto aceptar la hipótesis alternativa, i.e., que sí hay efecto.  \n\nMás allá de esta primera aproximación conceptual e intuitiva del $p-$value, notemos que lo que nos está indicando este es una medida numérica de la consistencia de la hipótesis nula planteada en función de los datos. Si el $p-$value es pequeño, *entonces* la hipótesis nula *no es* una buena descripción de los datos. Si es grande, al contrario, sí lo es. Y por tanto aceptamos la hipótesis nula. \n\n\nAhora bien, en general el $p-$value lo arroja el software o lenguaje de programación que estemos usando casi que por defecto según el análisis. En un modelo de regresión, por ejemplo, R calcula el $p-$*value* automáticamente. Generalmente, no lo calcularemos con tantos pasos como hicimos ahora, pero todo este rodeo fue para ejemplificar bien la intuición detrás y ver los pasos para poder **inferir** a partir de una muestra información general. En nuestro caso, inferimos estadísticamente que, dada la evidencia -contenida en nuestra muestra-, es razonable asumir que sí hay un efecto en ser mujer respecto a participar en el TDNR, dado que es muy improbable que no haya ningún efecto. \n\nEsto lo hicimos definiendo los **datos**, \n$$\nx_i \n\\begin{cases}\n  1, &\\quad \\text{ si participa en TDNR}\\\\\n  0, &\\quad \\text{ si no}\n\\end{cases}\n, \\qquad i=1,2, \\ldots, 16620.\n$$\nLuego, definimos nuestro modelo probabilístico\n$$\nX_1, X_2, \\ldots, X_{16620} \\quad \\text{ son i.i.d. Bernoulli con probabilidad de éxito } p.\n$$\nY, en función del modelo, planteamos hipótesis testeables empíricamente en nuestros datos, i.e., \n$$\nH_0\\colon p = p_0 \\quad \\wedge \\quad H_1\\colon p \\neq p_0,\n$$\nque testeamos con nuestro estadístico de prueba \n$$\nT = \\sum_{i=1}^{n}X_{i},\n$$ \ny que nos arrojó un $t_{obs} = 16{.}270$, que hacía muy improbable que no existiera efecto, ergo, razonable rechazar $H_0$ y aceptar, en cambio, $H_1$; mediante el calculo del $p-$value, que era $\\approx 0$. Esto, como ya dijimos, fue solo una introducción. De ahora en adelante, profundizaremos en el $p-$*value* y la **significancia estadística**, para luego pasar a test de hipótesis como tal según el tipo de distribución. Como los cálculos son lateros, y los suele hacer la calculadora del programa estadístico que se use, lo que sigue será mucho más breve y acotado, dejando intuiciones más breves, definiciones formales y código en R para realizar los test. Con ello, nos iremos adentrando cada vez más al calculo de **estimadores** más especializados, hasta llegar modelos econométricos en el siguiente capítulo. \n\nPor último, dejo dos formas más eficientes de calcular el $p-$*value* con `stats` de R. También uno que calcula exactamente el valor y sin notación científica, para ver lo realmente poco probable que era aceptar la hipótesis nula (realmente es aproximadamente 0) Solo como un adelanto de distintos test que iremos viendo: \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(dplyr)\n\n# Filtrar solo mujeres con respuestas válidas\nenut_muj <- enut %>% \n  filter(sexo == 2, p_tdnr_dt %in% c(0, 1))\n\n# Calcular n y éxitos\nn   <- nrow(enut_muj)\nx   <- sum(enut_muj$p_tdnr_dt == 1)\n\n# 1) p-valor aproximado: prueba de proporciones sin corrección de Yates\np_value_prop <- prop.test(\n  x     = x,\n  n     = n,\n  p     = 0.5,\n  correct = FALSE\n)$p.value\n\n# 2) p-valor exacto: test binomial de dos colas\np_value_exact <- binom.test(\n  x           = x,\n  n           = n,\n  p           = 0.5,\n  alternative = \"two.sided\"\n)$p.value\n\noptions(scipen = 999)\np_value_prop\n## [1] 0\np_value_exact\n## [1] 0.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000004940656\n```\n:::\n",
    "supporting": [
      "02-statistics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}